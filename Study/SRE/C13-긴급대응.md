### 테스트로 인한 장애
상황
- 구글은 시스템 신뢰성을 높이기 위해 재난과 긴급 상호앙에 대한 사전적 테스트 접근법을 취하고 있음.
- 시스템에 장애를 일으킨 후 실패 양상을 관찰. 대부분 계획된 대로 실패가 발생하지만, 때때로 예상하지 못한 결과가 나오기도함.
- 예로, MySQL 데이터베이스의 의존성을 테스트하기 위해, 100개중 하나의 데이터베이스에 대한 접근을 차단하는 테스트 실행. -> 그러나 이 테스트는 예상보다 큰 영향을 미침

대응
- 테스트 시작 몇분 만에 데이터베이스에 의존하는 여러 시스템으로부터 핵심 시스템에 접근할 수 없다는 오류 보고 받음. 일부 시스템은 간헐적으로만 가능.
- 테스트가 원인임을 파악, 즉시 테스트 중단후 롤백 시도했으나 실패
- 당황하지 않고, 이미 검증된 방법을 통해 권한을 복구, 개발자들과 협력하여 데이터베이스 애플리케이션 계층 라이브러리의 결함 수정.
- 약 한시간 이네에 모든 접근 권한 복구되었고 서비스는 정상화

잘한 부분
- 의존 서비스들이 즉각 문제 보고하여 빠르게 대응
- 즉시 테스트 종료한 것.
- 일부 팀들은 시스템을 재구성하여 테스트 데이터베이스를 회피하는 방법 사용
- 후속 조치가 빠르고 철저하게 이루어져 이와 같은 장애가 발생하지 않기 위한 *정기적인 테스트 계획 수립*

깨달은 사실
- 테스트는 철저하게 검토되었지만, 의존 시스템 간 상호 작용에 대한 이해가 부족했음을 깨달음
- 장애대응 절차가 테스트 시작하기 전 불과 몇주전에 마련될 정도로 완전히 검증되지 않음, 재정의하며 개선할 예정
- 대규모 테스트 전에 롤백 절차를 철저히 테스트할 것

### 변경으로 인한 장애
상황
- 구글은 복잡한 구성(configuration)을 지속적으로 변경하며 시스템에 영향을 미치지 않도록 테스트를 진행한다
- 하지만 그 규모와 복잡성 때문에 모든 의존성이나 상호작용을 예측할 수는 없다.
- 어느 금요일 설정 변경이 적용 ->  이 인프라스트럭처는 기본적으로 외부로 노출되는 모든 시스템과 함께 동작했하는 것이였는데, 설정변경이 시스템에 크래시 루프(crash loop)를 유발하여 많은 수의 내부 애플리케이션까지 다운되는 상황을 초래
대응
- 다운이되자 모니터링 시스템이 즉시 쏟아져 나옴.
- 여러 엔지니어들이 비상상황 대처를 위해 패닉룸(백업용 접근이 가능한)으로 모임.
- 5분내 변경된 설정 롤백, 이후 시스템 점차 복구
- 10내 긴급상황으로 선언하고 회사 전체에 알림. 일부 서비스는 원인 불명의 오류로 복구되지 않음.

잘한 부분
- 모니터링 시스템이 빠르게 감지하고 알림전송. 단, 알림이 너무 많이 울림
- Out of band communications 덕분에 소프트웨어 스택이 사용불가능한 상황에서도 사람들간의 소통가능
- 구글의 접근불가능한 상황에서도 설정 변경을 롤백할 수 있는 명령줄 도구, 대체 접근 방식으로 시스템 복구 가능
- 시스템이 업데이트를 점진적으로 제공하는 또 다른 보호 계층을 사용하기 대문에, 크래시 루프의 무한 발생을 어느정도 막음

깨달은 사실
- 까나리 테스트가 부족했다. 중요하지 않은 변경이라 생각되어 느슨하게 진행함
- 알림시스템이 과도하게 울려 실제 문제 해결을 방해

### 절차에 의한 장애
"자동화의 효율은 무서운 결과를 낳기도한다."
신속한 일 처리가 반드시 좋은것 만은 아니다 라는 좋은 예

상황
- 자동화를 통해 서버머신를 관리하는 시스템에서 오류 발생
- 두번째 서버종료 요청이 처리되면서 모든 서버에 대한 디스크 삭제 요청이 큐에 전송, 하드드라이브가 곧 삭제될 위기에 처함

대응
- 자동화 시스템을 일시적으로 비활성화
- 트래픽을 다른 지역으로 재배치하여 사용자 요청을 처리
- 3시간 내 주요 서비스 복구, 3일내 대부분 시스템 재설치 후 복구

잘한 부분
- 트래픽 우회 작업, 대형 서버에는 큰 영향 받지 않음
- 모니터링 시스템 복원과 자동화된 시스템의 종료처리가 신속하게 이루어짐
- 커뮤니케이션과 협업이 잘이루어져 문제 해결에 큰도움

깨달은 부분
- 자동화 서버에 대한 유효성을 적절하게 판단하지 않음, 빈응답이 잘못 처리되어 서버들이 디스크 삭제를 시작함
- 머신의 재설치가 느리고 불안정함. 네트워크 문제와 잘못된 QoS 설정이 주요 원인으로 분석됨
- 동시에 재설치를 처리하는 인프라의 한계가 드러났고, 이를 위해 인프라 튜닝이 필요를 깨달음.

결론,
### 구글의 비상대응 방법

도움요청하라
- 모든 문제에는 해결책이 있다는 것을  배웠다.
- 해결책이 떠오르지 않으면 팀원들을 끌어들이고 빠르게 대응하는 것이 중요하다
- 중요한것은 빠르게 해결하는 것

기록해라: 포스트모텀
- 포스트 모텀을 진행하여 같은 문제가 발생하지 않도록 문서화하는 것이 중요하다.
- 재발하지 않도록 예방

질문던지기
- 현실에서의 테스트가 가장 중요
- 비상상황 발생시 무엇을 할지, 누가대응할지, 시스템은 어떻게 반응할지를 미리 대비해야한다.
- 나에게 계속 질문을 던저보라

사전테스트 장려
- 이론과 현실은 다르다.
- 사전테스트를 통해 예상되는 실패 상황을 미리 확인하고 대응 방법을 준비하는 것이 중요하다.


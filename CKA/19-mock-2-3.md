
https://kubernetes.io/ko/docs/reference/kubectl/cheatsheet/
![[Pasted image 20240727223348.png|500]]

### mock 2

1.
Take a backup of the etcd cluster and save it to `/opt/etcd-backup.db`.

https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster
etcdctl backup

```shell
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=<trusted-ca-file> --cert=<cert-file> --key=<key-file> \
  snapshot save <backup-file-location>
```

```
cat /etc/kubernetes/manifests/etcd.yaml

-> etcdctl 명령 옵션으로 줄 값들을 조회


ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot save /opt/etcd-backup.db
```


2.
Create a Pod called `redis-storage` with image: `redis:alpine` with a Volume of type `emptyDir` that lasts for the life of the Pod.

https://kubernetes.io/docs/concepts/storage/volumes/#emptydir-configuration-example

![[Pasted image 20240727224018.png|400]]


```
k run --image=redis:alpine redis-storage --dry-run=client -o yaml >
 redis-storage.yml

vi redis-storage.yml

volume 추가
apply

```


3)
Create a new pod called `super-user-pod` with image `busybox:1.28`. Allow the pod to be able to set `system_time`.
The container should sleep for 4800 seconds.

https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container

![[Pasted image 20240727224814.png|400]]

```
k run --image=busybox:1.28 super-user-pod --dry-run=client -o yaml > super-user-pod.yml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: super-user-pod
  name: super-user-pod
spec:
  containers:
  - image: busybox:1.28
    name: super-user-pod
    command: ["sleep", "4800"]
    securityContext:
      capabilities:
        add: ["SYS_TIME"]
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}


k apply -f super-user-pod.yml 
```


4)
A pod definition file is created at `/root/CKA/use-pv.yaml`. Make use of this manifest file and mount the persistent volume called `pv-1`. Ensure the pod is running and the PV is bound.

mountPath: `/data`  
persistentVolumeClaim Name: `my-pvc`


https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims
pvc

![[Pasted image 20240727225403.png]]
pv -> 
CAPACITY: 10Mi
ACCESS MODES: RWO

이에 맞춰 pvc 생성

```shell
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Mi
```

pod에서 사용하도록 pod 설정수정.
```
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: use-pv
  name: use-pv
spec:
  containers:
  - image: nginx
    name: use-pv
    resources: {}
    volumeMounts:
    - mountPath: "/data"
      name: mypd
  volumes:
  - name: mypd
    persistentVolumeClaim:
      claimName: my-pvc
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```


5)
Create a new deployment called `nginx-deploy`, with image `nginx:1.16` and `1` replica. Next upgrade the deployment to version `1.17` using rolling update.

```
kubectl create deployment nginx-deploy --image=nginx:1.16 --dry-run=client -o yaml > deploy.yaml

# --record 옵션으로 rollout history 저장
kubectl apply -f deploy.yaml --record

kubectl rollout history deployment nginx-deploy

kubectl set image deployment/nginx-deploy nginx=nginx:1.17 --record

kubectl rollout history deployment nginx-deploy
```


6
)Create a new user called `john`. Grant him access to the cluster. John should have permission to `create, list, get, update and delete pods` in the `development` namespace . The private key exists in the location: `/root/CKA/john.key` and csr at `/root/CKA/john.csr`.  

  `Important Note`: As of kubernetes 1.19, the CertificateSigningRequest object expects a `signerName`.  
  Please refer the documentation to see an example. The documentation tab is available at the top right of terminal.

https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#create-certificatesigningrequest

인증서 인코딩> 인증서 서명요청  > 인증서 approve > role 생성 > role binding 생성

![[Pasted image 20240727231256.png|500]]
![[Pasted image 20240727231338.png|500]]

제공해준 john.csr 를 base64로 인코딩한 값을
csr 정의파일의 request부분에 넣어준다.
```
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: john-developer
spec:
  request: ~~~~
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth
```

![[Pasted image 20240727232012.png]]
배포 후 pending 상태 확인
인증서 approve

```
k certificate approve john-developer

# role 생성
kubectl create role developer --verb=create,update,delete,get,list,watch --resource=pods -n development

k auth can-i get pods --namespace=development --as john
-> no

# user에게 role binding
kubectl create rolebinding john-developer --role=developer --user=john -n development

k auth can-i get pods --namespace=development --as john
-> yes
```


7번하다 시간초과
7)
Create a nginx pod called `nginx-resolver` using image `nginx`, expose it internally with a service called `nginx-resolver-service`. Test that you are able to look up the service and pod names from within the cluster. Use the image: `busybox:1.28` for dns lookup. Record results in `/root/CKA/nginx.svc` and `/root/CKA/nginx.pod`

-> busybox 파드에 접속해서
expose 한 `nginx-resolver-service`와 nginx-resolver pod를 nslookup 질의한 결과를 
각각 `/root/CKA/nginx.svc` and `/root/CKA/nginx.pod` 에 저장하라는 것.

https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/

pod 조회: `172-17-0-3.default.pod.cluster.local`.

```

kubectl run nginx-resolver --image=nginx
kubectl expose pod nginx-resolver --name=nginx-resolver-service --port=80 --target-port=80 --type=ClusterIP

# created 되고 sleep 하는동안 nslookup 하도록 sleep 처리
k run busybox --image=busybox:1.28 -- sleep 4000


# svc nslookup
k exec busybox -- nslookup nginx-resolver-service > /root/CKA/nginx.svc

# pod nslookup
k get po
k describe po nginx-resolver
-> 조회된 ip로 nslookup

k exec busybox -- nslookup 10-244-192-4.default.pod.cluster.local > /root/CKA/nginx.pod

```


8)
Create a static pod on `node01` called `nginx-critical` with image `nginx` and make sure that it is recreated/restarted automatically in case of a failure.

Use `/etc/kubernetes/manifests` as the Static Pod path for example.

-> node01에 static pod를 생성하라.
static pod 이므로 delete 해도 자동으로 재 시작 되어야 한다.

```
# in controlplane
kubectl run nginx-critical --image=nginx --restart=Always --dry-run=client -o yaml 

# in node01
cat > /etc/kubernetes/manifests/nginx-critical.yaml
```

or

```
kubectl run nginx-critical --image=nginx --dry-run=client -o yaml > static.yaml

root@controlplane:~# scp static.yaml node01:/root/


root@controlplane:~# kubectl get nodes -o wide

# node01 접속
root@controlplane:~# ssh node01

# static pod 저장할 manifests 경로 생성
root@node01:~# mkdir -p /etc/kubernetes/manifests

# controlplane에서 생성한거 copy
root@node01:~# cp /root/static.yaml /etc/kubernetes/manifests/

root@node01:~# exit

root@controlplane:~# kubectl get pods
```

### mock 3

1.
Create a new service account with the name `pvviewer`. Grant this Service account access to `list` all PersistentVolumes in the cluster by creating an appropriate cluster role called `pvviewer-role` and ClusterRoleBinding called `pvviewer-role-binding`.  
Next, create a pod called `pvviewer` with the image: `redis` and serviceAccount: `pvviewer` in the default namespace.


serviceaAccount생성 > clusterRole 생성 > clusterRoleBinding  > pod에 serviceAccount 할당

```
kubectl create serviceaccount pvviewer

kubectl create clusterrole pvviewer-role --verb=list --resource=persistentvolumes

kubectl create clusterrolebinding pvviewer-role-binding --clusterrole=pvviewer-role --serviceaccount=default:pvviewer

```

```shell
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pvviewer
  name: pvviewer
spec:
  serviceAccountName: pvviewer
  containers:
  - image: redis
    name: pvviewer
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```


2.
List the `InternalIP` of all nodes of the cluster. Save the result to a file `/root/CKA/node_ips`.

`Answer should be in the format: `InternalIP of controlplane`<space>`InternalIP of node01` (in a single line)`

cluster 내 모든 노드의 internal IP 저장하라 -> json path 사용가능


![[Pasted image 20240728215516.png]]
```
kubectl get nodes -o=jsonpath='{.items[*].status.addresses[0].address}'
```

or
```
k get nodes -o json | jq | grep -i internalip -C 20

k get nodes -o json | jq -c 'paths' | grep type | grep -v condition
```

json path document,
kubectl cheat sheet document 참고 https://kubernetes.io/pt-br/docs/reference/kubectl/cheatsheet/

```bash
# Obtém ExternalIPs de todos os nós
kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="ExternalIP")].address}'
```

```
k get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}' > /root/CKA/node_ips
```



3.
Create a pod called `multi-pod` with two containers.  
Container 1: name: `alpha`, image: `nginx`  
Container 2: name: `beta`, image: `busybox`, command: `sleep 4800`  
  
Environment Variables:  
container 1:  
`name: alpha`  
  
Container 2:  
`name: beta`

```shell
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: alpha
  name: multi-pod
spec:
  containers:
  - image: nginx
    name: alpha
    env:
      - name: "name"
        value: "alpha"
  - image: busybox
    name: beta
    env:
      - name: "name"
        value: "beta"
    command: ["sleep", "4800"]
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```



4.
Create a Pod called `non-root-pod` , image: `redis:alpine`  
  
runAsUser: 1000  
  
fsGroup: 2000

```shell
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: non-root-pod
  name: non-root-pod
spec:
  securityContext:
    runAsUser: 1000
    fsGroup: 2000
  containers:
  - image: redis:alpine
    name: non-root-pod
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

![[Pasted image 20240728220822.png]]


5.
We have deployed a new pod called `np-test-1` and a service called `np-test-service`. Incoming connections to this service are not working. Troubleshoot and fix it.  
Create NetworkPolicy, by the name `ingress-to-nptest` that allows incoming connections to the service over port `80`.

Important: Don't delete any current objects deployed.


![[Pasted image 20240728221007.png]]

curl 테스트할 pod 생성
```
k run curlpod --image=alpine/curl --rm -it -- sh

$ curl np-test-service
```


```shell
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ingress-to-nptest
  namespace: default
spec:
  podSelector:
    matchLabels:
      run: np-test-1   
  policyTypes:
    - Ingress
  ingress:
    -
      ports:
        - protocol: TCP
          port: 80
```

![[Pasted image 20240728221402.png]]
#### 시험볼때 꼭 테스트 해보기


6.
Taint the worker node `node01` to be Unschedulable. Once done, create a pod called `dev-redis`, image `redis:alpine`, to ensure workloads are not scheduled to this worker node. Finally, create a new pod called `prod-redis` and image: `redis:alpine` with toleration to be scheduled on `node01`.

key: `env_type`, value: `production`, operator: `Equal` and effect: `NoSchedule`


![[Pasted image 20240728221529.png]]

taint nodschedule로 설정후 
파드 생성 한 다음
스케쥴 되지 않는지 확인

*taint에 대응하는 tolerations을 설정해줘야 node01에 스케줄 가능*

```shell
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: prod-redis
  name: prod-redis
spec:
  containers:
  - image: redis:alpine
    name: prod-redis
    resources: {}
  tolerations:
  - key: "env_type"
    operator: "Equal"
    value: "production"
    effect: "NoSchedule"
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```


7.
Create a pod called `hr-pod` in `hr` namespace belonging to the `production` environment and `frontend` tier .  
image: `redis:alpine`

Use appropriate labels and create all the required objects if it does not exist in the system already.

```
k create ns hr

k run --image=redis:alpine hr-pod -n hr --labels="environment=production,tier=frontend"
```


8.
A kubeconfig file called `super.kubeconfig` has been created under `/root/CKA`. There is something wrong with the configuration. Troubleshoot and fix it.

![[Pasted image 20240728222054.png|500]]

```
vi /root/CKA/super.kubeconfig
->
server: https://controlplane:9999
부분 포트 수정

cat .kube/config 
를 확인해서 수정.
포트 6443

```


![[Pasted image 20240728222309.png|500]]


9.
We have created a new deployment called `nginx-deploy`. scale the deployment to 3 replicas. Has the replica's increased? Troubleshoot the issue and fix it.

![[Pasted image 20240728222439.png]]

![[Pasted image 20240728222446.png]]


![[Pasted image 20240728222524.png]]

```
cd /etc/kubernetes/manifests/

vi kube-controller-manager.yaml 

오타수정
contro1ler -> controller
여러개..

```
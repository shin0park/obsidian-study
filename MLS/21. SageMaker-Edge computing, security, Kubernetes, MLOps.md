

## SageMaker on the Edge 

- AWS SageMaker는 클라우드에서 학습한 모델을 엣지 디바이스에 배포해, 로컬 데이터 기반 실시간 추론을 가능하게 하는 기능을 제공한다.
- SageMaker는 모델 훈련과 관리를 주로 클라우드에서 수행하는 서비스이지만, 그 결과로 생성된 모델은 종종 리소스가 제한적인 엣지 디바이스에서 실행될 필요가 있다. 
	- 이러한 엣지 디바이스는 IoT 센서, 차량 시스템, 산업용 로봇 등 광범위한 환경을 포괄한다.
	- SageMaker의 목표는 클라우드에서 구축된 복잡한 모델을 이러한 다양한 엣지 환경에 맞게 최적화하고 배포하여, 로컬에서 빠르고 효율적인 추론을 수행하도록 지원하는 것이다.

### SageMaker Neo 

- **개념: 머신러닝 모델을 한 번 학습시키면 어디서든 실행할 수 있게 해주는 기능 “Train once, run anywhere”**  
	- 즉, 한 번 훈련한 모델을 다양한 엣지 환경에서 최적화하여 실행할 수 있게 해주는 서비스
	  
- **대상 디바이스:** ARM, Intel, Nvidia 프로세서를 사용하는 엣지 디바이스(예: 자동차, 임베디드 장치 등)에 최적화
- **기능:** 
	- 특정 디바이스 하드웨어에 맞춰 코드를 최적화한다. 
	- 컴파일러(Compiler)와 런타임(Runtime)으로 구성
		- **컴파일러**: 훈련된 모델의 컴퓨팅 그래프를 분석하여 특정 대상 디바이스에 맞게 코드를 최적화하는 역할을 수행
			- 하드웨어 가속 기능을 최대한 활용할 수 있도록 모델을 변환
		- **런타임**: 컴파일된 모델을 해당 엣지 디바이스에서 효율적으로 로드하고 실행하기 위한 경량화된 환경을 제공
- **지원 프레임워크:** 
	- Tensorflow, MXNet, PyTorch, ONNX, XGBoost, DarkNet, Keras 등 주요 프레임워크를 지원
	- 광범위한 모델 타입을 처리

### Neo + AWS IoT Greengrass 

#### Neo 배포 방식:

두 가지 경로를 통해 운영환경에 배포 가능하다.

1. Neo로 컴파일된 모델을 HTTPS 엔드포인트(C5, M5, P3 인스턴스 등)에 배포
	- 단, 컴파일 시 사용한 인스턴스 타입과 동일해야 함.
		- 이는 단순한 운영상의 규칙이 아니라, Neo가 수행하는 하드웨어 특화 최적화의 결과이다.
		- Neo는 모델을 특정 하드웨어의 명령어 세트 아키텍처(ISA) 및 메모리 구조에 맞춰 성능을 극대화하도록 컴파일한다. 
		- 만약 컴파일된 환경과 다른 인스턴스에 배포할 경우, 최적화된 명령이 올바르게 실행되지 않거나, 예측된 성능을 보장할 수 없어 운영 안정성에 심각한 문제가 발생할 수 있기 때문이다. 

2. **IoT Greengrass**를 통해 실제 엣지 디바이스에 모델을 배포
	- Greengrass는 클라우드에서 훈련된 모델을 현장의 장치로 전달하는 역할을 한다.

#### IoT Greengrass:

- **장점:** 
	- 클라우드에서 학습된 모델을 사용하여 엣지 디바이스에서 로컬 데이터로 직접 추론(Inference)을 수행할 수 있다.
- 추론 작업은 주로 Lambda 추론 애플리케이션(Lambda inference applications)을 활용하여 실행한다.


---

## SageMaker 보안 (SageMaker Security)

### General AWS Security 

- **IAM (Identity and Access Management):** 
	- 사용자 계정 및 권한 관리의 핵심
	- 최소 권한 원칙에 따라 필요한 권한만 부여해야 한다
- **MFA (Multi-Factor Authentication):** 다중 요소 인증을 사용하여 보안을 강화.
- **SSL/TLS:** 모든 연결 시 보안 프로토콜을 사용
- **CloudTrail:** API 호출 및 사용자 활동을 기록(Log)하여 감사
- **암호화 & PII(개인식별정보) 보호:** 데이터 암호화를 적용하고 개인 식별 정보(PII) 관리에 주의해야 한다.

### 저장 데이터 보호 (Data at Rest)

- **KMS (Key Management Service):** 
	- 노트북 및 모든 SageMaker 작업(Job)에서 KMS 키를 통한 암호화를 지원한다
- **S3 암호화:** 
	- 훈련 데이터 및 모델 호스팅을 위해 암호화된 S3 버킷을 사용 가능
	- S3 버킷 자체도 KMS를 활용하여 서버 측 암호화(SSE-KMS)를 적용 가능
- **로컬 스토리지:** 
	- Notebook 인스턴스와 `/opt/ml/`, `/tmp` 디렉터리 하위의 모든 데이터는 KMS 키로 암호화 가능하다

### **전송 중 데이터 보호 (Data in Transit)**

- **기본 암호화:** SageMaker 환경 내외부로 전송되는 데이터는 TLS/SSL 프로토콜을 통해 보호된다.
	- 모든 트래픽 TLS/SSL 지원
- **IAM 역할:** SageMaker에 IAM 역할을 할당하여 리소스 접근 권한을 부여
  
- 분산 훈련 
	- 노드 간(inter-node) 통신 즉, 컨테이너 간 트래픽 암호화(inter-container traffic encryption)를 선택적으로 활성화 가능하다
		- 이 옵션은 보안 수준을 높이지만, 암호화 및 복호화 오버헤드 때문에 **딥러닝 작업 시 훈련 시간과 비용을 증가시킬 수 있다**는 점을 고려해야 한다.
		- 이 기능은 보안 요구 사항이 매우 높은 환경에서 신중하게 활성화되어야 한다.
	- 훈련 또는 튜닝 작업을 설정할 때 콘솔 또는 API를 통해 구성된다.


---

### **SageMaker와 VPC (Virtual Private Cloud)**

- **VPC 활용:** 
	- 훈련 작업은 기본적으로 VPC 내에서 실행되므로 기본적인 네트워크 격리 이점을 누릴 수 있다. 
	- 더 높은 수준의 보안이 필요할 경우, 사용자 지정 프라이빗 VPC를 사용할 수 있다.
- **S3 접근:** 
	- 프라이빗 VPC를 사용한다면,
		- 훈련 데이터와 모델을 저장하는 S3에 접근하기 위해 **S3 VPC 엔드포인트** 를 설정해야한다 
		- 이러한 접근은 사용자 지정 엔드포인트 정책과 S3 버킷 정책을 통해 세부적으로 통제되고 보안이 유지가능하다.
- **인터넷 접근 제어:**
    - Notebook, Training / Inference 컨테이너는 기본적으로 인터넷 연결이 활성화되어 있어 보안 취약점이 될 수 있다. → 필요 시 비활성화
	    - 특히 Notebook의 경우, 이는 보안 취약점(security hole)이 될 수 있다.
    - 인터넷 연결을 비활성화할 경우, 훈련 및 호스팅이 작동하려면 **VPC 인터페이스 엔드포인트(PrivateLink)** 또는 **NAT 게이트웨이**가 필요하다.
    - 최고 수준의 격리를 제공하는 네트워크 격리(Network Isolation) 옵션도 있지만, 이는 S3 접근까지 차단됨을 유의해야 한다.
	    - 훈련 데이터의 입출력 방식에 대한 별도의 설계가 필요

---

### SageMaker와 IAM 권한

- **주요 작업 권한:** 
	- CreateTrainingJob
	- CreateModel
	- CreateEndpointConfig
	- CreateTransformJob
	- CreateHyperParameterTuningJob
	- CreateNotebookInstance 등
    
- **사전 정의된 정책 (Managed Policies):**
    - `AmazonSageMakerReadOnly`: 읽기 전용
    - `AmazonSageMakerFullAccess`: 전체 접근
    - `AdministratorAccess`: 관리자 권한
    - `DataScientist`: 데이터 과학자용 권한
    
---

### 로깅 및 모니터링 (Logging and Monitoring)

- **CloudWatch:**
    - 엔드포인트의 호출(Invocations) 및 지연 시간(Latency) 모니터링.
    - 인스턴스 노드의 상태(CPU, 메모리 등) 확인
    - **Ground Truth:** 활성 작업자 수 및 작업량 모니터링

- **CloudTrail:** 
	- SageMaker 내의 사용자, 역할, 서비스의 모든 API call 기록
	- 로그 파일은 S3로 전달되어 저장되며, 이는 규정 준수 및 보안 감사(auditing) 목적으로 사용된다.

---
### SageMaker 리소스 관리 (Managing Resources)

#### **인스턴스 유형 선택**

- **학습 (Training):** 딥러닝 알고리즘은 GPU 인스턴스(P3, G4dn 등)가 유리
- **추론 (Inference):** 학습보다 리소스가 덜 필요하므로 컴퓨팅 인스턴스(C5 등)로 충분한 경우가 많다. 

-> GPU 인스턴스는 높은 성능을 제공하지만, 비용이 매우 비싸다는 점(really pricey)을 항상 고려해야 된다.
    

#### **관리형 스팟 학습 (Managed Spot Training)**

- **비용 절감:** EC2 스팟 인스턴스를 사용하여 온디맨드 대비 최대 90% 비용을 절감할 수 있다.
- **주의사항:** 
	- AWS의 미사용 용량을 활용하므로 수요에 따라 언제든지 중단(interrupted)될 수 있다는 위험을 내포하고 있다.
- **해결책:** 
	- S3에 **체크포인트(Checkpoints)** 를 저장하도록 설정하여, 중단 시 학습을 이어서 진행할 수 있게 해야 한다. 
		- 복구 메커니즘은 장시간 훈련에 필수적이다.
		- 스팟 인스턴스 가용성을 기다려야 하므로, 훈련 작업의 총 소요 시간이 증가할 수 있다는 점 또한 운영상의 고려 사항이다.
		- Managed Spot Training은 하이퍼파라미터 튜닝과 같은 자동 모델 튜닝 작업에도 적용될 수 있으며, 관련 지표 및 로그는 CloudWatch에서 확인 가능하다
    

#### **탄성적 추론(Elastic Inference - EI) 를 활용한 추론 가속**

- 딥러닝 추론을 가속화하는 동시에, 전체 GPU 인스턴스를 사용하는 것보다 훨씬 적은 비용(fraction of cost)으로 구현할 수 있는 비용 효율적인 솔루션이다.
- **방식:** EI 아키텍처는 EI 가속기(accelerators)를 표준 CPU 인스턴스(예: `ml.eia1.medium/large/xlarge`)에 보조적으로 추가하여 가속 기능을 제공한다.
- **지원:** 
	- TensorFlow, PyTorch, MXNet 사전 빌드 컨테이너와 함께 작동한다.
	- Notebook에도 사용 가능
	- ONNX를 사용하여 모델을 MXNet으로 내보낼 수 있으며, Image Classification 및 Object Detection과 같은 내장 알고리즘도 지원한다.
	- Custom Container에서도 EI-enabled 환경 가능

---

### Automatic Scaling 

- 자동 확장은 프로덕션 엔드포인트의 부하에 따라 인스턴스 수를 동적으로 조정하는 기능
- **설정:** 
	- CloudWatch 지표(Metrics)를 기반으로 목표값, 최소/최대 용량, 쿨다운 기간을 정의하는 조정 정책(Scaling Policy)을 설정
	- 이 기능은 CloudWatch와 연동되어 모니터링 지표에 반응하며 확장된다.
- **필수:** 적용 전 부하 테스트(Load test)가 반드시 필요
    

### **서버리스 추론 (Serverless Inference)**

- 2022년에 도입
- **특징:** 컨테이너 이미지, 메모리, 동시성 요구사항만 지정하면 기본 용량이 자동으로 프로비저닝되고 확장되는 배포 방식
- **적합한 사례:** 
	- 트래픽이 간헐적이거나 예측하기 어려운 경우에 적합. 
	- 요청이 없으면 **0으로 축소(Scale down to zero)** 된다.
- 비용: 사용량(밀리초 단위)에 따라 과금
- 모니터링: CloudWatch(ModelSetupTime, MemoryUtilization 등)로 모니터링

---

### **Amazon SageMaker Inference Recommender**

- 모델 배포를 위한 최적의 인스턴스 유형과 구성을 추천하여 운영 최적화를 자동화하는 도구
- **기능:** 모델에 가장 적합한 인스턴스 유형과 endpoint 구성을 추천하고, 부하 테스트 및 튜닝을 자동화한다.
- **Workflow:** 
	1. 모델을 Model Registry에 등록
	2. 여러 endpoint 구성을 벤치마크
	3. 다양한 엔드포인트 구성에 대해 벤치마킹을 수행하고 지표를 수집
	4. 모델 튜닝을 위한 로드 테스트를 자동화하며, 다음과 같은 두 가지 유형의 추천을 제공:   
		1. **인스턴스 추천 (Instance Recommendations):** 
			- 추천된 인스턴스 유형에 대해 로드 테스트를 실행하며 약 45분이 소요
		2. **엔드포인트 추천 (Endpoint Recommendations):** 
			- 사용자 정의 로드 테스트로, 사용자가 인스턴스, 트래픽 패턴, 지연 시간, 처리량 요구 사항을 지정하며 약 2시간이 소요 
	5. 추천 결과는 `CostPerHour`, `CostPerInference`, `ModelLatency` 등 상세한 지표를 포함하여 최적의 구성을 결정하는 데 필요한 데이터를 제공

---

### 가용성 영역 (Availability Zones)

- **고가용성:** SageMaker는 엔드포인트 인스턴스를 자동으로 AZ 분산
	- 단, **여러 인스턴스**를 배포해야 AZ 분산이 적용됨
	  
- **VPC 구성:** 최소 두 개의 서브넷을 각각 다른 AZ에 위치하도록 구성해야한다.

---

### SageMaker와 MLOps

MLOps는 머신러닝 시스템의 배포, 모니터링 및 유지 관리를 자동화하고 표준화하는 엔지니어링 관행이다.

#### **Kubernetes와의 통합**

- SageMaker는 기존의 Kubernetes 기반 ML 인프라와의 통합을 지원하여, 조직이 클라우드와 온프레미스 환경을 유연하게 결합할 수 있도록 돕는다.
    
- **도구:**
    - **SageMaker Operators for Kubernetes:** 
	    - ![](images/Pasted%20image%2020251201225440.png)
	    - Kubernetes 기반 ML 인프라와 SageMaker 서비스를 통합하는 데 사용된다.
	    - 즉, Kubernetes에서 SageMaker 작업(학습, 튜닝, 추론)을 제어할 수 있게 해준다.
		    - EKS 클러스터 내에서 SageMaker 작업을 Kubernetes CRD(Custom Resource)로 정의
		    - Kubernetes에서 직접 SageMaker Training/Inference 자원 생성 및 관리
		    - EKS Worker Node에서 SageMaker Operator가 요청을 SageMaker로 전달
    - **SageMaker Components for Kubeflow Pipelines:** 
	    - ![](images/Pasted%20image%2020251201225506.png)
	    - **Kubeflow Pipelines**의 구성 요소와 통합되어, 기존에 Kubeflow 위에 구축된 ML 플랫폼을 클라우드 환경으로 확장하거나 결합하는 것을 가능하게 한다.
	    - Kubeflow 파이프라인의 각 단계(전처리, 학습, 튜닝, 배포 등)에서 SageMaker 기능을 수행하도록 지원한다.
		    - Processing
		    - Hyperparameter Tuning
		    - Training
		    - Inference 
			    - → 모든 단계가 SageMaker 리소스를 활용해 실행
	    

---

### SageMaker Projects

![](images/Pasted%20image%2020251201225921.png)

- SageMaker Studio의 **네이티브 MLOps(CI/CD) 솔루션**
- **기능:** SageMaker Studio 내의 기본 MLOps 솔루션으로 CI/CD(지속적 통합/배포)를 제공한다.
- **워크플로:** 
	- 이미지 빌드 -> data 준비, feature engineering -> Train models -> Evaluate models -> Deploy models -> 모니터링의 전체 수명 주기를 관리한다.
- **구성 요소:** 
	- 코드 리포지토리(CodeCommit/GitHub 등)와 SageMaker Pipelines를 사용하여 모델 구축 및 배포 자동화를 정의

---

### **추론 파이프라인 (Inference Pipelines)**

- 추론 파이프라인은 단일 엔드포인트 내에서 여러 처리 단계를 순차적으로 실행할 수 있도록 설계된 아키텍처이다.
- **구조:** 2개에서 최대 15개까지의 컨테이너를 선형 순서(Linear sequence)로 연결하여 처리
- **활용:** 
	- Pre-processing → Model inference → Post-processing
	- 전처리 -> 추론 -> 후처리 위 과정을 하나의 엔드포인트로 묶을 수 있다.
		- 실제 운영 환경에서 모델의 입력 데이터는 종종 정규화나 특징 추출과 같은 복잡한 전처리 과정을 거쳐야 한다.
		- 추론 파이프라인은 이러한 전처리 로직, 모델 추론, 그리고 최종 결과 정제 과정을 하나의 관리형 엔드포인트로 캡슐화한다. 
		- 이는 클라이언트 측 애플리케이션의 복잡성을 줄이고, ML 시스템 내에서 전처리 로직의 버전 관리를 통합하여 MLOps 효율성을 크게 향상시킨다.
- **호환성:** 
	- 내장 알고리즘과 사용자 지정 Docker 컨테이너를 조합 가능 
	- Spark ML이나 Scikit-learn 컨테이너도 사용 가능
		- Spark ML은 Glue 또는 EMR과 함께 실행 가능, 모델은 MLeap 형식으로 직렬화된다.
- **지원:** 
	- 실시간 추론(Real-time inference)과 배치 변환(Batch transform)을 모두 지원

---

## Lab

- Sagemaker console > Create notebook instance > instance type 지정 (elastic inference 지정 가능)> 인스턴스 생성
- jupyter 접속하여 원하는 작업 실행 (예제: Keras CNN 훈련 배포하는 스크립트)
- s3로 데이터 업로드(ex. mnist)
- CNN 스크립트 테스트
	- sagemaker, keras, numpy 등을 import
	- mnist.load_data() 하여 train 데이터 추출
	- mnist 데이터를 s3에 업로드
	- cnn train 스크립트를 로컬 notebook 인스턴스에서 실행
		- `!pygmentize mnist-train-cnn.py`
	- gpu 인스턴스에서 Train with 10 epochs
		- ml.p3.2xlarge
- 콘솔 Sagemake > Training jobs > 실행완료됨을 확인
- 모델 배포 (tf_estimator.deploy())
	- ml.c5.large 인스턴스 타입
	- ml.eia1.medium 탄성적 추론인(EI) accelerator_type 설정
- -> mnist data에 대해 CNN을 미리 훈련하여 손글씨 인식 가능하도록 한것
- 실제 손글씨 이미지를 주입하여 예측 결과 출력
- Automatic Model Tuning을 통한 하이퍼파라미터 튜닝
	- 하이퍼파라미터 range 설정: epoch, learning-rate, batch-size
	- val_acc: 최적화 하려는 객체 이름 (유효성 검사세트에서의 정확도)
	- hyperparameterTuner 실행 (parallel 하게 실행)
	- 동일하게 콘솔 Sagemake > hyperparameter jobs > 실행완료됨을 확인
- 모델 재배포 후 다시 예측 수행 -> 정확도 높아짐


---

## Quiz
### ML Implementation and Operations
1. Where does SageMaker's automatic scaling get the data it needs to determine how many endpoints you need? 
	- cloudwatch 
		- Amazon SageMaker의 자동 확장 기능은 **엔드포인트의 부하 상태(예: CPU 사용률, 요청 수, 지연 시간 등)** 를 기준으로 확장 여부를 판단
		- 이러한 **모니터링 지표(metric)** 는 **Amazon CloudWatch**에서 수집·관리
2. Your SageMaker inference is based on a Tensorflow or MXNet network. You want it to be fast, but don't want to pay for P2 or P3 inference nodes. What's a good solution?
	- Elastic Inference
		- Elastic Inference는 EI 가속기를 통해 **딥러닝 가속을 필요한 만큼만 CPU 인스턴스에 붙여서 사용**하는 방식
		- P2/P3 같은 고가의 GPU 인스턴스를 쓰지 않고도 **추론 성능을 크게 향상**
3. You are deploying SageMaker inside a VPC, but the Internet access from SageMaker notebooks is considered a security hole. How might you address this?
	- Disable internet access when creating the notebook, and set up a NAT Gateway to allow the outbound connections needed for training and hosting.
	- 노트북 생성 시 인터넷 접근을 비활성화하고 NAT Gateway 사용
		- 필요한 외부 통신(S3, ECR 등)은 **NAT Gateway**를 통해 제한적으로 허용하는 것이 권장 아키텍처
4. You want to deploy your trained semantic segmentation model from SageMaker to an embedded ARM device in a car. Which services might you use to accomplish this?
	- SageMaker Neo and IoT GreenGrass
		- 임베디드/엣지 환경 배포에서 사용
5. When constructing your own training container for SageMaker, where should the actual training script files be stored?
	- /opt/ml/code/
		- SageMaker는 **`/opt/ml/code/` 디렉토리에서 학습 스크립트를 실행**하도록 설계되어 있다.
		- `/opt/ml/model/`: 학습 결과(모델 아티팩트) 저장 위치 ❌
		- `/opt/ml/train/`: 존재하지 않는 표준 경로 ❌

---

### Warmup Test: Quick Assessment

1. Your company wishes to monitor social media, and perform sentiment analysis on Tweets to classify them as positive or negative sentiment. You are able to obtain a data set of past Tweets about your company to use as training data for a machine learning system, but they are not classified as positive or negative. How would you build such a system?
	- 레이블링되지 않은 데이터로 감성 분석
	- $\rightarrow$ **Use SageMaker Ground Truth to label past Tweets as positive or negative, and use those labels to train a neural network on SageMaker.**
		- 먼저 Ground Truth로 레이블링 작업을 수행하여 정답 데이터셋을 만든 후, 이 데이터를 사용하여 SageMaker에서 모델을 훈련
   
   
2. Your automatic hyperparameter tuning job in SageMaker is consuming more resources than you would like, and coming at a high cost. What are TWO techniques that might reduce this cost?
	- 하이퍼파라미터 튜닝 비용 절감
	- $\rightarrow$ **Use less concurrency while tuning** : 튜닝 시 더 적은 동시성(병렬 처리)을 사용
		- 한 번에 띄워지는 인스턴스 수가 줄어들어 전체 튜닝 비용이 절감
	- $\rightarrow$ **Use logarithmic scales on your parameter ranges** :파라미터 범위에 로그 스케일을 사용
		- `learning rate`나 `regularization` 계수 등 **10의 거듭제곱** 단위로 탐색하는 것이 효율적인 하이퍼파라미터에 로그 스케일($\log$)을 명시하면, 탐색 공간을 더 효율적으로 조사하여 같은 예산으로 더 빨리 좋은 조합에 도달할 수 있어 결과적으로 비용을 줄일 수 있다.
   
   
3. You are developing an autonomous vehicle that must classify images of street signs with extremely low latency, processing thousands of images per second. What AWS-based architecture would best meet this need? 
	-  초저지연 엣지 컴퓨팅 아키텍처 (Low-Latency Edge Computing)
	- $\rightarrow$ **Develop your classifier with TensorFlow, and compile it for an NVIDIA Jetson edge device using SageMaker Neo, and run it on the edge with IoT GreenGrass.**
		   - 초저지연, 초당 수천 장 처리는 **클라우드로 데이터를 전송하는 방식으로는 달성하기 어렵기 때문에 차량 내부의 엣지 디바이스에서 추론이 이루어져야 한다. 
		   - SageMaker Neo:** 학습된 ML 모델을 **특정 하드웨어 아키텍처(예: NVIDIA Jetson)** 에 맞게 최적화된 실행 파일로 컴파일하여 추론 속도를 극대화 
		   - **AWS IoT Greengrass:** 컴파일된 모델을 Jetson 디바이스와 같은 엣지 디바이스에 배포하고 로컬에서 추론을 실행하여 네트워크 지연 없이 **실시간 추론**을 가능하게 한다.
   
   
4. After training a deep neural network over 100 epochs, it achieved high accuracy on your training data, but lower accuracy on your test data, suggesting the resulting model is overfitting. What are TWO techniques that may help resolve this problem?
	- 딥러닝 모델 과적합 해결 (Overfitting)
	- $\rightarrow$ **Use early stopping**
		- 훈련 정확도는 계속 높아지더라도 **검증 손실(Validation Loss)** 이 더 이상 개선되지 않을 때 학습을 미리 중단하여, 훈련 데이터에만 너무 맞춘 가중치로 가는 것을 방지하고 과적합을 완화
	-  $\rightarrow$ **Use dropout regularization** : 드롭아웃 정규화(Dropout regularization)
		- 훈련 시 각 미니 배치마다 **일부 뉴런을 랜덤하게 비활성화**하여(0으로 설정), 특정 경로에 과도하게 의존하는 패턴을 줄이고 앙상블 효과를 내어 과적합을 완화
   
   
5. A large news website needs to produce personalized recommendations for articles to its readers, by training a machine learning model on a daily basis using historical click data. The influx of this data is fairly constant, except during major elections when traffic to the site spikes considerably. Which system would provide the most cost-effective and simplest solution?
	
	- 클릭 데이터 기반 개인화 추천 시스템 (Personalized Recommendations)
	- $\rightarrow$ **Publish click data into Amazon S3 using Kinesis Firehose, and process the data nightly using Apache Spark and MLLib using spot instances in an EMR cluster. Publish the model's results to DynamoDB for producing recommendations in real-time.**
		- Kinesis Firehose를 사용하여 클릭 데이터를 Amazon S3에 게시하고, EMR 클러스터의 Spot 인스턴스를 사용하여 Apache Spark 및 MLLib으로 매일 밤 데이터를 처리. 모델 결과를 DynamoDB에 게시하여 실시간으로 추천을 생성
	- 트래픽이 간헐적으로 급증하는 상황에서 완전한 실시간 스트리밍 학습은 복잡하고 비용이 많이 들 수 있음
	- **Kinesis Firehose $\rightarrow$ S3:** 실시간 스트리밍 이벤트(클릭 로그)를 안정적으로 수집하여 저렴한 **S3 저장소**에 배치 파일 형태로 적재
	- *EMR Spot Instance + Spark MLLib:** 매일 밤(트래픽 급증이 없는 시간대) **Spot 인스턴스**를 사용하는 EMR 클러스터에서 Spark MLLib으로 추천 모델을 재학습하여 **비용 효율성**을 극대화.
	- **DynamoDB:** 예측 결과(추천 목록/점수)를 **Key-Value NoSQL 데이터베이스**인 DynamoDB에 저장하면, 웹 서버가 실시간 추천 요청에 대해 **초저지연**으로 빠르게 조회하고 응답할 수 있어 성능과 확장성을 확보
   
   
6. You wish to use a SageMaker notebook within a VPC. SageMaker notebook instances are Internet-enabled, creating a potential security hole in your VPC. How would you use SageMaker within a VPC without opening up Internet access?
	   
   - VPC 내 SageMaker 노트북 인스턴스 보안
   - $\rightarrow$ **Disable direct Internet access when specifying the VPC for your notebook instance, and use VPC interface endpoints (PrivateLink) to allow the connections needed to train and host your model. Modify your instance's security group to allow outbound connections for training and hosting.**
	   - VPC 인터페이스 엔드포인트(AWS PrivateLink) 또는 nat gw 사용
   
   
7. You are developing a computer vision system that can classify every pixel in an image based on its image type, such as people, buildings, roadways, signs, and vehicles. Which SageMaker algorithm would provide you with the best starting point for this problem?
   - 컴퓨터 비전 - 픽셀별 분류 (Pixel-wise Classification)
   - $\rightarrow$ **Semantic Segmentation** (시맨틱 분할)
	   - 이미지 전체에서 객체의 위치를 박스로 찾는 **객체 탐지(Object Detection)**가 아님. 
	   - "이미지의 **각 픽셀**을 특정 클래스(사람, 건물 등)로 분류하는 작업"이므로, **시맨틱 분할(Semantic Segmentation)**이 가장 적합
   
   
8. You are training an XGBoost model on SageMaker with millions of rows of training data, and you wish to use Apache Spark to pre-process this data at scale. What is the simplest architecture that achieves this?
   - SageMaker와 Spark를 이용한 대규모 데이터 처리
   - $\rightarrow$ **Use sagemaker_pyspark and XGBoostSageMakerEstimator to use Spark to pre-process, train, and host your model using Spark on SageMaker.** 
   - (sagemaker_pyspark와 XGBoostSageMakerEstimator를 사용하여 Spark를 통해 데이터를 전처리하고, Spark on SageMaker를 사용하여 모델을 훈련하고 호스팅)
	   - **`sagemaker_pyspark` 라이브러리**는 Apache Spark 환경에서 SageMaker를 쉽게 통합할 수 있도록 설계되었음
	   - **`XGBoostSageMakerEstimator`**와 같은 기능을 사용하면, Spark로 대규모 데이터 전처리(Transformation)를 수행한 후, 동일한 파이프라인 내에서 추가적인 설정 복잡도 없이 **SageMaker의 관리형 훈련 및 배포 서비스**를 직접 호출하여 모델을 훈련하고 호스팅할 수 있어 가장 간단하고 효율적인 방법

   
9. A system designed to classify financial transactions into fraudulent and non-fraudulent transactions results in the confusion matrix below. What is the recall of this model?
   - ![](images/Pasted%20image%2020251216012823.png)
   - 90%
	   - **재현율 (Recall)**은 실제 긍정(Positive)인 데이터 중에서 모델이 긍정으로 **올바르게 예측한 비율**
	   - $$\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}} = \frac{90}{90 + 10} = \frac{90}{100} = 0.9 = 90\%$$

9. You are developing a machine learning model to predict house sale prices based on features of a house. 10% of the houses in your training data are missing the number of square feet in the home. Your training data set is not very large. Which technique would allow you to train your model while achieving the highest accuracy?
	- 결측치 보간 (Missing Value Imputation)
	- $\rightarrow$ **Impute the missing square footage values using KNN** (KNN을 사용하여 누락된 면적 값을 보간)
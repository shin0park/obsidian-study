

## SageMaker on the Edge 

- AWS SageMaker는 클라우드에서 학습한 모델을 엣지 디바이스에 배포해, 로컬 데이터 기반 실시간 추론을 가능하게 하는 기능을 제공한다.
- SageMaker는 모델 훈련과 관리를 주로 클라우드에서 수행하는 서비스이지만, 그 결과로 생성된 모델은 종종 리소스가 제한적인 엣지 디바이스에서 실행될 필요가 있다. 
	- 이러한 엣지 디바이스는 IoT 센서, 차량 시스템, 산업용 로봇 등 광범위한 환경을 포괄한다.
	- SageMaker의 목표는 클라우드에서 구축된 복잡한 모델을 이러한 다양한 엣지 환경에 맞게 최적화하고 배포하여, 로컬에서 빠르고 효율적인 추론을 수행하도록 지원하는 것이다.

### SageMaker Neo 

- **개념: 머신러닝 모델을 한 번 학습시키면 어디서든 실행할 수 있게 해주는 기능 “Train once, run anywhere”**  
	- 즉, 한 번 훈련한 모델을 다양한 엣지 환경에서 최적화하여 실행할 수 있게 해주는 서비스
	  
- **대상 디바이스:** ARM, Intel, Nvidia 프로세서를 사용하는 엣지 디바이스(예: 자동차, 임베디드 장치 등)에 최적화
- **기능:** 
	- 특정 디바이스 하드웨어에 맞춰 코드를 최적화한다. 
	- 컴파일러(Compiler)와 런타임(Runtime)으로 구성
		- **컴파일러**: 훈련된 모델의 컴퓨팅 그래프를 분석하여 특정 대상 디바이스에 맞게 코드를 최적화하는 역할을 수행
			- 하드웨어 가속 기능을 최대한 활용할 수 있도록 모델을 변환
		- **런타임**: 컴파일된 모델을 해당 엣지 디바이스에서 효율적으로 로드하고 실행하기 위한 경량화된 환경을 제공
- **지원 프레임워크:** 
	- Tensorflow, MXNet, PyTorch, ONNX, XGBoost, DarkNet, Keras 등 주요 프레임워크를 지원
	- 광범위한 모델 타입을 처리

### Neo + AWS IoT Greengrass 

#### Neo 배포 방식:

두 가지 경로를 통해 운영환경에 배포 가능하다.

1. Neo로 컴파일된 모델을 HTTPS 엔드포인트(C5, M5, P3 인스턴스 등)에 배포
	- 단, 컴파일 시 사용한 인스턴스 타입과 동일해야 함.
		- 이는 단순한 운영상의 규칙이 아니라, Neo가 수행하는 하드웨어 특화 최적화의 결과이다.
		- Neo는 모델을 특정 하드웨어의 명령어 세트 아키텍처(ISA) 및 메모리 구조에 맞춰 성능을 극대화하도록 컴파일한다. 
		- 만약 컴파일된 환경과 다른 인스턴스에 배포할 경우, 최적화된 명령이 올바르게 실행되지 않거나, 예측된 성능을 보장할 수 없어 운영 안정성에 심각한 문제가 발생할 수 있기 때문이다. 

2. **IoT Greengrass**를 통해 실제 엣지 디바이스에 모델을 배포
	- Greengrass는 클라우드에서 훈련된 모델을 현장의 장치로 전달하는 역할을 한다.

#### IoT Greengrass:

- **장점:** 
	- 클라우드에서 학습된 모델을 사용하여 엣지 디바이스에서 로컬 데이터로 직접 추론(Inference)을 수행할 수 있다.
- 추론 작업은 주로 Lambda 추론 애플리케이션(Lambda inference applications)을 활용하여 실행한다.


---

## SageMaker 보안 (SageMaker Security)

### General AWS Security 

- **IAM (Identity and Access Management):** 
	- 사용자 계정 및 권한 관리의 핵심
	- 최소 권한 원칙에 따라 필요한 권한만 부여해야 한다
- **MFA (Multi-Factor Authentication):** 다중 요소 인증을 사용하여 보안을 강화.
- **SSL/TLS:** 모든 연결 시 보안 프로토콜을 사용
- **CloudTrail:** API 호출 및 사용자 활동을 기록(Log)하여 감사
- **암호화 & PII(개인식별정보) 보호:** 데이터 암호화를 적용하고 개인 식별 정보(PII) 관리에 주의해야 한다.

### 저장 데이터 보호 (Data at Rest)

- **KMS (Key Management Service):** 
	- 노트북 및 모든 SageMaker 작업(Job)에서 KMS 키를 통한 암호화를 지원한다
- **S3 암호화:** 
	- 훈련 데이터 및 모델 호스팅을 위해 암호화된 S3 버킷을 사용 가능
	- S3 버킷 자체도 KMS를 활용하여 서버 측 암호화(SSE-KMS)를 적용 가능
- **로컬 스토리지:** 
	- Notebook 인스턴스와 `/opt/ml/`, `/tmp` 디렉터리 하위의 모든 데이터는 KMS 키로 암호화 가능하다

### **전송 중 데이터 보호 (Data in Transit)**

- **기본 암호화:** SageMaker 환경 내외부로 전송되는 데이터는 TLS/SSL 프로토콜을 통해 보호된다.
	- 모든 트래픽 TLS/SSL 지원
- **IAM 역할:** SageMaker에 IAM 역할을 할당하여 리소스 접근 권한을 부여
  
- 분산 훈련 
	- 노드 간(inter-node) 통신 즉, 컨테이너 간 트래픽 암호화(inter-container traffic encryption)를 선택적으로 활성화 가능하다
		- 이 옵션은 보안 수준을 높이지만, 암호화 및 복호화 오버헤드 때문에 **딥러닝 작업 시 훈련 시간과 비용을 증가시킬 수 있다**는 점을 고려해야 한다.
		- 이 기능은 보안 요구 사항이 매우 높은 환경에서 신중하게 활성화되어야 한다.
	- 훈련 또는 튜닝 작업을 설정할 때 콘솔 또는 API를 통해 구성된다.


---

### **SageMaker와 VPC (Virtual Private Cloud)**

- **VPC 활용:** 
	- 훈련 작업은 기본적으로 VPC 내에서 실행되므로 기본적인 네트워크 격리 이점을 누릴 수 있다. 
	- 더 높은 수준의 보안이 필요할 경우, 사용자 지정 프라이빗 VPC를 사용할 수 있다.
- **S3 접근:** 
	- 프라이빗 VPC를 사용한다면,
		- 훈련 데이터와 모델을 저장하는 S3에 접근하기 위해 **S3 VPC 엔드포인트** 를 설정해야한다 
		- 이러한 접근은 사용자 지정 엔드포인트 정책과 S3 버킷 정책을 통해 세부적으로 통제되고 보안이 유지가능하다.
- **인터넷 접근 제어:**
    - Notebook, Training / Inference 컨테이너는 기본적으로 인터넷 연결이 활성화되어 있어 보안 취약점이 될 수 있다. → 필요 시 비활성화
	    - 특히 Notebook의 경우, 이는 보안 취약점(security hole)이 될 수 있다.
    - 인터넷 연결을 비활성화할 경우, 훈련 및 호스팅이 작동하려면 **VPC 인터페이스 엔드포인트(PrivateLink)** 또는 **NAT 게이트웨이**가 필요하다.
    - 최고 수준의 격리를 제공하는 네트워크 격리(Network Isolation) 옵션도 있지만, 이는 S3 접근까지 차단됨을 유의해야 한다.
	    - 훈련 데이터의 입출력 방식에 대한 별도의 설계가 필요

---

### SageMaker와 IAM 권한

- **주요 작업 권한:** 
	- CreateTrainingJob
	- CreateModel
	- CreateEndpointConfig
	- CreateTransformJob
	- CreateHyperParameterTuningJob
	- CreateNotebookInstance 등
    
- **사전 정의된 정책 (Managed Policies):**
    - `AmazonSageMakerReadOnly`: 읽기 전용
    - `AmazonSageMakerFullAccess`: 전체 접근
    - `AdministratorAccess`: 관리자 권한
    - `DataScientist`: 데이터 과학자용 권한
    
---

### 로깅 및 모니터링 (Logging and Monitoring)

- **CloudWatch:**
    - 엔드포인트의 호출(Invocations) 및 지연 시간(Latency) 모니터링.
    - 인스턴스 노드의 상태(CPU, 메모리 등) 확인
    - **Ground Truth:** 활성 작업자 수 및 작업량 모니터링

- **CloudTrail:** 
	- SageMaker 내의 사용자, 역할, 서비스의 모든 API call 기록
	- 로그 파일은 S3로 전달되어 저장되며, 이는 규정 준수 및 보안 감사(auditing) 목적으로 사용된다.

---
### SageMaker 리소스 관리 (Managing Resources)

#### **인스턴스 유형 선택**

- **학습 (Training):** 딥러닝 알고리즘은 GPU 인스턴스(P3, G4dn 등)가 유리
- **추론 (Inference):** 학습보다 리소스가 덜 필요하므로 컴퓨팅 인스턴스(C5 등)로 충분한 경우가 많다. 

-> GPU 인스턴스는 높은 성능을 제공하지만, 비용이 매우 비싸다는 점(really pricey)을 항상 고려해야 된다.
    

#### **관리형 스팟 학습 (Managed Spot Training)**

- **비용 절감:** EC2 스팟 인스턴스를 사용하여 온디맨드 대비 최대 90% 비용을 절감할 수 있다.
- **주의사항:** 
	- AWS의 미사용 용량을 활용하므로 수요에 따라 언제든지 중단(interrupted)될 수 있다는 위험을 내포하고 있다.
- **해결책:** 
	- S3에 **체크포인트(Checkpoints)** 를 저장하도록 설정하여, 중단 시 학습을 이어서 진행할 수 있게 해야 한다. 
		- 복구 메커니즘은 장시간 훈련에 필수적이다.
		- 스팟 인스턴스 가용성을 기다려야 하므로, 훈련 작업의 총 소요 시간이 증가할 수 있다는 점 또한 운영상의 고려 사항이다.
		- Managed Spot Training은 하이퍼파라미터 튜닝과 같은 자동 모델 튜닝 작업에도 적용될 수 있으며, 관련 지표 및 로그는 CloudWatch에서 확인 가능하다
    

#### **엘라스틱 인퍼런스 (Elastic Inference - EI) 를 활용한 추론 가속**

- 딥러닝 추론을 가속화하는 동시에, 전체 GPU 인스턴스를 사용하는 것보다 훨씬 적은 비용(fraction of cost)으로 구현할 수 있는 비용 효율적인 솔루션이다.
- **방식:** EI 아키텍처는 EI 가속기(accelerators)를 표준 CPU 인스턴스(예: `ml.eia1.medium/large/xlarge`)에 보조적으로 추가하여 가속 기능을 제공한다.
- **지원:** 
	- TensorFlow, PyTorch, MXNet 사전 빌드 컨테이너와 함께 작동한다.
	- Notebook에도 사용 가능
	- ONNX를 사용하여 모델을 MXNet으로 내보낼 수 있으며, Image Classification 및 Object Detection과 같은 내장 알고리즘도 지원한다.
	- Custom Container에서도 EI-enabled 환경 가능

---

### Automatic Scaling 

- 자동 확장은 프로덕션 엔드포인트의 부하에 따라 인스턴스 수를 동적으로 조정하는 기능
- **설정:** 
	- CloudWatch 지표(Metrics)를 기반으로 목표값, 최소/최대 용량, 쿨다운 기간을 정의하는 조정 정책(Scaling Policy)을 설정
	- 이 기능은 CloudWatch와 연동되어 모니터링 지표에 반응하며 확장된다.
- **필수:** 적용 전 부하 테스트(Load test)가 반드시 필요
    

### **서버리스 추론 (Serverless Inference)**

- 2022년에 도입
- **특징:** 컨테이너 이미지, 메모리, 동시성 요구사항만 지정하면 기본 용량이 자동으로 프로비저닝되고 확장되는 배포 방식
- **적합한 사례:** 
	- 트래픽이 간헐적이거나 예측하기 어려운 경우에 적합. 
	- 요청이 없으면 **0으로 축소(Scale down to zero)** 된다.
- 비용: 사용량(밀리초 단위)에 따라 과금
- 모니터링: CloudWatch(ModelSetupTime, MemoryUtilization 등)로 모니터링

---

### **Amazon SageMaker Inference Recommender**

- 모델 배포를 위한 최적의 인스턴스 유형과 구성을 추천하여 운영 최적화를 자동화하는 도구
- **기능:** 모델에 가장 적합한 인스턴스 유형과 endpoint 구성을 추천하고, 부하 테스트 및 튜닝을 자동화한다.
- **Workflow:** 
	1. 모델을 Model Registry에 등록
	2. 여러 endpoint 구성을 벤치마크
	3. 다양한 엔드포인트 구성에 대해 벤치마킹을 수행하고 지표를 수집
	4. 모델 튜닝을 위한 로드 테스트를 자동화하며, 다음과 같은 두 가지 유형의 추천을 제공:   
		1. **인스턴스 추천 (Instance Recommendations):** 
			- 추천된 인스턴스 유형에 대해 로드 테스트를 실행하며 약 45분이 소요
		2. **엔드포인트 추천 (Endpoint Recommendations):** 
			- 사용자 정의 로드 테스트로, 사용자가 인스턴스, 트래픽 패턴, 지연 시간, 처리량 요구 사항을 지정하며 약 2시간이 소요 
	5. 추천 결과는 `CostPerHour`, `CostPerInference`, `ModelLatency` 등 상세한 지표를 포함하여 최적의 구성을 결정하는 데 필요한 데이터를 제공

---

### 가용성 영역 (Availability Zones)

- **고가용성:** SageMaker는 엔드포인트 인스턴스를 자동으로 AZ 분산
	- 단, **여러 인스턴스**를 배포해야 AZ 분산이 적용됨
	  
- **VPC 구성:** 최소 두 개의 서브넷을 각각 다른 AZ에 위치하도록 구성해야한다.

---

### SageMaker와 MLOps

MLOps는 머신러닝 시스템의 배포, 모니터링 및 유지 관리를 자동화하고 표준화하는 엔지니어링 관행이다.

#### **Kubernetes와의 통합**

- SageMaker는 기존의 Kubernetes 기반 ML 인프라와의 통합을 지원하여, 조직이 클라우드와 온프레미스 환경을 유연하게 결합할 수 있도록 돕는다.
    
- **도구:**
    - **SageMaker Operators for Kubernetes:** 
	    - ![](images/Pasted%20image%2020251201225440.png)
	    - Kubernetes 기반 ML 인프라와 SageMaker 서비스를 통합하는 데 사용된다.
	    - 즉, Kubernetes에서 SageMaker 작업(학습, 튜닝, 추론)을 제어할 수 있게 해준다.
		    - EKS 클러스터 내에서 SageMaker 작업을 Kubernetes CRD(Custom Resource)로 정의
		    - Kubernetes에서 직접 SageMaker Training/Inference 자원 생성 및 관리
		    - EKS Worker Node에서 SageMaker Operator가 요청을 SageMaker로 전달
    - **SageMaker Components for Kubeflow Pipelines:** 
	    - ![](images/Pasted%20image%2020251201225506.png)
	    - **Kubeflow Pipelines**의 구성 요소와 통합되어, 기존에 Kubeflow 위에 구축된 ML 플랫폼을 클라우드 환경으로 확장하거나 결합하는 것을 가능하게 한다.
	    - Kubeflow 파이프라인의 각 단계(전처리, 학습, 튜닝, 배포 등)에서 SageMaker 기능을 수행하도록 지원한다.
		    - Processing
		    - Hyperparameter Tuning
		    - Training
		    - Inference 
			    - → 모든 단계가 SageMaker 리소스를 활용해 실행
	    

---

### SageMaker Projects

![](images/Pasted%20image%2020251201225921.png)

- SageMaker Studio의 **네이티브 MLOps(CI/CD) 솔루션**
- **기능:** SageMaker Studio 내의 기본 MLOps 솔루션으로 CI/CD(지속적 통합/배포)를 제공한다.
- **워크플로:** 
	- 이미지 빌드 -> data 준비, feature engineering -> Train models -> Evaluate models -> Deploy models -> 모니터링의 전체 수명 주기를 관리한다.
- **구성 요소:** 
	- 코드 리포지토리(CodeCommit/GitHub 등)와 SageMaker Pipelines를 사용하여 모델 구축 및 배포 자동화를 정의

---

### **추론 파이프라인 (Inference Pipelines)**

- 추론 파이프라인은 단일 엔드포인트 내에서 여러 처리 단계를 순차적으로 실행할 수 있도록 설계된 아키텍처이다.
- **구조:** 2개에서 최대 15개까지의 컨테이너를 선형 순서(Linear sequence)로 연결하여 처리
- **활용:** 
	- Pre-processing → Model inference → Post-processing
	- 전처리 -> 추론 -> 후처리 위 과정을 하나의 엔드포인트로 묶을 수 있다.
		- 실제 운영 환경에서 모델의 입력 데이터는 종종 정규화나 특징 추출과 같은 복잡한 전처리 과정을 거쳐야 한다.
		- 추론 파이프라인은 이러한 전처리 로직, 모델 추론, 그리고 최종 결과 정제 과정을 하나의 관리형 엔드포인트로 캡슐화한다. 
		- 이는 클라이언트 측 애플리케이션의 복잡성을 줄이고, ML 시스템 내에서 전처리 로직의 버전 관리를 통합하여 MLOps 효율성을 크게 향상시킨다.
- **호환성:** 
	- 내장 알고리즘과 사용자 지정 Docker 컨테이너를 조합 가능 
	- Spark ML이나 Scikit-learn 컨테이너도 사용 가능
		- Spark ML은 Glue 또는 EMR과 함께 실행 가능, 모델은 MLeap 형식으로 직렬화된다.
- **지원:** 
	- 실시간 추론(Real-time inference)과 배치 변환(Batch transform)을 모두 지원


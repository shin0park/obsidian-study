## **SageMaker Canvas**

**SageMaker Canvas 개요**

- **비즈니스 애널리스트를 위한 No-code(코드 없는) 머신러닝 도구**
    - 프로그래밍 지식이 없어도 예측 모델을 만들 수 있음
    - 사용자 친화적인 시각적 인터페이스 제공
    - Canvas는 데이터 준비부터 모델 구축, 예측 생성 및 배포에 이르는 전체 ML 워크플로우를 포괄적으로 지원
    - Canvas는 내부적으로 SageMaker Autopilot의 자동 머신러닝(AutoML) 기능을 활용하여, 사용자가 데이터와 타겟 변수를 지정하면 자동으로 여러 알고리즘을 훈련하고 하이퍼파라미터 최적화(Hyperparameter Optimization)를 수행하여 가장 정확한 맞춤형 모델을 생성
    - 최근 Amazon Q Developer와 통합되어, 사용자가 자연어로 비즈니스 목표를 설명할 경우, Q Developer가 데이터 준비 및 모델 구축 과정을 안내하고 지원하는 기능까지 제공한다.
      
- 초기 CSV 형식의 데이터 업로드만 지원
	- 현재 Amazon S3, Amazon Redshift, Amazon Athena, Snowflake 등 50개 이상의 다양한 데이터 소스로부터 데이터를 가져와 통합하는 광범위한 기능을 제공
    
- **예측 대상 컬럼을 선택 → 자동으로 모델 생성 및 예측 수행**
	- Target column을 선택
	- 타켓에 따라 자동으로 지도 학습(Supervised Learning) 모델 유형을 식별
		- **분류(Classification)** 또는 **회귀(Regression)** 문제 지원
- **데이터셋 병합 기능**도 제공
- **자동 데이터 정제(Auto data cleaning)** 기능 포함
    - 결측값(Missing values) 처리: 누락된 데이터 포인트의 감지 및 처리
    - 이상치(Outliers) 처리: 데이터 분포에서 크게 벗어나 모델 정확도를 저해할 수 있는 값의 식별 및 조정
    - 중복값(Duplicates) 제거
      
- SageMaker Data Wrangler와의 통합: 
	- 최근 통합하면서 데이터 준비 기능을 확장함.
	- 사용자는 300개 이상의 내장된 변환 및 분석 기능을 활용하여 데이터 품질을 개선하고 모델 성능을 향상시킬 수 있다.`
	- Amazon Q Developer를 활용하여 자연어 명령을 통해 복잡한 데이터 변환을 수행 가능 하다.
	  
- **SageMaker Studio와의 연동**
    - 생성한 모델과 데이터셋을 공유 가능
    - 비즈니스 사용자가 빠르게 구축한 노코드 모델을 전문 데이터 과학자가 Studio 환경으로 가져와 코드 수준의 정밀한 검토, 추가 미세 조정, 또는 복잡한 MLOps 파이프라인으로의 통합을 수행 가능해졌다.

### **SageMaker Canvas Demo**

- 사용자는 데이터를 업로드하고, 예측할 타깃 컬럼을 지정한 후  
	모델을 생성 및 결과를 시각적으로 확인할 수 있음
	
- 완전히 GUI 기반이므로, 데이터 과학자가 아닌 비즈니스 담당자도 활용 가능

- 모델 훈련 완료시, Canvas는 SageMaker Autopilot이 생성한 모델 성능 보고서와 데이터 탐색 보고서를 통해 모델의 효과를 평가할 수 있다.
	- **배치 예측 (Batch Prediction):** 새로운 대규모 데이터셋(예: CSV 파일)을 업로드하여 한 번에 예측을 수행. 이 방식은 대량의 데이터에 대해 주기적으로 예측이 필요할 때 유용
    
	- **단일 예측 (Single Prediction):** 사용자 인터페이스에 특정 데이터 포인트를 수동으로 입력하여 실시간으로 예측 결과를 얻는다. 이 방식은 예측의 확률 점수(Confidence Score)를 즉시 확인하고 해석하는 데 사용


### **SageMaker Canvas – The Finer Points (세부 사항)**

SageMaker Canvas는 비즈니스 사용자가 쉽게 접근할 수 있도록 설계되었지만, 그 운영 환경은 IT 관리자의 세밀한 구성 및 관리를 필요로 한다.

- 로컬 파일 업로드 및 S3/CORS 설정:
	- **로컬 파일 업로드 설정**은 조직의 IT 관리자가 사전에 구성해야 함
	- Canvas는 데이터를 안전하게 저장하고 처리하기 위해 Amazon S3 버킷을 활용
	- **S3 버킷 연결** 시 데이터에 안전하게 접근할 수 있도록 적절한 CORS(Cross-Origin Resource Sharing) 권한 설정 필요
    
- **Okta SSO(Single Sign-On)** 통합 가능
    
- Canvas는 SageMaker Domain 내에서 실행되는 애플리케이션이며, 이 도메인은 최신 기능 및 보안 환경 유지를 위해 관리자에 의해 **수동으로 업데이트(manually updated)** 되어야 한다.
	- 메인은 Canvas를 비롯한 다양한 SageMaker ML 환경을 지원하며, 사용자 권한 및 리소스 격리를 위한 기본 경계 역할을 한다.
    
- 외부 데이터 소스 및 IAM 연동
	- **Amazon Redshift**로부터 직접 데이터 가져오기(import) 가능
	- 시계열 데이터(Time series)를 사용하여 미래를 예측하는 기능을 활용하기 위해서는 IAM 정책을 통해 해당 기능이 명시적으로 활성화 되어야 한다.
    
- **VPC 환경 내 실행** 지원 (보안 네트워크 환경에서의 사용 가능)
	- 보안 및 규정 준수 요구사항이 높은 환경에서 Canvas를 운영하기 위해, Canvas는 고객 소유의 **VPC(Virtual Private Cloud) 내에서 실행**될 수 있다.

- **요금 구조:**
	- (강의)
	    - 시간당 **$1.90**
	    - 모델 내 **training cell 수에 따라 추가 요금 부과**
    - (AWS 문서)
	    - https://aws.amazon.com/sagemaker/ai/canvas/pricing/
	    - **Workspace 인스턴스 (Session-Hrs):** 
		    - 사용자가 Canvas 애플리케이션에 로그인하여 사용하는 시간에 따라 부과되는 시간당 요금
		    - 세션은 애플리케이션을 시작할 때 시작되며, 사용자가 명시적으로 로그아웃하거나 관리자가 세션을 종료할 때까지 지속되므로, 사용 시간을 관리하는 것이 비용 관리에 중요
		      
		- **모델 훈련 요금 (Model Training Charges):** 훈련 요금은 모델 유형에 따라 다르게 계산
		    - **표 형식 모델 (Tabular Models):** 분류(Classification), 회귀(Regression), 시계열(Time-series) 예측과 같은 표 형식 데이터 모델의 경우, 훈련에 사용된 데이터셋의 **총 셀 수(number of cells)**를 기준으로 요금이 부과
	        
		    - **컴퓨터 비전(CV) 및 자연어 처리(NLP) 모델:** 이러한 복잡한 모델의 훈련 요금은 모델을 훈련하는 데 소요된 **컴퓨팅 시간(compute time)**을 기준으로 부과되며, 이는 SageMaker 훈련 인스턴스의 시간당 요금(예: ml.g4dn.12xlarge의 시간당 $4.89 등)을 따른다.
---

## **SageMaker Training Compiler**

**SageMaker Training Compiler란?**
- 딥러닝(DL) 모델의 훈련 속도를 가속화하기 위해 GPU 인스턴스에서 훈련 작업을 컴파일하고 최적화하는 기능
- AWS Deep Learning Containers(DLCs)에 **통합되어 제공**
    → 사용자 지정 컨테이너(Bring Your Own Container, BYOC) 사용이 불가라 되어있지만, AWS DLC를 기반으로 **확장된 사용자 지정 컨테이너는 지원될 수 있음

- 주요 목표는 GPU 인스턴스의 활용 효율을 극대화하여 훈련 성능을 높이는 것 
	- **학습 속도 최대 50%까지 향상 가능
	- **하드웨어 최적화:** 
		- 딥러닝 모델을 고수준 언어 표현(PyTorch, TensorFlow)에서 **하드웨어에 최적화된 명령어(hardware-optimized instructions)**로 변환
	- **그래프 수준 최적화:** 
		- 연산자 융합(Operator Fusion), 메모리 계획(Memory Planning) 등의 그래프 수준 최적화가 포함되어 GPU 메모리 사용 효율을 높임
	- **배치 크기 증가:** 
		- 메모리 사용 효율이 개선되면 훈련 중 더 큰 배치 크기(batch size)를 사용할 수 있게 되며, 이는 GPU 활용률을 높여 전반적인 훈련 속도를 가속화

- **GPU 인스턴스**(예: `ml.p3`, `ml.p4`)에서 학습 작업을 **컴파일 및 최적화**
    
- **Hugging Face Transformers 라이브러리**와 통합 테스트 완료
	- 사용자가 직접 개발한 모델(Bring your own model)의 통합도 지원

- **SageMaker 분산 학습(distributed training) 라이브러리와는 호환되지 않음.**
	- SageMaker의 분산 훈련 라이브러리는 수백 또는 수천 개의 가속기를 사용하는 대규모 언어 모델(LLM)과 같이 대규모 모델 병렬화가 필수적인 시나리오를 위해 설계.
	- 훈련 작업을 여러 GPU 또는 인스턴스에 걸쳐 분산해야 하는 경우, 컴파일러 대신 SageMaker 분산 훈련 라이브러리를 사용해야 한다.
    

**모범 사례 (Best Practices):**

1. GPU 인스턴스(`ml.p3`, `ml.p4`) 사용 보장
2. PyTorch 모델은 **PyTorch/XLA의 모델 저장 함수(model save function)** 로 저장해야 함
	1. 이는 컴파일러가 XLA(Accelerated Linear Algebra) 기반의 최적화를 수행하기 위해 필요하다.
3. 디버깅 활성화: 컴파일러를 통한 훈련 작업에 문제가 발생할 경우, 디버깅을 위해 `compiler_config`의 `debug` 플래그를 활성화
	1. 예: `compiler_config=TrainingCompilerConfig(debug=True)`
	2. 다만, 디버깅 로깅을 활성화하면 계산 성능에 오버헤드가 추가되어 훈련 시간이 늘어날 수 있다는 점을 고려해야 한다.
4. **하이퍼파라미터 재조정:** 
	1. 컴파일러는 모델의 메모리 사용 특성을 변경하므로, 최적화된 환경에 맞춰 배치 크기(batch size)와 학습률(learning rate)을 재조정하는 것이 성능을 극대화하는 데 중요하다
	2. 컴파일 자체에 초기 오버헤드가 발생하므로, 컴파일러는 오버헤드를 상쇄할 만큼 충분히 긴 시간 동안 실행되는 훈련 작업에 가장 효과적이다.

---

## SageMaker Feature Store

SageMaker Feature Store 개요

- **Feature(특징)**: 머신러닝 모델을 학습하기 위해 사용하는 데이터 속성
    예: 개인의 정치 성향을 예측할 때, 주소, 소득, 나이 등이 “특징”이 된다.
    
- **문제점**: 모델 학습 시 빠르고 보안이 유지된 Feature 접근이 필요하지만,  
    여러 모델에서 같은 Feature를 공유하거나 관리하기가 어렵다.
    
- **해결책**: **SageMaker Feature Store**는 Feature 데이터를 중앙에서 관리하고,  
    학습 및 추론 시 일관된 데이터를 제공하는 **전용 저장소(Repository)**


### Where the features come from is up to you

![](images/Pasted%20image%2020251021013102.png)

Feature Store에 저장되는 Feature의 출처는 사용자가 직접 결정한다.

- **특징 작성 (Feature Authoring):** SageMaker Data Wrangler, Glue, DataBrew와 같은 도구를 사용하여 특징을 만든다.
    
- **특징 처리 (Feature Processing):** EMR, Glue, SageMaker Processing, 스트리밍(Kinesis, Kafka) 등을 통해 특징 데이터를 처리
    
- **특징 검색 (Feature Discovery):** SageMaker Studio를 통해 특징을 찾는다.
    
- **Feature Store:** 처리된 특징을 저장
    
- **배포:** 저장된 특징은 다음 용도로 사용
    - **온라인 추론 (Online Inference):** 
	    - SageMaker Hosting, Lambda를 통해 실시간 추론에 사용
    - **훈련 및 배치 스코어링 (Training, Batch Scoring):** 
	    - SageMaker Training, SageMaker Batch Transform에 사용
	    - 배치 접근 시에는 Athena를 Offline S3 Store와 함께 사용할 수 있다.
        
- **Feature Pipeline:** Step Functions, SageMaker Pipelines, 또는 Apache Airflow와 같은 도구를 사용하여 전체 파이프라인을 구축한다. 


### How SageMaker Feature Store Organizes Your Data

데이터 구성 - SageMaker Feature Store는 데이터를 계층적으로 구성

![](images/Pasted%20image%2020251021013126.png)

1. **Feature Store:** 최상위 저장소
2. **Feature Group (특징 그룹):** 유사한 특징들을 논리적으로 그룹화한 단위
3. **Record (레코드):** 
    - Feature identifier (특징 식별자)
    - Feature name (특징 이름)
    - Event time (이벤트 시간)

### Data Ingestion (streaming or batch)

데이터 적재

데이터 수집은 스트리밍 또는 배치 방식으로 이루어질 수 있으며, Spark, Data Wrangler와 같은 도구 및 Amazon Kinesis 또는 Apache Kafka를 통해 Feature Store로 데이터가 유입된다.

![](images/Pasted%20image%2020251021013138.png)

두 가지 유형의 스토어 제공

- **Online Store**
    - 스트리밍 streaming 접근을 위해 사용
    - 실시간 API(`PutRecord`, `GetRecord`)로 접근 가능
    - 예: 모델 실시간 추론용
        
- **Offline Store (Amazon S3 기반)**
    - 배치(Batch) 접근용, 장기 보존 및 대규모 분석에 적합
    - 저장위치: S3
    - 접근 방식: S3에 접근하는 모든 서비스(예: Athena, Data Wrangler)와 함께 사용할 수 있음
    - 통합: 자동으로 Glue Data Catalog를 생성
        
- **데이터 소스 예시**
    - Amazon Kinesis
    - Amazon MSK (Managed Streaming for Apache Kafka)
    - Apache Spark, SageMaker Data Wrangler 등


### SageMaker Feature Store Security

- **암호화:** 전송 중(in transit) 및 저장 중(at rest) 모두 암호화
- **KMS (AWS Key Management Service)** 기반 고객 관리형 키 사용 가능
- **IAM을 통한 세분화된 접근 제어**
- 네트워크 보안을 위해 **AWS PrivateLink** 연동 가능

---

## **SageMaker ML Lineage Tracking**


- **ML Lineage Tracking**은 SageMaker 내 MLOps 워크플로우를 추적 및 기록하는 기능이다.
- 모델의 생성·변경·배포 이력을 관리하며, **감사(Audit)**와 **컴플라이언스(Compliance)** 에 활용된다.
- **Lineage Metadata:** SageMaker는 워크플로우를 추적하는 계보 엔티티 메타데이터의 연결 그래프를 자동으로 생성한다.
- **통합:** 계정 간 계보 추적(cross-account lineage)을 위해 AWS Resource Access Manager와 통합

![](images/Pasted%20image%2020251021013228.png)

#### Lineage Tracking Entities

| **엔티티 (Entity)**           | **설명 (Description)**                           |
| -------------------------- | ---------------------------------------------- |
| Trial Component (시행 구성 요소) | 처리 작업, 훈련 작업, 변환 작업과 같은 개별 작업 단위               |
| Trial (시행)                 | Trial Component들로 구성된 모델을 의미                   |
| Experiment (실험)            | 특정 목적의 여러 Trial 집합                             |
| Context (컨텍스트)             | 엔티티들의 논리적 그룹핑                                  |
| Action (액션)                | 워크플로우 단계(예: 모델 배포)                             |
| Artifact (아티팩트)            | 실제 객체 (S3 데이터, ECR 이미지 등)                      |
| Association (연관)           | 엔터티 간 연결 관계 (유형: ContributedTo, DerivedFrom 등) |

#### Querying Lineage Entities

- Python용 **SageMaker SDK**에서 **LineageQuery API** 제공  
    → 모델, 엔드포인트 등과 관련된 모든 엔터티를 검색 가능
- 예: 특정 S3 버킷(Artifact)을 사용하는 모든 모델 찾기
- 시각화도 가능 (Visualizer 클래스 필요)


---

## SageMaker Data Wrangler

- 머신러닝을 위한 데이터를 준비하는 데 사용되는 **시각적 인터페이스**로 데이터 준비(전처리)를 수행
- SageMaker Studio 내에서 작동하며, 데이터 과학자가 코드를 작성하지 않고도 데이터 준비 작업을 수행할 수 있도록 지원한다.
    
- 기능:
    - 데이터 가져오기(Import)
    - 데이터 시각화(Visualize)
    - 데이터 변환(Transform) 
	    - 300개 이상의 내장 변환(transformations) 중에서 선택하거나, Pandas, PySpark, PySpark SQL을 사용하여 사용자 정의 변환(custom xforms)을 통합
    - **Quick Model**: 간단한 모델 학습 및 성능 평가 기능 제공
        
### Data Wrangler Sources

![](images/Pasted%20image%2020251021013401.png)

→ 다양한 AWS 및 외부 데이터 소스에서 데이터를 가져올 수 있음

### Data Wrangler Workflow

| 단계                   | 설명                                                               |
| -------------------- | ---------------------------------------------------------------- |
| **Import Data**      | 데이터셋을 불러오기                                                       |
| **Preview Data**     | 데이터 미리보기 및 품질 확인                                                 |
| **Visualize Data**   | 분포 및 관계를 파악하기 위해 데이터를 그래프로 표현                                    |
| **Transform Data**   | 데이터 정제, 엔지니어링 및 전처리를 적용하는 핵심 단계<br>정규화, 인코딩, 결측치 처리 등 300+ 변환 수행 |
| **Quick Model**      | 간단한 모델 훈련 및 성능 측정                                                |
| **Export Data Flow** | 전체 변환 흐름(Flow)을 저장 또는 SageMaker Pipeline으로 내보내기                  |

### Data Wrangler Troubleshooting

- **IAM 권한 확인**
    - Studio 사용자는 **AmazonSageMakerFullAccess** 정책 필요
      
- **데이터 소스 권한:** 데이터 소스의 권한이 Data Wrangler 접근을 허용하는지 확인
        
- **EC2 인스턴스 한도 초과 오류**
    - 예: “The following instance type is not available…”
    - → **Service Quotas** 콘솔에서 **ml.m5.4xlarge** 인스턴스 한도 증가 요청 필요
        

---
### 요약

| 항목                          | 핵심 기능                        |
| --------------------------- | ---------------------------- |
| **SageMaker Feature Store** | Feature의 중앙 관리 및 실시간/배치 접근   |
| **ML Lineage Tracking**     | 모델 개발·운영 전 과정 추적 - MLOps     |
| **Data Wrangler**           | GUI 기반 데이터 전처리 및 통합 파이프라인 생성 |

---


### Amazon S3 Overview

Amazon S3는 사용자가 "buckets"(directories)에 objects(files)를 저장할 수 있도록 한다.

- 버킷은 전역적으로 고유한 이름을 가져야 한다.
- objects(files)는 Key를 가진다. 키는 전체 경로이다.
    - `<my_bucket>/my_file.txt`  
    - `<my_bucket>/my_folder1/another_folder/my_file.txt`  
- 최대 객체 크기는 5TB이다.
- Object Tags(키/값 쌍, 최대 10개)는 보안 및 수명 주기 관리에 유용하다.
    
### Amazon S3 for Machine Learning

S3는 많은 AWS ML 서비스(SageMaker)의 Backbone 역할을 한다.  

- Create a “Data Lake”
    - 무한한 크기, 프로비저닝 불필요  
    - 99.999999999%의 내구성  
	- 스토리지(S3)와 컴퓨팅(EC2, Amazon Athena, Amazon Redshift Spectrum, Amazon Rekognition, AWS Glue)의 분리  
- 중앙 집중식 아키텍처  
- 객체 스토리지이므로 모든 파일 형식을 지원한다.
    - ML을 위한 일반적인 형식: CSV, JSON, Parquet, ORC, Avro, Protobuf  
        
### Amazon S3 Data Partitioning

데이터 파티셔닝은 range 기반의 쿼리의 속도를 높이는 패턴이다. (ex: AWS Athena)
- 날짜별: `s3://bucket/my-data-set/year/month/day/hour/data_00.csv`  
- 제품별: `s3://bucket/my-data-set/product-id/data_32.csv`  
- 다양한 전략 정의 가능, Glue 등을 통해 자동화 가능

---
### S3 Storage Classes

Amazon S3는 다양한 스토리지 클래스를 제공한다.

- Amazon S3 Standard - General Purpose 표준  
- Amazon S3 Standard-Infrequent Access (IA)  - 엑세스 빈도 낮음
- Amazon S3 One Zone-Infrequent Access  - 단일 가용영역용 IA
- Amazon S3 Glacier Instant Retrieval   - 아카이빙용
- Amazon S3 Glacier Flexible Retrieval  
- Amazon S3 Glacier Deep Archive  
- Amazon S3 Intelligent-Tiering  - 사용패턴에 따라 자동 계층 이동
- 수동으로 또는 S3 수명 주기 구성(Lifecycle configurations)을 사용하여 클래스 간에 이동할 수 있다.

### S3 Durability and Availability

- **내구성 (Durability):**  
    - 여러 AZ(가용 영역)에 걸쳐 객체의 높은 내구성(99.999999999%, 11 9's)을 제공
    - Amazon S3에 1,000만 개의 객체를 저장하는 경우, 평균적으로 10,000년에 한 번 객체 손실이 발생
    - 모든 스토리지 클래스에 동일하게 적용
        
- **가용성 (Availability):**  
    - 서비스를 얼마나 쉽게 사용할 수 있는지를 측정
    - 스토리지 클래스에 따라 다르다.
    - 예: S3 Standard는 99.99%의 가용성을 가지며, 이는 연간 53분 동안 사용할 수 없음을 의미한다.
        
#### S3 Standard – General Purpose

- 99.99% 가용성  
- 자주 액세스하는 데이터에 사용
- 낮은 지연 시간과 높은 처리량을 제공
- 두 개의 동시 시설 장애를 견딜 수 있다
- 사용 사례: 빅 데이터 분석, 모바일 및 게임 애플리케이션, 콘텐츠 배포 등  
    
#### S3 Storage Classes – Infrequent Access

덜 자주 액세스하지만 필요할 때 빠른 액세스가 필요한 데이터에 사용

- S3 Standard보다 저렴하다.
- **Amazon S3 Standard-Infrequent Access (S3 Standard-IA):**  
    - 99.9% 가용성  
    - 사용 사례: 재해 복구, 백업  
        
- **Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA):**  
    - 단일 AZ에서 높은 내구성(99.999999999%)을 제공하지만, AZ가 파괴되면 데이터가 손실된다
    - 99.5% 가용성  
    - 사용 사례: 온프레미스 데이터의 보조 백업 사본 또는 다시 생성할 수 있는 데이터 저장  
        
#### Amazon S3 Glacier Storage Classes

아카이빙/백업을 위한 저비용 객체 스토리지

- 가격: 스토리지 비용 + 객체 검색 비용  
- **Amazon S3 Glacier Instant Retrieval:**  
    - 밀리초 단위 검색, 분기별 한 번 액세스하는 데이터에 적합
    - 최소 스토리지 기간 90일  
        
- **Amazon S3 Glacier Flexible Retrieval (이전 Amazon S3 Glacier):**  
    - 신속(1~5분), 표준(3~5시간), 대량(5~12시간) 검색 옵션 제공  
    - 최소 스토리지 기간 90일  
        
- **Amazon S3 Glacier Deep Archive - 장기 스토리지용:**  
    - 표준(12시간), 대량(48시간) 검색 옵션 제공  
    - 최소 스토리지 기간 180일  

#### S3 Intelligent-Tiering

- 소액의 월별 모니터링 및 자동 계층화 수수료가 부과된다.
- 사용량에 따라 객체를 액세스 계층 간에 자동으로 이동한다.
- S3 Intelligent-Tiering에서는 검색 요금이 없다.

- **자주 액세스 계층 (Frequent Access tier) (자동):** 기본 계층  
- **자주 액세스하지 않는 계층 (Infrequent Access tier) (자동):** 30일 동안 액세스되지 않은 객체  
- **아카이브 즉시 액세스 계층 (Archive Instant Access tier) (자동):** 90일 동안 액세스되지 않은 객체  
- **아카이브 액세스 계층 (Archive Access tier) (선택 사항):** 90일에서 700일 이상으로 구성 가능  
- **Deep Archive 액세스 계층 (Deep Archive Access tier) (선택 사항):** 180일에서 700일 이상으로 구성 가능  


![](images/Pasted%20image%2020250714142554.png)
![](images/Pasted%20image%2020250714142606.png)
외울 필요는 없음

---
#### Amazon S3 – Moving between Storage Classes

![400](images/Pasted%20image%2020250714142930.png)

- 객체를 스토리지 클래스 간에 전환할 수 있다.
	- 자주 액세스하지 않는 객체는 Standard IA로 이동
	- 빠른 액세스가 필요 없는 아카이브 객체는 Glacier 또는 Glacier Deep Archive로 이동
- 수동 또는 수명 주기 규칙(Lifecycle Rules)을 사용하여 객체 이동을 자동화할 수 있다.

### Amazon S3 – Lifecycle Rules

- **전환 작업 (Transition Actions):** 객체를 다른 스토리지 클래스로 전환하도록 구성
    - 생성 후 60일이 지나면 객체를 Standard IA 클래스로 이동
    - 6개월 후 아카이빙을 위해 Glacier로 이동
        
- **만료 작업 (Expiration actions):** 일정 시간 후 객체가 만료(삭제)되도록 구성
    - 액세스 로그 파일은 365일 후 삭제되도록 설정할 수 있다.  
    - 파일의 이전 버전(버전 관리가 활성화된 경우)을 삭제하는 데 사용할 수 있다.  
    - 불완전한 Multi-Part 업로드를 삭제하는 데 사용할 수 있다.  
        
- 규칙은 특정 접두사(ex: `s3://mybucket/mp3/*`)에 대해 생성할 수 있다.  
- 규칙은 특정 객체 태그(ex: `Department: Finance`)에 대해 생성할 수 있다.  
     
### Amazon S3 – Lifecycle Rules (Scenario 1)

**시나리오:** EC2의 애플리케이션이 Amazon S3에 프로필 사진이 업로드된 후 이미지 썸네일을 생성합니다. 이 썸네일은 쉽게 다시 생성할 수 있으며 60일 동안만 보관하면 됩니다. 원본 이미지는 이 60일 동안 즉시 검색할 수 있어야 하며, 그 후에는 사용자가 최대 6시간까지 기다릴 수 있습니다. 어떻게 설계하시겠습니까?  

**설계:**
- S3 원본 이미지는 Standard에 보관하고, 60일 후 Glacier로 전환하는 수명 주기 구성을 설정한다
- S3 썸네일은 One-Zone IA에 보관하고, 60일 후 만료(삭제)하는 수명 주기 구성을 설정한다
	- 접근이 드물고, 재생성이 쉽기 때문

### Amazon S3 – Lifecycle Rules (Scenario 2)

**시나리오:** 회사 규칙에 따라 삭제된 S3 객체를 30일 동안 즉시 복구할 수 있어야 하지만, 이는 드물게 발생할 수 있습니다. 이 기간이 지난 후 365일까지는 삭제된 객체를 48시간 이내에 복구할 수 있어야 합니다.  

**설계:**
- **Enable S3 Versioning**: 객체 버전을 가지기 위해 S3 버전 관리를 활성화하여 "삭제된 객체"가 실제로 "삭제 마커"에 의해 숨겨지고 복구될 수 있도록 한다.
- 객체의 "noncurrent versions"을 Standard IA로 전환한다
- 그 후 "noncurrent versions"을 Glacier Deep Archive로 전환한다

#### Amazon S3 Analytics – Storage Class Analysis

- 객체를 올바른 스토리지 클래스로 전환할 시기를 결정하는 데 도움이 된다.
- Standard 및 Standard IA에 대한 권장 사항을 제공
	- One-Zone IA 또는 Glacier에는 작동하지 않는다.
- 보고서는 매일 업데이트된다.
- 데이터 분석을 시작하는 데 24~48시간이 걸
- 수명 주기 규칙을 설정(또는 개선)하는 좋은 첫 단계이다.

---
### Hands-on

아래의 rule들을 선택하여 설정할 수 있음.
#### Lifecycle rule actions
- Move current verions of object between storage classes
- Move noncurrent versions of object between storage classes
- Expire current version of object
- Permanently delete noncurrent version of objects
- Delete expired object delete markers or incomplete multipart uploads

---

### Amazon S3 – Security

- **사용자 기반 (User-Based):**  
    - IAM 정책: IAM의 특정 사용자에 대해 허용되어야 하는 API 호출을 정의
        
- **리소스 기반 (Resource-Based):**  
    - **Bucket Policies**: S3 콘솔에서 버킷 전체에 적용되는 규칙 - allows cross account
    -  **Object Access Control List**(ACL): 더 세분화된 제어(비활성화 가능)  
    -  **Bucket Access Control List**(ACL): 덜 일반적(비활성화 가능)  
        
- **참고:** IAM 주체는 다음 경우에 S3 객체에 액세스할 수 있다.
    - 사용자 IAM 권한이 허용하거나 OR 리소스 정책이 허용하는 경우 AND 명시적인 DENY가 없는 경우  
        
- **암호화 (Encryption):** 암호화 키를 사용하여 Amazon S3의 객체를 암호화 가능

### S3 Bucket Policies

- JSON based policies
	- **Resources:** 버킷 및 객체  
	- **Effect:** 허용(Allow) / 거부(Deny)  
	- **Actions:** 허용 또는 거부할 API 호출 집합  
	- **Principal:** 정책을 적용할 계정 또는 사용자  
    
- S3 버킷 정책을 사용하는 경우:  
    - 버킷에 대한 공개 액세스 권한 부여  
    - 업로드 시 객체 암호화 강제  
    - 다른 계정에 액세스 권한 부여 (Cross Account)

- Example
	- Public access - Use bucket policy
		- s3 bucket policy를 public access 할수 있도록 공개허용한 경우
	- User Access to s3 - IAM permissions
		- IAM 사용자가 접근하는 경우로 IAM 정책을 해당 s3 버킷과 객체를 허용하도록 설정
	- EC2 instance access - Use IAM Roles
		- EC2 인스턴스의 IAM Role을 통해 s3 버킷에 접근한다. 동일하게 해당 버킷에 대한 권한 부여 필요
	- Cross-Account Access - Use bucket policy
		- s3 bucket policy를 사용하여 다른 AWS 계정의 IAM User에게 Cross-Account Acces를 허용한다.

#### Bucket settings for Block Public Access 
public access 차단을 위한 버킷 설정

![](images/Pasted%20image%2020250714170845.png)

- 이 설정은 회사 데이터 유출을 방지하기 위해 생성됨
- 버킷이 절대 공개되어서는 안 된다면 이 설정을 on.
- 계정 수준에서 설정할 수 있다.

---
### Amazon S3 – Object Encryption

S3 버킷의 객체를 4가지 방법 중 하나를 사용하여 암호화할 수 있다.

- **서버 측 암호화 (Server-Side Encryption, SSE):**  
    - **Amazon S3 관리형 키를 사용한 서버 측 암호화 (SSE-S3):** 기본적으로 활성화
        - AWS가 처리, 관리, 소유하는 키를 사용하여 S3 객체를 암호화한다.
            
    - **AWS KMS에 저장된 KMS 키를 사용한 서버 측 암호화 (SSE-KMS):**  
        - AWS Key Management Service (AWS KMS)를 활용하여 암호화 키를 관리  
            
    - **고객 제공 키를 사용한 서버 측 암호화 (SSE-C):**  
        - 사용자가 자체 암호화 키를 관리하려는 경우에 사용
            
- **클라이언트 측 암호화 (Client-Side Encryption):**  
	- 클라이언트에서 암호화한 파일 자체를 s3에 저장하는 것.
    

#### Amazon S3 Encryption – SSE-S3

![](images/Pasted%20image%2020250714174718.png)

- AWS가 처리, 관리, 소유하는 키를 사용한 암호화
- 객체는 서버 측에서 암호화
- 암호화 유형은 AES-256
- `"x-amz-server-side-encryption": "AES256"` 헤더를 설정필요
- 새 버킷 및 새 객체에 대해 기본적으로 활성화
    
#### Amazon S3 Encryption – SSE-KMS

![](images/Pasted%20image%2020250714174813.png)

- AWS KMS(Key Management Service)가 처리하고 관리하는 키를 사용한 암호화
- KMS의 장점: 사용자 제어 가능 + CloudTrail을 사용한 키 사용 audit 가능
- 객체는 서버 측에서 암호화
- `"x-amz-server-side-encryption": "aws:kms"` 헤더를 설정 필요
#### SSE-KMS Limitation

- SSE-KMS를 사용하는 경우 KMS 제한의 영향을 받을 수 있.
- 업로드 시 `GenerateDataKey` KMS API를 호출
- 다운로드 시 `Decrypt` KMS API를 호출
	- 이에 대한 권한 필요
- 초당 KMS 할당량(리전별 5500, 10000, 30000 요청/초)에 포함
- Service Quotas 콘솔을 사용하여 할당량 증가를 요청할 수 있음
    

#### Amazon S3 Encryption – SSE-C

![](images/Pasted%20image%2020250714175007.png)

- AWS 외부에서 고객이 완전히 관리하는 키를 사용한 서버 측 암호화
- Amazon S3는 사용자가 제공하는 암호화 키를 저장하지 않는다.  
- HTTPS를 사용해야 한다.
	- s3가 키를 저장하지 않기때문에, 고객이 업로드시 키를 포함하여 전송할 것으로 HTTPS 필수.
- 모든 HTTP 요청에 대해 암호화 키를 HTTP 헤더에 제공해야 한다.
    

#### Amazon S3 Encryption – Client-Side Encryption
 ![](images/Pasted%20image%2020250714175047.png)
 
- Amazon S3 클라이언트 측 암호화 라이브러리와 같은 클라이언트 라이브러리를 사용
- 클라이언트는 Amazon S3로 전송하기 전에 데이터를 직접 암호화해야 한다.
- 클라이언트는 Amazon S3에서 데이터를 검색할 때 데이터를 직접 해독해야 한다.  
- 고객이 키와 암호화 주기를 완전히 관리한다 
    

#### Amazon S3 – Encryption in transit (SSL/TLS)

- 전송 중 암호화는 SSL/TLS 를 말하는 것
- Amazon S3는 두 가지 엔드포인트를 노출한다.
    - HTTP 엔드포인트 – 암호화되지 않음          
    - HTTPS 엔드포인트 – 전송 중 암호화  
- HTTPS 사용을 권장
- SSE-C의 경우 HTTPS가 필수  
- 대부분의 클라이언트는 기본적으로 HTTPS 엔드포인트를 사용  
    

#### Amazon S3 – Force Encryption in Transit aws:SecureTransport

![](images/Pasted%20image%2020250714175220.png)

버킷 정책에서 `aws:SecureTransport` 조건을 사용하여 전송 중 암호화를 강제할 수 있다.  

#### Amazon S3 – Default Encryption vs. Bucket Policies

![](images/Pasted%20image%2020250714175313.png)

- SSE-S3 암호화는 S3 버킷에 저장되는 새 객체에 자동으로 적용된다.
- 선택적으로 버킷 정책을 사용하여 "암호화 강제"를 설정하고 암호화 헤더(SSE-KMS 또는 SSE-C) 없이 S3 객체를 PUT하는 API 호출을 거부할 수 있다.
- **참고:** 버킷 정책은 "기본 암호화"보다 먼저 평가된다.

---

### VPC Endpoint Gateway for S3

![](images/Pasted%20image%2020250714175459.png)

- S3 는 글로벌 서비스이며, S3 버킷은 AWS 클라우드에 존재한다.
	- 이는 S3가 특정 리전에 국한되지 않고 전 세계적으로 접근 가능한 서비스임을 의미
- 따라서 VPC 내에서 S3에 기본적으로 접근하려면 인터넷 게이트웨이(IGW)를 통해 퍼블릭 엔드포인트를 사용하게 된다.
	- `AWS:SourceIP`(공개 IP) 로 필터링 가능
- Public 인터넷을 통한 접근을 원하지 않는 경우 VPC Endpoint Gateway 통해 비공개 접근 가능
	- VPC Endpoint Gateway는 VPC 내의 인스턴스가 인터넷 게이트웨이, NAT 디바이스, VPN 연결 또는 AWS Direct Connect 연결 없이 S3에 비공개로 연결할 수 있도록 한다.
- 버킷 정책에서 `AWS:SourceIP`(공개 IP) 또는 `AWS:SourceVpce`(하나 또는 몇 개의 엔드포인트) 또는 `AWS:SourceVpc`(모든 가능한 VPC 엔드포인트 포함)를 사용하여 액세스를 제어할 수 있다.
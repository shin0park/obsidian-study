
### Tuning Neural Networks (신경망 튜닝)

#### Learning Rate (학습률)

- 신경망 모델의 성능을 최적화하는 데 있어 학습률(Learning Rate)은 가장 기본적인 하이퍼파라미터 중 하나
- 신경망은 **경사하강법(gradient descent)** 으로 학습
	- 학습률은 경사 하강법(gradient descent)과 같은 최적화 알고리즘이 손실 함수(cost function)를 최소화하기 위해 모델의 파라미터, 즉 가중치를 업데이트할 때 얼마나 큰 '보폭'으로 이동할지를 결정하는 값
	- 모델이 최적의 해에 도달하는 속도와 안정성에 직접적인 영향을 미친다.
	- AWS는 Amazon ML 서비스에서 학습률을 확률적 경사 하강법(SGD) 알고리즘에 사용되는 상수 값으로 정의하며, 이 값이 알고리즘이 최적의 가중치에 수렴하는 속도에 영향을 미친다고 한다.
- 너무 크면 최적점을 지나쳐 버리고, 너무 작으면 수렴이 매우 느림
	- 즉,
    
#### Batch Size (배치 크기)

- 배치 크기는 모델의 수렴 경로와 훈련 속도에 영향을 미치는 중요한 하이퍼파라미터이다.
- ![](images/Pasted%20image%2020250901005645.png) (https://bruders.tistory.com/79)
- 각 훈련 epoch에서 모델이 가중치를 업데이트하기 위해 처리하는 훈련 데이터 샘플의 수를 의미
	- epoch 란
		- 사전적의미: 시대
		- 딥러닝에서의 epoch는 전체 데이터 셋이 신경망을 통과한 횟수, 즉 모든 데이터셋을 학습 하는 횟수를 의미
		- 예. 1-epoch: 전체 데이터셋이 하나의 신경망에 적용되어 순전파와 역전파를 통해 신경망을 한번 통과했다는 의미
		- epoch값이 너무 작으면 underfitting, 너무 크면 overfitting이 발생할 확률이 높다.

- ![400](images/Pasted%20image%2020250901005907.png)
- **작은 배치**: 
	- 손실 함수 곡선의 지역 최솟값(local minima)에서 벗어나기 쉬워 더 나은 일반화 성능을 얻을 수 있는 경향이 있음
	- 배치 크기가 너무 작아지면 적은 데이터로 가중치가 자주 업데이트되어 훈련이 불안정해질 수 있음
- **큰 배치**: 
	- 모델이 잘못된 해에 수렴하여 훈련이 제대로 진행되지 않거나, 지역 최솟값에 갇히게 될 가능성이 있음
	- 한번에 처리해야할 데이터의 양이 많아져, 학습속도가 느려지고 메모리 부족문제가 발생할 수 있음.
- epoch마다 무작위 섞기가 필요
    

 요약 (To Recap)

- 작은 배치 크기는 모델이 손실 함수 곡선의 지역 최솟값에 갇히는 경향을 줄일 수 있다.
- 큰 배치 크기는 때때로 잘못된 솔루션으로 수렴할 수 있다.
- 높은 학습률은 최적의 솔루션을 지나쳐 오버슈팅할 수 있다.
- 낮은 학습률은 모델 훈련에 필요한 총 시간을 증가시킨다.

---
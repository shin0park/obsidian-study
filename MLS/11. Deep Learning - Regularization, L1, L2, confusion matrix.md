
### Tuning Neural Networks (신경망 튜닝)

#### Learning Rate (학습률)

- 신경망 모델의 성능을 최적화하는 데 있어 학습률(Learning Rate)은 가장 기본적인 하이퍼파라미터 중 하나
- 신경망은 **경사하강법(gradient descent)** 으로 학습
	- 학습률은 경사 하강법(gradient descent)과 같은 최적화 알고리즘이 손실 함수(cost function)를 최소화하기 위해 모델의 파라미터, 즉 가중치를 업데이트할 때 얼마나 큰 '보폭'으로 이동할지를 결정하는 값
	- 모델이 최적의 해에 도달하는 속도와 안정성에 직접적인 영향을 미친다.
	- AWS는 Amazon ML 서비스에서 학습률을 확률적 경사 하강법(SGD) 알고리즘에 사용되는 상수 값으로 정의하며, 이 값이 알고리즘이 최적의 가중치에 수렴하는 속도에 영향을 미친다고 한다.
- ![](images/Pasted%20image%2020250901010647.png)
- 너무 크면 최적점을 지나쳐 버리고, 너무 작으면 수렴이 매우 느림
	- 학습률이 너무 높은 경우: 경사 하강법이 손실 함수의 최솟값을 지나쳐 '오버슈팅(overshoot)'하게 되면서 발산하거나 최적의 해를 찾지 못하고 불안정하게 맴돌 수 있다.
	- 학습률이 너무 작은 경우: 가중치 업데이트가 매우 미세하게 이루어져 최적의 해에 도달하는 데 지나치게 오랜 시간이 소요되어 훈련 효율성이 떨어진다.
- Amazon SageMaker는 자동 모델 튜닝(Automatic Model Tuning, HPO) 기능을 제공하여 학습률, 배치 크기 등 여러 하이퍼파라미터의 최적 조합을 자동으로 탐색한다. 
	- 사용자가 최적의 값을 수동으로 찾아야 하는 부담을 줄이고, AWS의 최적화된 시스템에 의존하여 튜닝 프로세스의 효율성을 극대화할 수 있도록 도움
    
#### Batch Size (배치 크기)

- 배치 크기는 모델의 수렴 경로와 훈련 속도에 영향을 미치는 중요한 하이퍼파라미터이다.
- ![](images/Pasted%20image%2020250901005645.png) (https://bruders.tistory.com/79)
- 각 훈련 epoch에서 모델이 가중치를 업데이트하기 위해 처리하는 훈련 데이터 샘플의 수를 의미
	- epoch 란
		- 사전적의미: 시대
		- 딥러닝에서의 epoch는 전체 데이터 셋이 신경망을 통과한 횟수, 즉 모든 데이터셋을 학습 하는 횟수를 의미
		- 예. 1-epoch: 전체 데이터셋이 하나의 신경망에 적용되어 순전파와 역전파를 통해 신경망을 한번 통과했다는 의미
		- epoch값이 너무 작으면 underfitting, 너무 크면 overfitting이 발생할 확률이 높다.

- ![400](images/Pasted%20image%2020250901005907.png)
- **작은 배치**: 
	- 손실 함수 곡선의 지역 최솟값(local minima)에서 벗어나기 쉬워 더 나은 일반화 성능을 얻을 수 있는 경향이 있음
	- 배치 크기가 너무 작아지면 적은 데이터로 가중치가 자주 업데이트되어 훈련이 불안정해질 수 있음
- **큰 배치**: 
	- 모델이 잘못된 해에 수렴하여 훈련이 제대로 진행되지 않거나, 지역 최솟값에 갇히게 될 가능성이 있음
	- 한번에 처리해야할 데이터의 양이 많아져, 학습속도가 느려지고 메모리 부족문제가 발생할 수 있음.
- epoch마다 무작위 섞기(random shuffling)가 필요하다.
	- 데이터가 무작위로 섞여 있지 않고 특정 유형의 데이터가 연속적으로 제공될 경우, 모델이 해당 데이터에만 과도하게 편향되어 전체 데이터에 대한 최적의 솔루션을 찾지 못할 수 있다.

#### Iteration
- 1-epoch를 마치는데 필요한 미니 배치의 수를 의미
- 즉, 1-epoch를 마치는데 필요한 파라미터 업데이트 횟수
- step이라 부르기도한다.
    

 요약 (To Recap)

- 작은 배치 크기는 모델이 손실 함수 곡선의 지역 최솟값에 갇히는 경향을 줄일 수 있다.
- 큰 배치 크기는 때때로 잘못된 솔루션으로 수렴할 수 있다.
- 높은 학습률은 최적의 솔루션을 지나쳐 오버슈팅할 수 있다.
- 낮은 학습률은 모델 훈련에 필요한 총 시간을 증가시킨다.

---

### Neural Network Regularization (정규화 기법)

#### What is Regularization?

- 모델이 훈련 데이터에 지나치게 '과적합(overfitting)'되는 현상을 방지하는 데 사용되는 일련의 기법들을 의미
- 학습 데이터에 존재하는 특정 패턴이나 노이즈까지 학습하여, 학습 데이터에서는 매우 높은 정확도를 보이지만 새로운 데이터에는 일반화 성능이 떨어지는 문제
- 학습 시 **training, evaluation, testing dataset** 구분 필요
	- 과적합을 탐지하고 방지하기 위해, 모델은 일반적으로 훈련(training), 평가(evaluation), 테스트(testing) 데이터 세트를 사용하여 검증된다.
	- 훈련 데이터셋에서는 높은 정확도를 보이지만, 평가 또는 테스트 데이터셋에서는 낮은 정확도를 보이는 것이 과적합의 전형적인 증상
	- 정규화 기법의 목적은 이러한 과적합을 방지하고 모델의 일반화 능력(generalization ability)을 향상시키는 것 -> 모델의 유연성(flexibility)을 인위적으로 감소시켜 훈련 데이터의 노이즈에 덜 민감하게 반응하도록 만든다.
- 모델의 훈련 과정은 과적합(높은 분산)과 과소적합(underfitting, 높은 편향) 사이의 균형점을 찾는 과정이다.
    
#### Dropout

![](images/Pasted%20image%2020250901014415.png)

- 학습 과정에서 무작위로 일부 뉴런을 '제거(drop out)'하여 과적합 방지한다.
- 매 훈련 반복(iteration)마다 다른 뉴런 조합을 사용하여 가중치를 업데이트한다.
- 이를 통해 각 뉴런이 다른 특정 뉴런에 과도하게 의존하는 것을 방지하고, 모델이 더 견고하고 일반화된 패턴을 학습하도록 유도한다.
- Amazon SageMaker의 `Object2Vec` 알고리즘은 훈련 파라미터로 `dropout`을 명시적으로 제공하여 이 기법을 적용할 수 있도록함

#### Early Stopping

- 모델의 훈련 과정에서 훈련 데이터에 대한 성능은 계속 개선되지만, 별도의 검증 데이터(validation data)에 대한 성능이 더 이상 개선되지 않거나 오히려 악화되는 지점에서 훈련을 중단하는 정규화 기법
- 모델이 훈련 데이터의 노이즈에 맞춰 과적합되기 시작하는 최적의 시점을 포착하여 훈련을 멈춤으로써, 과적합을 효과적으로 방지
- 조기 종료의 핵심은 적절한 '타이밍'을 판단하는 것
	- 훈련을 너무 일찍 멈추면 모델이 충분히 학습되지 않아 과소적합이 발생할 수 있고 반대로 너무 늦게 멈추면 모델이 과적합되어 일반화 성능이 저하될 수 있다.
- AWS
	- AWS는 이 복잡한 타이밍 판단을 자동화하여 훈련 과정의 비효율성을 줄인다. 
	- Amazon SageMaker의 하이퍼파라미터 튜닝 기능은 `TrainingJobEarlyStoppingType` 파라미터를 통해 조기 종료를 지원하며, 이를 `OFF` 또는 `AUTO`로 설정할 수 있다.

---
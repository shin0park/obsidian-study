## Feature Engineering

- 단순히 원시 데이터를 모델에 투입하는 행위를 넘어, 데이터와 특정 모델에 대한 깊은 이해를 바탕으로 모델 훈련에 적합한 '더 나은' feature를 생성하는 과정이다.    
- 머신러닝의 성공에 있어 가장 중요한 요소 중 하나로 간주됨
  
- 주요 고려사항:
    - 어떤 피처를 사용할 것인가?
    - 피처를 변환해야 하는가?
    - 결측값(missing data)은 어떻게 처리할 것인가?
    - 기존 데이터에서 새로운 피처를 생성할 필요가 있는가?
        
- 원시(raw) 데이터를 그대로 투입해서는 좋은 결과를 기대하기 어려움.
    
- Andrew Ng: _“Applied machine learning is basically feature engineering”_.
	- feature engineering은 부수적인 전처리 단계가 아닌 모델의 성능에 직결되는 핵심적인 작업임을 의미
    

---

### Curse of Dimensionality (차원의 저주)

- 피처의 수가 너무 많아져 데이터가 희소(sparse)해지며 모델 성능이 저하되는 문제
- 각각의 피처는 데이터 공간에서 새로운 차원(dimension)을 형성하며, 피처의 수가 증가할수록 데이터 공간은 기하급수적으로 확장된다.
	- 이로 인해 전체 데이터 공간에서 실제 데이터 샘플이 차지하는 밀도가 매우 낮아져 데이터가 희소(sparse)해지는 문제가 발생합
- 데이터가 희소해지면 모델이 의미 있는 패턴을 학습하기 어려워지고, 결과적으로 모델이 훈련 데이터에만 과도하게 적응하는 과적합(overfitting)을 유발
    
- 많은 feature 중 문제에 가장 관련성이 높은 피처를 선택해야 함 → 도메인 지식(domain knowledge) 필요.
    
- 차원 축소 기법 활용 가능: 비지도 학습(unsupervised learning) 기법을 활용
    
    - PCA, Principal Component Analysis (주성분 분석)
	    - 데이터의 주요 패턴을 캡처하면서 차원을 줄이는 분석 기법
	    - 핵심 원리는 여러 피처의 분산을 가장 잘 설명하는 = (데이터의 분산을 최대화하는) 새로운 주성분을 찾아 차원을 축소하는 것
	    - 분산은 데이터가 얼마나 특정 방향으로 퍼져 있는지를 나타내며, 큰 분산은 해당 방향에 데이터의 주요 정보나 패턴이 포함되어 있음을 의미
	    - 분산이 클 수록 데이터의 특징을 잘 나타냄
	    - PCA 수행
		    - 데이터의 구조를 가장 잘 반영하는 기저를 찾는 것이 중요하다.
		      -> **최적의 기저(Optimal Bases)**라고 부른다
		    - PCA에서는 자동으로 분산이 최대가 되는 방향, 즉 데이터 간의 중복성이 가장 적은 방향을 최적의 기저로 찾아낸다.
	    - https://maloveforme.tistory.com/221
        
    - K-Means 기반 축소
	    - ![](images/Pasted%20image%2020250818222947.png)
	    - 데이터를 요약하거나 차원을 축소하는 데 응용될 수 있음.
	    - k개의 클러스터로 묶는 것 (거리가 가까운 데이터끼리)
		    - 클러스터의 중심이 되는 data point=cluster center 가 K개
		    - 이를 반복 -> 평균 

- 차원 축소(**Dimensionality Reduction**)
	- ![](images/Pasted%20image%2020250818222023.png)
	- 데이터의 차원(피처의 수)을 줄이며 데이터의 중요한 정보를 최대한 보존하는 것을 의미

#### Supervised Learning vs Unsupervised Learning
- Supervised Learning
	- 데이터: 레이블이 붙은 데이터 사용 (입력과 출력이 쌍으로 주어짐)
	- 목적: 입력 데이터에 대한 출력 값을 예측
	- 예시: 이미지 분류, 스팸 이메일 검출 
	- 알고리즘: 회귀, 분류 (예: 결정 트리, 랜덤 포레스트, SVM)
- Unsupervised Learning
	- 데이터: 레이블이 없는 데이터 사용 (출력 값이 없음, clustering) 
	- 목적: 데이터의 구조를 찾고 그룹화
	- 예시: 군집화, 차원 축소
	- 알고리즘: K-Means, PCA, DBSCAN

---

### 결측값 처리 (Imputing Missing Data)

이를 어떻게 처리하느냐에 따라 모델의 성능이 크게 달라질 수 있다.

#### 평균값 대치 (Mean Replacement)

- 결측값이 있는 열(피처)의 평균값으로 해당 결측값을 채우는 방식
- 빠르고 간단하며 전체 평균/샘플 크기에 영향 없음.
- 하지만, 피처 간의 상관관계를 전혀 고려하지 못하며, 특정 피처의 평균만을 사용하여 결측값을 채우기 때문에 정확도가 낮음.
- 이상치(outlier)가 많으면 중앙값(median) 활용이 더 나음.
- 단점:
    - 다른 feature 간 상관관계 무시
    - 범주형 데이터에는 부적합 (최빈값 대체 가능)
    - 정확도가 낮음

-> 결론: 매우 좋지 않음. 오직 컬럼 수준에서만 작동하는 한계 존재.
        

####  행 삭제 (Dropping)

- 결측값이 포함된 행 전체를 삭제하는 방식
- 결측값이 소수이고, 삭제가 데이터 편향을 만들지 않는 경우 사용
- 시간이 촉박할 때 합리적인 선택이 될 수 있지만 최선의 접근법은 아님
- "거의 모든 다른 방법이 더 낫다"
- 결국 정보 손실임
    

#### 머신러닝 기법 활용

결측값 처리를 위해 머신러닝 모델을 활용하는 고도화된 방법들

- **KNN (K-Nearest Neighbors)**: 'K'개의 가장 가깝고 유사한 행을 찾아, 해당 행들의 값들을 기반으로 결측값을 채움
	- 주로 수치형 데이터에 적합하며, 범주형 데이터는 해밍 거리(Hamming distance)와 같은 방법을 통해 처리
	- 해밍거리
		- x-y : 같으면 0, 다르면 1
    
- **회귀분석(Regression)**: 결측값이 있는 피처와 다른 피처들 간의 선형 또는 비선형 관계를 파악하여 결측값을 예측
	- 이 중 - **MICE**(Multiple Imputation by Chained Equations): 고급 기법
		- 평균(mean)이나 중앙값(median) 등 하나의 값으로 결측값을 채워넣는 single imputation과 달리, 결측치(`NA`)를 여러가지 변수를 고려하여 채워넣는 방법
    
- **딥러닝**: 결측값 예측을 위해 별도의 머신러닝 모델을 구축하는 방법
	- 범주형 데이터 대체에 강력
	- 구현 및 관리가 복잡
    
#### 최선의 방법: 더 많은 실제 데이터 확보
- 가장 좋은 결측값 처리 방법은 "더 많은 실제 데이터를 확보하는 것"
    

---

### 불균형 데이터 처리 (Handling Unbalanced Data)

- '긍정(positive)' 케이스와 '부정(negative)' 케이스 간의 큰 불일치가 있는 데이터셋을 말함. (예: 사기 거래 탐지 fraud detection)
	- 데이터 불균형은 특히 신경망 모델에서 문제를 일으키는 경향 있음
    
- "양성(positive)" = 모델이 탐지하려는 사건이 실제로 발생한 경우

#### 해결 방법

- **Oversampling**: 소수 클래스의 샘플을 복제하여 데이터 불균형을 해결하는 방법
	- 무작위로 수행될 수 있음
	- 소수 클래스의 데이터 양을 증가시켜 모델이 해당 클래스를 더 잘 학습하도록 돕는다.
    
- **Undersampling**: 다수 클래스의 샘플을 제거하여 데이터 불균형을 해소 (데이터 손실 위험)
	- 일반적으로 바람직하지 않음
    
- **SMOTE (Synthetic Minority Over-sampling TEchnique)**: 소수 클래스의 새로운 샘플을 인공적으로 생성하는 기법
	- 단순히 기존 샘플을 복제하는 오버샘플링보다 진보된 형태
	- KNN 기반으로새로운 샘플을 생성
	- SMOTE는 새로운 샘플을 생성하면서 동시에 다수 클래스 언더샘플링도 함께 수행할 수 있어, 일반적으로 단순한 오버샘플링보다 더 나은 결과를 제공
    
- **Threshold 임계값 조정**: 분류 기준 확률값을 조정해 False Positive / False Negative 균형 맞춤
	- 분류 모델은 보통 예측 결과를 확률 값으로 출력하며, 이 확률이 특정 임계값을 넘을 때 '긍정' 케이스로 분류한다.
	- 임계값 조정은 이러한 확률 임계값을 변경하여 분류 결과를 조절하는 방법
    

---

### 이상치 처리 (Handling Outliers)

이상치(outliers)는 데이터셋의 다른 값들과 현저하게 다른 값들을 의미하며, 모델의 성능에 부정적인 영향을 미칠 수 있다.

- **분산(Variance)**: 평균으로부터 제곱 편차의 평균
	- ![](images/Pasted%20image%2020250818224817.png)
    
- **표준편차(Standard Deviation, σ)**: 분산의 제곱근
	- ![](images/Pasted%20image%2020250818224836.png)
    
- 평균에서 몇 σ 떨어져 있는지를 통해 이상치 판단
	- 이상치가 평균으로부터 얼마나 멀리 떨어져 있는지는 "몇 시그마(how many sigmas)"로 표현
    
- 처리 방법:
	- 이상치를 왜 제거하는지 명확히 이해해야 하며, 데이터의 맥락을 고려해야 한다.
    - 로그 데이터의 봇(Bot)과 같은 비정상적 패턴은 제거
    - 단, 실제 분석 목적에 따라 부자(억만장자) 같은 극단값도 유지해야 할 수 있음.(미국 시민의 평균 소득을 모델링할 때)
	    - 억만장자를 단순히 이상치로 간주하여 제거하는 것은 부적절할 수 있음.
        
- AWS Random Cut Forest 알고리즘: 
	- 이상치 탐지를 위해 특별히 설계됨.
	- QuickSight, Kinesis Analytics, SageMaker 등 다양한 AWS 서비스에 내장되어 있음
    

---

### 데이터 변환 및 처리 기법 (Data Transformation and Preprocessing Techniques)

- **데이터 분할 (Binning)**: 수치형 값을 특정 범위로 묶어 데이터를 버킷(bucket)화하는 기법 (예: 나이대 분류)
	- 범주형 데이터화
	- **분위수 분할(Quantile binning)**
		- 데이터 분포에서 데이터의 위치를 기준으로 데이터를 분류하여 각 버킷의 크기를 균일하게 맞출 수 있음
    
- **데이터 변형 (Transforming)**: 피처를 모델 훈련에 더 적합하게 만들기 위해 함수를 적용하는 것
	- 예, 지수적 추세(exponential trend)를 가진 피처 데이터는 로그 변환을 적용하여 선형성을 높일 수 있음
    
- **Encoding (인코딩)**: 데이터를 모델이 요구하는 새로운 표현으로 변환하는 것
	- 범주형 데이터를 숫자로 변환 (One-hot encoding 등)
		- 각 범주(category)에 대해 별도의 '버킷'을 만들고, 해당 범주에는 1을, 나머지는 0을 할당하는 방식
		- 이 방법은 특히 딥러닝에서 각 범주가 개별 출력 '뉴런(neuron)'으로 표현될 때 매우 흔히 사용
    
- **Scaling/Normalization (스케일링/정규화)**:
	- 피처의 값을 비교 가능한 범위로 조정하는 것
    - 값 크기가 큰 feature가 영향력을 과도하게 갖지 않도록 조정
	    - 예, 나이와 수입을 피처로 사용할 때, 수입의 값은 나이보다 훨씬 크기 때문에 스케일링을 하지 않으면 모델이 수입 피처에 부당하게 더 많은 가중치를 부여할 수 있음
	- 모델에 따라 데이터가 0 주변에 정규분포 형태를 이루기를 선호함. (신경망)
    - scikit-learn의 `MinMaxScaler` 등 사용.
        
- **데이터 셔플링 (Shuffling)**: 훈련 데이터의 순서를 무작위로 섞는 과정
	- 데이터가 수집된 순서로 인해 발생하는 '잔여 신호(residual signals)'를 모델이 학습하는 것을 방지하여, 모델의 일반화 성능을 향상시키는 데 기여

---
###  SageMaker Ground Truth

#### 훈련데이터 생성 (Generating Training Labels)

때로는 훈련 데이터가 아예 없는 경우도 있으며, 이 경우 사람이 직접 데이터를 라벨링해야 한다. 
이러한 작업을 관리하는 AWS 서비스가 **SageMaker Ground Truth**이다.

- **Ground Truth**: 사람이 직접 레이블링(labeling)한 데이터를 효율적으로 관리
- 초기 데이터 레이블링은 사람이 필요 (예: 이미지 분류 태깅).
- 핵심기능
	- **능동 학습(Active Learning)**의 원리를 적용
	- Ground Truth는 사람이 레이블링할 때 동시에 모델을 학습시켜, 불확실한(ambiguous) 데이터만 사람이 다시 확인하도록 함 → 비용 70% 절감
    
- 라벨링 인력:
    - AWS Mechanical Turk 인력 활용
    - 내부 팀
    - 전문 레이블링 회사
        
- **Ground Truth Plus**:
    - AWS 전문가 팀이 라벨링 작업과 팀을 관리해주는 '턴키(turnkey)' 솔루션을 제공하여, 고객이 라벨링 과정의 복잡성을 신경 쓰지 않도록 돕는다
    - 고객은 intake form 작성 → AWS와 협의 → 진행 상황 포털에서 확인.
      
- 추가 라벨링 생성 방법
	- AWS에서 제공하는 사전 훈련된 모델을 활용
		- **Amazon Rekognition**: 이미지 자동 분류
		- **Amazon Comprehend**: 텍스트 주제/감정 자동 분류
	- 기타 사전 학습된 모델이나 비지도 학습(unsupervised technique) 기법 활용 가능
	    

---

### TF-IDF 및 텍스트 데이터 처리 (TF-IDF and Text Data Processing)

TF-IDF는 텍스트 데이터에서 용어의 중요성을 파악하는 데 널리 사용되는 기법

#### TF-IDF의 개념

- TF-IDF는 **용어 빈도(Term Frequency)**와 **역문서 빈도(Inverse Document Frequency)**의 약자이다.
	- **TF (Term Frequency)**: 특정 문서 내 단어 출현 빈도
		- 어가 자주 등장할수록 해당 문서의 의미를 파악하는 데 더 중요하다고 볼 수 있음
	- **DF (Document Frequency)**: 전체 문서 집합 내 단어 출현 빈도
	- **IDF**: 특정 단어가 전체 문서 집합에서 얼마나 자주 등장하는지를 측정
		- 'a', 'the', 'and'와 같이 모든 문서에 흔하게 나타나는 단어는 문서의 고유한 특징을 나타내지 않으므로 가치가 낮다고 판단
- 이 개념은 특정 문서에 대한 단어의 관련성을 파악하는 데 중요하게 사용된다
- **TF-IDF = TF × IDF** → 문서 내 해당 단어의 상대적 중요도 계산
	- 문서에 자주 등장하면서도 전체 문서 집합에서는 드문 단어를 찾아 문서에 대한 단어의 중요성 및 고유성을 측정하는 척도

- 로그 가중치로 변환된 IDF 사용 → 단어 빈도의 지수적 분포 보정
	- TF-IDF 실제 적용 시에는 단어 빈도가 지수적으로 분포하는 경향이 있어, IDF에 로그를 취하여 가중치를 더 잘 조정하기도 한다.
    
- **Bag of Words** 모델 기반:
    - **Bag of Words** 모델: 문서를 단순히 단어 집합으로 취급
    - 실제 구현 시에는 동의어, 다양한 시제, 약어, 철자 오류 등을 처리하는 것이 대부분의 작업이다.
        
- 확장 기법:
    - **N-그램 (N-grams)**: TF-IDF를 개별 단어뿐만 아니라 여러 단어가 조합된 구(phrase)로 확장하는 개념
	    - unigram(단어 단위), bigram(연속 2단어), trigram(연속 3단어) 등.
	    - **유니그램(Unigrams)**: "I", "love", "certification", "exams"
		- **바이그램(Bi-grams)**: "I love", "love certification", "certification exams"
		- **트라이그램(Tri-grams)**: "I love certification", "love certification exams"
        
- 활용:
	- 검색 엔진과 같은 정보 검색 시스템의 근간을 이룬다.
    - 검색 알고리즘: TF-IDF 점수를 기준으로 문서 정렬
	    - 이는 언어의 복잡성을 통계적 수치로 단순화하여 컴퓨터가 이해할 수 있게 만드는 대표적인 '피처 엔지니어링' 사례
    - 대규모 처리에 Spark 활용


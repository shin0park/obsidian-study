
## 1. Image Classification (이미지 분류)

- 용도**: 이미지 지도 학습 알고리즘의 일종으로, 이미지에 하나 이상의 라벨을 할당 
	- 다중 레이블 분류 지원
	- 입력 이미지에 포함된 객체가 무엇인지를 식별하여 해당 레이블을 출력
	- 공간적 정보는 제공하지 않음: object detection과 달리 객체 위치는 알 수 없음, 단순히 이미지에 어떤 이미지가 존재하는지만 알려ㅁ
- **사용 방법**:
    - **MXNet**: 
	    - **전체 훈련 모드 (Full training mode):** 
		    - 네트워크를 무작위 가중치로 초기화하고 데이터세트로 처음부터 훈련을 진행
	    - **전이 학습 모드 (Transfer learning mode)**:
		    - 전이학습 기반 파인튜닝
		    - ImageNet과 같은 대규모 데이터세트에서 미리 훈련된 가중치로 네트워크를 초기화
			    - 이 경우, 일반적으로 최상위 완전 연결 계층(top fully-connected layer)만 무작위 가중치로 초기화되며, 새로운 훈련 데이터로 네트워크를 세부 조정(fine-tuning)하여 사용자의 특정 작업에 맞춘다
    - **TensorFlow**: 
	    - MobileNet, Inception, ResNet, EfficientNet 등 다양한 Tensorflow Hub 모델을 활용
	    - 사용자는 최상위 분류 계층(Top classification layer)을 세부 조정하거나 추가 훈련에 활용할 수 있다.
	    - 이 알고리즘은 기본적으로 3채널 224x224 크기의 이미지를 사용하도록 설정되어 있는데, 이는 ImageNet 데이터세트의 표준 크기를 따른 것
        
- **하이퍼파라미터**: 
	- 배치 크기(Batch size), 학습률(learning rate), 옵티마이저(optimizer)
	- 옵티마이저별로 특화된 파라미터들이 존재. 
		- 가중치 감쇠(Weight decay), 베타 1 (β1​), 베타 2 (β2​), 엡실론 (ϵ), 감마 (γ) 등
		- MXNet과 Tensorflow 버전 간에 미묘하게 다를 수 있다.

- **인스턴스**: 
	- 대규모 컨볼루션 신경망(CNN)을 훈련하기 위해서는 병렬 처리 능력이 중요하므로, 훈련에는 GPU 인스턴스가 권장. 추론(Inference) 단계에서는 비용 효율성과 필요 처리량에 따라 CPU 또는 GPU 모두 사용할 수 있음
	- GPU(p2, p3, g4dn, g5)로 학습, CPU/GPU 모두 추론 가능
    

---

## 2. Semantic Segmentation (시맨틱 분할)

- ![400](images/Pasted%20image%2020250928231731.png)
- **용도**: 이미지 내의 객체를 픽셀 수준에서 분류
    - 이미지 전체 라벨링(X), 바운딩 박스 탐지(X).
    - 이미지의 모든 픽셀에 클래스 레이블을 태그하여 객체를 식별하고 위치를 파악
    - 차이점:
	    - **이미지 분류와의 차이:** 이미지 전체에 단일 레이블이나 다중 레이블을 할당하는 이미지 분류와 달리, 시맨틱 세그멘테이션은 픽셀별 분류를 수행
		- **객체 탐지와의 차이:** 객체 탐지가 객체 주변에 경계 상자(bounding boxes)를 할당하는 반면, 이 알고리즘은 정확한 객체 경계를 정의
	- 세그멘테이션 마스크(segmentation mask)를 출력
		- 마스크: 입력 이미지와 동일한 모양의 행렬(또는 그레이스케일 이미지)로, 각 픽셀이 해당하는 클래스 레이블을 나타낸다.
		- 이러한 정밀한 기술은 자율 주행 차량, 의료 영상 진단, 로봇 감지 등 객체의 정확한 형태와 경계가 중요한 분야에서 유용하게 활용
        
- **입력**: 
	- JPG 이미지 + PNG 주석(annotation) 파일 쌍으로 구성
	- 라벨 맵 필요: 주석에 대한 설명
	- 대규모 데이터세트의 효율적인 처리를 위해 파이프(Pipe) 입력 모드에서 **증강 매니페스트 이미지 형식**(`application/x-image`)이 지원
		- 증강 매니페스트는 훈련 데이터세트와 함께 메타데이터를 포함할 수 있게 하며, Amazon S3에서 직접 훈련 작업을 구성하여 데이터 준비 과정을 간소화함
	- 추론 시에는 JPG 이미지가 입력으로 사용됨
    
- **알고리즘**: MXNet Gluon 프레임워크와 Gluon CV 툴킷을 기반으로 구축됨. 이 알고리즘은 세 가지 최첨단 모델 중 하나를 선택하여 학습을 진행
	- Fully-Convolutional Network (FCN)
	- Pyramid Scene Parsing (PSP)
	- DeepLabV3
    
- **백본**: 
	- ResNet50, ResNet101 (ImageNet 기반 학습)
		- ImageNet에서 사전 훈련되어 강력한 특징 추출 능력을 갖춤
		  
- 훈련 방식은 처음부터(from scratch) 훈련하는 방식과 기존 모델을 활용하는 점진적 훈련(incremental training)을 모두 지원
    
- **하이퍼파라미터**: Epoch, 학습률, 배치 크기, 옵티마이저, 알고리즘, 백본
    
- **인스턴스**: 
	- 학습은 GPU 필수, 높은 병렬 처리 능력을 위해 다중 GPU 및 다중 머신 환경이 지원
	- CPU/GPU 추론 가능
- 시맨틱 세그멘테이션은 이미지 분류(IC)에 비해 요구되는 컴퓨팅 리소스가 급격히 증가함
    

---

## 3. Random Cut Forest (RCF) 무작위 절단 포레스트

- **용도**: 
	- 데이터세트 내에서 이상 데이터 포인트를 감지하기 위한 강력한 **비지도 학습** 알고리즘
	- 구조화되거나 패턴화된 데이터로부터 벗어난 관측치, 즉 변칙(anomalies)을 식별
	- 변칙은 시계열 데이터에서 예기치 않은 급증(spikes)이나 주기성 내의 끊어짐(breaks in periodicity), 또는 분류할 수 없는 데이터 지점
	- 각 데이터 포인트에 이상 점수(anomaly score)를 할당하여, 해당 데이터 포인트가 얼마나 일반적이지 않은지를 정량화
    
- **입력**: RecordIO-Protobuf 또는 CSV (File/Pipe)
    
- **사용방식**: 
	- 훈련 데이터를 무작위로 샘플링하고, 이 샘플링된 데이터를 기반으로 나무(trees)의 포레스트를 생성
	- 각 나무는 훈련 데이터의 파티션 역할을 함
	- RCF는 데이터 포인트를 나무에 추가할 때 발생하는 예상되는 나무 복잡도의 변화를 관찰하여 이상 점수를 계산한다.
	- 이상 점수의 평균을 사용하여 최종 이상 점수를 산출
    
- **활용**: SageMaker뿐 아니라 Kinesis Analytics(스트리밍 데이터)에서도 사용 (AWS에서 개발)
    
- **하이퍼파라미터**:
    - `num_trees`: 
	    - 포레스트를 구성할 나무의 수
	    - 많을수록 노이즈 감소
    - `num_samples_per_tree`: 
	    - 각 나무에 무작위로 샘플링되어 전송되는 데이터 포인트의 수
	    - 일반적으로, 이 파라미터의 역수(1/num_samples_per_tree)는 데이터세트 내 이상치 대비 정상 데이터의 추정 비율과 근사하도록 선택되어야 한다.
		    - 즉, 비정상/정상 비율에 맞게 조정
        
- **인스턴스**: 
	- RCF는 트리 기반의 통계적 모델로, 훈련 시 GPU의 병렬 처리 장점을 활용하지 못함: GPU 불필요
	- 학습: CPU(M4, C4, C5) 권장. 추론: ml.c5.xlarge과 같은 CPU 인스턴스
    

---

## 4. Neural Topic Model (NTM) 신경망 기반 주제 모델

- **용도**: 문서를 주제(Topic)별로 자동 분류/요약 (비지도 학습)
	- NTM은 단순한 TF/IDF 기법을 넘어서 작동하며, 예를 들어 "오토바이," "자동차," "기차," "주행 거리"와 같은 단어가 자주 발생하는 문서를 하나의 "운송" 관련 잠재 주제로 묶을 수 있다.
	- 감지된 주제를 기반으로 문서를 분류, 요약, 또는 주제 유사성을 활용한 콘텐츠 추천 및 정보 검색에 활용될 수 있다. 
    
- **알고리즘**: 신경망 변이형 추론(Neural Variational Inference)
    
- **입력**: RecordIO-Protobuf 또는 CSV
    - NTM은 4개의 데이터 채널을 지원
	    - `train` 채널 필수, `validation/test/auxiliary(보조)` 선택사항
    - 전처리 까다로움
	    - **토큰화:** 텍스트 내의 단어는 먼저 정수로 토큰화되어야 한다.
		- **CSV 형식:** CSV를 사용하는 경우, 모든 문서는 어휘(vocabulary) 내 모든 단어에 대한 횟수(count)를 포함해야 한다.
		- **Auxiliary 채널:** 어휘 정보를 담는 데 사용되는 선택적 'auxiliary' 채널이 있다.
	- file / pipe 둘다 지원
        
- **하이퍼파라미터**:
    - `num_topics`: 데이터세트에서 발견하고자 하는 고유한 주제의 수를 정의 (핵심 파라미터)
    - `mini_batch_size`, `learning_rate`: 
	    - 두 파라미터를 낮추면 검증 손실(validation loss)을 줄이는 데 도움이 될 수 있으나, 훈련 시간이 길어질 수 있다.
        
- **인스턴스**: 
	- 신경망을 기반으로 하기 때문에 GPU 또는 CPU를 모두 지원하지만, 훈련에는 GPU 인스턴스가 권장
	- 추론의 경우 CPU 사용이 가능하며, 이는 더 저렴한 비용으로 운영될 수 있다.
    

---

## 5. LDA (Latent Dirichlet Allocation) 잠재 디리클레 할당

**디리클레 분포**(Dirichlet distribution)는 연속 확률분포의 하나로, k![{\displaystyle k}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c3c9a2c7b599b37105512c5d570edc034056dd40)차원의 실수 벡터 중 벡터의 요소가 양수이며 모든 요소를 더한 값이 1인 경우 (이를 k−1차원 단체라고 한다)에 대해 확률값이 정의되는 분포
- https://ko.wikipedia.org/wiki/%EB%94%94%EB%A6%AC%ED%81%B4%EB%A0%88_%EB%B6%84%ED%8F%AC

- **용도**: 
	- 토픽 모델링 (비지도 학습, 비딥러닝)
	- 관측치 집합을 구별되는 범주의 혼합으로 설명하려는 **비지도 학습** 알고리즘
	- NTM과 달리 LDA는 딥러닝 기반이 아닌 통계적 모델
	- 주제 자체는 레이블이 지정되지 않으며, 단지 공통의 단어 하위 집합을 공유하는 문서들의 그룹일 뿐 -> 문서 내 단어 패턴 기반 그룹화
	- LDA는 텍스트 분석 외에도 구매 내역을 기반으로 고객을 클러스터링하거나 음악의 하모닉 분석 등 다양한 비지도 학습 작업에 적용
        
- **입력**: 
	- 채널: Train 채널 필수, Test 채널 선택
	- RecordIO-Protobuf 또는 CSV 
		- CSV 형식을 사용하는 경우, NTM과 유사하게 각 문서는 어휘 내 모든 단어에 대한 횟수(count)를 포함해야한다. 
	- 제한 사항: Pipe 모드는 recordIO 형식에서만 지원
    
- **활용**: 
	- 지정한 주제 개수에 따라 문서를 분류
		- 사용자가 지정하는 수(Num_topics)만큼의 주제를 생성하는 비지도 방식으로 작동 
	- 선택적 테스트 채널은 단어별 로그 우도(Per-word log likelihood)와 같은 결과를 채점하는 데 사용될 수 있다.  
	- 기능적으로 LDA는 NTM과 유사한 역할을 수행하지만, 통계적 모델 기반이므로 CPU 기반으로 작동
		- CPU 중심의 설계는 NTM의 GPU 기반 접근 방식에 비해 때때로 더 저렴하거나 효율적일 수 있다.
    
- **하이퍼파라미터**:
    - `num_topics`: 발견하고자 하는 주제의 수
    - `alpha0`: 집중도 매개변수(concentration parameter)의 초기 추측. (작을수록 희소한 토픽, 클수록 균등한 토픽)
        
- **인스턴스**: LDA는 통계적 모델 기반이므로 컴퓨팅 리소스 요구 사항이 NTM보다 낮으며, **단일 인스턴스 CPU 훈련만** 지원
    
---

## 6. KNN (K-Nearest Neighbors) 최근접 이웃

- **용도**: 지도 학습(Supervised Learning) 알고리즘, 분류(Classification) 또는 회귀(Regression)를 위한 단순하면서도 효과적인 인덱스 기반 알고리즘
    - 분류: 가까운 K개 데이터 중 다수 라벨
    - 회귀: 가까운 K개 데이터의 평균 값

- **입력**:
    - 선택적인 test 채널을 통해 정확도 또는 MSE(평균 제곱 오차)를 산출 할 수 있음
    - RecordIO-Protobuf 또는 CSV (첫 열은 라벨)
    - file/pipe 모드 지원

- **활용**: 효율적인 추론을 위해 다단계 훈련 프로세스를 따른다.
    - 샘플링 (Sampling): 입력 데이터는 먼저 샘플링되어, 전체 데이터세트 크기를 메모리에 맞게 줄이는 역할을 한다.
    - 차원 감소 (Dimensionality Reduction): SageMaker는 차원 감소 단계를 포함함. 
      이는 고차원 데이터(d>1000)에서 발생하는 희소성 문제, 즉 '차원의 저주'(curse of dimensionality)를 회피하기 위해 수행된다.
      차원 감소를 통해 모델의 메모리 점유율이 줄어들고 추론 지연 시간이 단축되지만, 노이즈나 정확도 손실이 발생할 수 있다. 
      제공되는 차원 감소 방법으로는 'sign' 또는 'fjlt'(빠른 Johnson-Lindenstrauss 변환)이 있다.
    - 인덱스 구축: 최종 목표는 이웃을 찾는 인덱스(index)를 구축하고 모델을 직렬화하는 것. 이 인덱스는 주어진 K에 대해 효율적으로 쿼리할 수 있도록 한다.

- **하이퍼파라미터**:
    - `k` : 최근접 이웃의 수
    - `sample_size`: 학습 데이터세트에서 샘플링할 데이터 포인트의 총 수를 지정, 인덱스 빌드를 위해 최종적으로 수집될 샘플 데이터의 양을 결정

- **인스턴스**:
    - 학습: CPU/GPU 모두 지원 (m5.2xlarge, p2.xlarge)
    - 추론: 소규모 → CPU 저지연, 대규모 → GPU 고처리량

---

## 7. K-Means

- **용도**: 비지도 클러스터링 (데이터를 K개의 그룹으로 분류)
    - 알고리즘의 목표는 그룹 내 멤버들이 서로 최대한 유사하게, 그리고 다른 그룹 멤버들과는 최대한 다르게 만드는 것
    - 유사성의 척도는 유클리드 거리(Euclidean distance)로 측정

- **입력**:
    - RecordIO-Protobuf 또는 CSV (train/test)
    - file/pipe 모드 지원

- **알고리즘**:
    - 초기 클러스터(k-means++), 반복 계산 후 최적 클러스터 중심 탐색
    - 모든 관측치는 특징(features)의 수에 해당하는 n차원 공간에 매핑되며 , 알고리즘은 K개 클러스터의 중심(center)을 최적화하는 방식으로 작동

    - 단계:
        - 초기 중심 결정: 초기 클러스터 중심은 무작위(Random) 또는 k-means++ 접근 방식 중 하나로 결정 k-means++는 초기 중심들을 가능한 한 멀리 떨어뜨려 초기화에 따른 결과의 불안정성을 줄이는 것을 목표.
        - 반복: 훈련 데이터를 반복하며 클러스터 중심을 계산
        - 추가 중심(Extra Cluster Centers): 사용자는 정확도 향상을 위해 초기 클러스터 중심의 수(K)를 원하는 클러스터 수(k)보다 크게 설정할 수 있다 (K=k×x). 이 추가 중심들은 최종 단계에서 k개로 감소한다.
        - 최종화: Lloyd의 방법과 k-means++ 초기화를 사용하여 클러스터 수를 K에서 k로 줄이는 과정을 거침

- **하이퍼파라미터**:
    - `k`: 클러스터 수 (최적의 K를 선택하는 것은 중요하며, 이를 위해 Elbow Method 사용)
    - `mini_batch_size`, `extra_center_factor`
    - `init_method`: 클러스터 위치를 초기화하는 방법(random 또는 k-means++)을 지정

- **인스턴스**: CPU 권장(반복 계산 중심의 알고리즘 특성상), GPU (SageMaker는 인스턴스당 하나의 GPU만 지원하므로 ml.g4dn.xlarge 추천) 그외 p2, p3, g4dn, g5 인스턴스가 지원

---
  
#### PCA (Principal Component Analysis)

- **용도**:
    - 비지도 학습 기반의 차원 축소(Dimensionality reduction) 알고리즘
    - 주된 목적은 정보 손실을 최소화하면서 특징(features)이 많은 고차원 데이터를 저차원 공간(예: 2D 플롯)으로 투영하는 것
    - 축소된 차원은 주성분(components)이라고 불리며 데이터의 변동성을 설명하는 축의 역할을 한다. 첫 번째 주성분은 가능한 가장 큰 변동성(variability)을 가지며, 두 번째 주성분은 그다음으로 큰 변동성을 가진다.

- **입력**:
    - RecordIO-Protobuf 또는 CSV
    - file/pipe 모드 지원

- **알고리즘**: 공분산 행렬(Covariance matrix) 생성 → 특이값 분해(Singular Value Decomposition, SVD)
    - Regular 모드: 희소 데이터, 중간 규모에 적합. 이 모드에서는 워커들이 공분산 행렬을 공동으로 계산
    - Randomized 모드: 대규모 데이터에 적합. 근사(approximation) 알고리즘을 사용하여 공분산 행렬을 추정

- **하이퍼파라미터**:
    - `algorithm_mode`: Regular 또는 Randomized 모드 중 선택하여 훈련 전략을 결정
    - `subtract_mean`: 데이터의 평균을 빼서 비편향화(Unbias)할지 여부를 결정

- **인스턴스**: CPU 또는 GPU 모두 가능
    - 대부분의 비지도 학습 알고리즘(RCF, K-Means, LDA, 때로는 PCA)은 CPU 사용을 권장하거나 GPU 활용에 제한을 둔다. 이는 이들 알고리즘이 딥러닝 기반의 대규모 행렬 연산보다는 반복 계산이나 통계적 샘플링에 의존하기 때문이다.

---
  
#### Factorization Machines (FM) 인수 분해 머신

- **용도**:
    - 희소 데이터(sparse data) 처리에 특화된 지도 학습 알고리즘
    - 클릭 예측, 추천 시스템 (클릭이나 추천을 위한 사용자의 데이터가 생각보다 많지 않음, 데이터가 희소함)
    - 이는 개별 사용자가 대부분의 페이지나 제품과 상호 작용하지 않기 때문에 발생하는 데이터의 희소성을 효율적으로 다루기 위함
    - FM은 상호작용을 쌍별 상호작용(pair-wise interactions)으로 제한하여, 예를 들어 사용자-항목 쌍과 같은 관계를 모델링한다. 따라서 최소 2차원이다.

- **학습 유형**: 지도학습 (분류·회귀)

- **입력**: Float32 텐서가 포함된 recordIO-protobuf 형식만 지원

    - 주요 사용 사례가 희소 데이터에 초점이 맞춰져 있기 때문에, 텍스트 기반인 CSV 형식은 실용적이지 않다고 판단되어 지원되지 않음

    - RecordIO로 래핑된 protobuf 형식에 대해서는 File 모드와 Pipe 모드 훈련이 모두 지원

- **활용**: User-Item 행렬에서 상호작용 예측

    - 두 요소(예: 사용자 및 항목)를 나타내는 행렬이 주어졌을 때, 분류(클릭 또는 구매 여부) 또는 값(예상 평점)을 예측하는 데 사용할 수 있는 잠재 인수(latent factors)를 찾는다.

    - 알고리즘은 일반적으로 추천 시스템의 컨텍스트에서 사용되며, 잠재 인수를 활용하여 사용자-항목 상호작용을 모델링

- **하이퍼파라미터**: 초기화 방법 (bias, factor, linear term) → uniform/normal/constant

    - 사용자는 각 방법의 속성을 세부 조정할 수 있음.

- **인스턴스**: CPU 권장 (GPU는 밀집 데이터에서만 효과)

---

## 10. IP Insights

- **용도**: IP 주소 사용 패턴 학습 → 이상 행위 탐지.
    
    - 비정상 IP 로그인, 리소스 생성 탐지.
        
- **입력**: CSV (엔티티, IP). Validation 채널에서 AUC 계산 가능.
    
- **활용**: 신경망 기반으로 엔티티/ IP를 벡터화하여 학습.
    
    - 엔티티는 해시 후 임베딩.
        
    - 자동으로 negative sample 생성.
        
- **하이퍼파라미터**:
    
    - `num_entity_vectors`: 해시 크기 (엔티티 수 ×2).
        
    - `vector_dim`: 임베딩 벡터 크기.
        
    - Epoch, 학습률, 배치 크기 등.
        
- **인스턴스**: GPU 권장 (ml.p3.2xlarge 이상, 멀티 GPU 가능).
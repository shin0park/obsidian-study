## Deep Learning 101

- 심층 학습(Deep Learning)은 인간 뇌의 뉴런 연결 방식을 모방한 신경망을 기반으로 함
- 단순한 뉴런 단위는 간단하지만, 다층으로 연결될 경우 학습과 추론이 가능
    
### Biological Inspiration (생물학적 영감)

- 뉴런은 축삭(axon)으로 연결되며, 충분한 입력 자극이 오면 신호를 발화(fire)
- 뇌에는 수십억 개의 뉴런과 수천 개의 연결이 있어 복잡한 인지와 학습이 가능
    
#### Cortical Columns (피질 기둥 구조)

- 뉴런들은 **mini-column**(약 100개 뉴런 단위)으로 구성되고, 이들이 모여 **hyper-column**을 형성한다.
- 약 1억 개 이상의 mini-column이 병렬로 정보 처리 → GPU 구조와 유사하다.
	- 이 유사성은 딥러닝 모델의 훈련에 왜 GPU가 CPU보다 훨씬 효율적인지를 설명하는 근본적인 이유가 된다.
	- 딥러닝 모델은 수십억 개의 매개변수에 대한 행렬 연산을 수행하는데, GPU는 수천 개의 작은 코어를 통해 이러한 대규모 병렬 연산을 동시에 처리하도록 설계되어 있다. 
    

---

### Deep Neural Networks (심층 신경망)

- ![](images/Pasted%20image%2020250826012906.png)
  
- 심층 신경망(Deep Neural Networks, DNN)은 여러 개의 숨겨진 층(hidden layers)을 가진 신경망을 의미한다.
- 기본구조: 입력신호(x) → 가중치(weight) 곱하고-> 편향(Bias, b)를 합산(Σ) → 활성화 함수(activation) → 출력(y) 이 과정을 반복한다.
	- **가중치(Weight):** 입력 신호의 중요도를 조절하는 역할을 한다. 학습 과정에서 이 가중치들이 조정되며 데이터의 패턴을 파악하게 된다.
	- **편향(Bias):** 뉴런의 활성화 임계값을 조절하는 상수 값으로, 입력 신호가 0일 때도 뉴런이 활성화될 수 있게 해준다.
    
- 다층 구조를 통해 복잡한 입력-출력 매핑 가능하다
    
### Deep Learning Frameworks

- 대표 프레임워크: 
	- **TensorFlow/Keras**
		- 구글이 개발한 오픈 소스 라이브러리로, 딥러닝 모델 개발에 가장 널리 사용된다. 
		- Keras는 Tensorflow 위에서 작동하는 고수준(high-level) API로, 복잡한 딥러닝 모델의 구축을 더 직관적이고 쉽게 만들어준다.
	- **MXNet**
		- Amazon이 후원하는 오픈 소스 딥러닝 프레임워크. 
		- AWS 환경에 최적화된 장점이 있으며, AWS EMR(Elastic MapReduce)에서 Apache MXNet을 공식적으로 지원
---

### Types of Neural Networks

- **피드포워드 신경망(Feedforward Neural Network):** 
	- 가장 기본적인 신경망 형태로, 데이터가 입력층에서 출력층으로 오직 한 방향으로만 전달된다.
    
- **컨볼루션 신경망(Convolutional Neural Networks, CNN):** 
	- 이미지 분류(예: 이미지에서 정지 표지판 식별)와 같이 정형화되지 않은 데이터 내의 공간적 특징을 찾는 데 특화되어 있다.
	- 이미지 분류, 기계 번역, 문장 분류 등.
    
- **순환 신경망(Recurrent Neural Networks, RNN):** 
	- 시계열 데이터(예: 주가 예측)나 문장 속 단어 이해, 기계 번역 등 시간적 순서가 있는 시퀀스 데이터를 처리하는 데 사용된다. 
	- LSTM(Long Short-Term Memory)과 GRU(Gated Recurrent Unit)는 RNN의 대표적인 변형 모델

---

### Activation Functions (활성화 함수)

- 활성화 함수는 뉴런 입력 신호를 기반으로 출력 결정하며, 신경망의 표현력을 결정하는 핵심적인 요소이다.
- 단순한 활성화 함수는 여러 가지 근본적인 한계를 가진다
- 학습(역전파)을 가능하게 하려면 **비선형 함수** 필요
    

#### Linear

- 단순 선형 함수, 입력과 출력이 비례 관계
- 뉴런에 아무런 '변환'을 수행하지 않기 때문에, 여러 층을 쌓아도 결국 전체 네트워크가 하나의 선형 함수로 축소된다. 
- 기울기가 항상 일정하므로, 기울기 기반의 가중치 업데이트를 사용하는 역전파(backpropagation)가 의미 있게 작동하지 않는다.
    

#### Binary Step

- ![](images/Pasted%20image%2020250826013551.png)
- 0/1 출력만 가능, 다중 분류 불가
- 미분 불가능 지점(수직 기울기)이 존재하여 미적분 기반의 역전파 알고리즘에 문제를 일으킴

---

-> 신경망이 복잡한 데이터의 패턴을 학습하려면 '비선형성(non-linearity)'이 필수적이다.
#### Sigmoid / TanH

- ![](images/Pasted%20image%2020250826013604.png)
- **Sigmoid / Logistic 함수:** 
	- 모든 입력값을 0과 1 사이의 값으로 변환하는 부드러운 S자 곡선 형태
- **TanH(Hyperbolic Tangent) 함수:** 
	- 모든 입력값을 -1과 1 사이의 값으로 변환하는 부드러운 곡선. 
	- 출력값이 0을 중심으로 분포되어 학습이 더 효율적이므로 Sigmoid보다 일반적으로 선호된다

- 단점: **'기울기 소실(Vanishing Gradient)' 문제
	- 입력값이 매우 크거나 작을 때, 함수의 기울기가 0에 가까워지는 포화(saturation) 현상이 발생
	- 이로 인해 역전파 과정에서 가중치 업데이트가 거의 일어나지 않아 학습이 매우 느려지거나 멈추는 현상이 나타남 
	- 계산 비용이 상대적으로 비쌈
    

#### ReLU

**ReLU(Rectified Linear Unit):** 
- 음수 입력은 0으로, 양수 입력은 그대로 통과. 
- Sigmoid나 TanH에 비해 계산이 훨씬 빠르고 효율적. 
- 하지만 음수 입력에 대한 기울기가 항상 0이므로, 한 번 0보다 작은 입력이 들어오면 해당 뉴런이 더 이상 학습되지 않는 **'죽은 ReLU(Dying ReLU)'** 문제가 발생할 수 있다.
- ![](images/Pasted%20image%2020250826013615.png)
    

**Leaky ReLU:** 
- 죽은 ReLU' 문제를 해결하기 위해 음수 입력에 대해 0이 아닌 아주 작은 기울기(일반적으로 0.01)를 부여
- ![](images/Pasted%20image%2020250826013627.png)
    

**PReLU(Parametric ReLU):** 
- Leaky ReLU의 고정된 음수 기울기 값을 역전파를 통해 학습하도록 개선한 형태
- 즉, 음수 구간 기울기를 학습으로 최적화
- ![](images/Pasted%20image%2020250826013638.png)

    

#### 기타 ReLU 변형

- ![](images/Pasted%20image%2020250826013652.png)
- **ELU(Exponential Linear Unit):** 음수 영역에서 기울기가 0으로 포화되지 않고 부드럽게 감소하는 특징이 있다.
- **Swish:** 구글이 개발한 함수로, 특히 40개 이상의 층을 가진 매우 깊은 네트워크에서 뛰어난 성능을 보인다.
- **Maxout:** 여러 개의 선형 함수 입력 중 최댓값을 출력하는 함수로, ReLU의 일반화된 형태. 
	- 그러나 학습해야 할 매개변수가 두 배로 늘어나 실용성이 떨어진다는 단점이 있다.
    

####  Softmax

- ![](images/Pasted%20image%2020250826013701.png)
- 다중 클래스 분류 문제의 마지막 출력 레이어에 사용
- 각 클래스에 대한 출력을 확률 값(총합이 1)으로 변환하여, 가장 높은 확률을 가진 클래스를 예측 결과로 선택할 수 있게 해준다.
    

---

### Choosing Activation Functions

- **다중 분류 문제의 최종 레이어→ Softmax
    
- **RNN:** Tanh가 좋은 성능을 보임
    
- **그 외:** 우선 ReLU로 시작하여, '죽은 ReLU' 문제가 발생하거나 성능 개선이 필요할 경우 Leaky ReLU를 시도. PReLU, Maxout 등은 최후의 수단으로 고려할 수 있다. 

- 매우 깊은 네트워크를 구축할 때는 Swish 함수를 고려


**활성화 함수 비교표**

|함수명|특성|장점|단점|권장 사용처|
|---|---|---|---|---|
|**선형**|f(x)=x|없음|역전파 불가, 비선형성 부족|없음|
|**이진 계단**|x>0이면 1, 아니면 0|직관적|미분 불가, 다중 분류 부적합|없음|
|**Sigmoid**|0∼1로 변환|부드러움, 미분 가능|기울기 소실, 계산 비쌈|과거에 주로 사용|
|**TanH**|−1∼1로 변환|Sigmoid보다 학습 효율적|기울기 소실, 계산 비쌈|RNN 계열|
|**ReLU**|음수 0, 양수 그대로|계산 빠르고 효율적|'죽은 ReLU' 문제|대부분의 딥러닝 모델|
|**Leaky ReLU**|음수에도 작은 기울기|'죽은 ReLU' 문제 해결|ReLU보다 약간 복잡|ReLU 대체재|
|**PReLU**|음수 기울기를 학습|Leaky ReLU보다 유연|복잡, 성능 보장 어려움|실험적 접근|
|**Softmax**|출력을 확률 분포로 변환|다중 분류 최종 출력에 필수|단일 레이블에만 적합|다중 클래스 분류의 출력층|

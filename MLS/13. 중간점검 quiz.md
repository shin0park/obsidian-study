
### Feature Engineering

- 피처의 수가 많아지면 모델 성능이 떨어질 수 있다. 그 이유는 무엇이며? 이런 현상을 무엇이라고 칭하는가? 
	- 각 피처는 데이터 공간에서 새로운 차원을 형성한다. 따라서 피처가 많을 수록 차원은 증가하고, 데이터가 전체 데이터 공간에서 가지는 밀도가 줄어들기 때문에 데이터가 sparse(희소)해지는 현상이 발생한다. 데이터 분포가 희소해지면 모델이 의미있는 패턴을 학습하기 어려워지기 때문에 성능이 떨어지게 된다.
	- 이를 차원의 저주 Curse of Dimensionality 라고 한다.
	  
- PCA(Principal Component Analysis) 주성분 분석 기법에 대해 간략하게 설명
	- 차원 축소기법 중 하나로, 데이터의 구조를 가장 잘 반영하는 분산을 최대화하는 최적의 기저(optimal base)를 찾아 차원을 축소하는 기법. 즉, 데이터의 중복성이 가장 적은 방향을 최적의 기저로 찾아내 차원을 축소한다.
	  
- 지도학습과 비지도학습의 차이점은 무엇인가? 이에 해당하는 알고리즘 한두개?
	- 지도학습: 레이블이 존재하는 데이터 사용 (입력과 출력이 쌍으로 존재)
		- 목적: 입력 데이터에 대한 출력 값 예측
		- 알고리즘: 회귀, 분류
	- 비지도학습: 레이블이 없는 데이터 사용 (출력값이 없음, clustering)
		- 목적: 데이터 구조를 찾고 그룹화
		- 알고리즘: K-means, pca
		  
- TF-IDF 란 무엇인지 간략하게 설명 (Term frequency Inverse Document Frequency)
	- 용어 빈도와 역문서 빈도의 약자로, 텍스트 데이터에서 용어의 중요성을 파악하는데 사용하는 기법이다.
	- 단어가 특정 문서에는 자주 등장(TF)하면서도, 전체 문서 집합에는 드문 단어(IDF)를 찾아 단어의 고유성 및 중요도를 계산하는 기법이다
	- IDF는 지수성 분포를 나타나는 경우가 있어 로그 처리를 하기도 한다.

### Deep Learning

- CNN과 RNN의 특징에 대해 간략하게 설명해주세요
	- CNN:
		- 이미지 분류와 같이 정형화 되지 않은 데이터 내의 특징을 찾아내는데 탁월한 모델이다.
		- 필터 또는 커널이라고 불리는 극소수용영역이 이미지를 스캔하여 모서리나 선과 같은 특징을 찾아내고 이 것이 상위계층으로 전달되어 특정 물체를 인식하게 된다.
		- 높을 시소스, 많은 하이퍼파라미터 필요
	- RNN:
		- 시계열 테이터나 문장 속 단어, 기계번역 등 시퀀스 데이터를 처리하는데 사용되는 모델이다.
		- 메모리 셀을 통해 이전 단계의 정보를 현재 단계의 처리과정에서 반영하는 순환적인 구조를 가진다. 
		- 과거의 상태를 기억하고 다음에 예측하며 판악한다.
		- BGTT (Backpropagation Through Time) 사용: 모든 시간 단계에 대해 역전파가 적용되다보니 네트워크가 매우 깊은 것처럼 되어 기울기 소실 및 폭발 문재가 있다.
			- 즉, 과거의 중요 정보가 시간이 지남에 따라 점차 희석되어 사라지는 문제이다.
			  
- 손실함수와 역전파는 무엇인가
	- 손실함수는 모델의 예측값과 실제값 사이의 오차를 계산하는 함수이다. 
		- 즉, 손살함수의 값이 작을수록 모델의 예측이 더 정확하다.
	- 역전파란 딥러닝 모델 학습 과정에서 사용되는 알고리즘으로, 모델의 가중치를 최적화하기 위해 손실함수의 기울기를 계산하고 이를 바탕으로 가중치를 업데이트하는 과정을 말한다.
		- 기울기를 통해 손살함수의 값을 최소화 하는 방향으로 가중치를 업데이트한다
		  
- 학습률은 무엇인가
	- 모델 성능 최적화의 가장 기본 하이퍼파라미터로, 경사하강법(gradient descent)와 같은 최적화 알고리즘이 손실함수를 최소하기 위해 가중치를 업데이트 할때, 얼마나 큰 '보폭'으로 이동할지를 결정하는 값이다. 너무 크면 최적점을 지나쳐 버리고 너무 작으면 수렴이 매우 느리다
	  
- L1, L2 정규화의 차이점과 그 이유에 대해 설명해주세요
	- 정규화는 모델의 과적합을 방지하는데 사용되는 머신러닝 기법으로 손실함수에 가중치 제약을 추가한다.
	- L1 Regularization
		- 모델의 손실함수에 가중치 절댓값 합에 비례하는 penalty term 추가
		- Lasso 라고도 불린다.
		- 손실이 최소가 되게 학습하니 패널티항도 최소가 되게 학습된다.
		- 특정 가중치들은 0으로 수렴해서 불필요한 피처 제거
		- 덜 중요한 피처의 가중치를 0으로 만들기 때문에 피처 셀렉에 좋다
	- L2 Regularation
		- 모델의 손실함수에 가중치 제곱 합에 비례하는 penalty term 추가
		- Ridge 라고도 불린다.
		- 가중치가 0으로 수렴하지만 완전히 0이 되지는 않음
		- 모든 피처를 모델에 유지하지만 가중치 값을 0으로 가깝게 줄여 과적합을 방지하나 완전히 0이 되진 않음

- 기울기 소실문제는 무엇이고 이를 해결하기 위한 해결책은 무엇인가
	- 기울기가 역전파를 통해 이전 계층으로 전달될때 점진적으로 작아져 거의 0에 수렴하는 현상을 말한다.
	- 해결책: 
		- 다중계층구조 (전체 네트워크를 서브로 나눠서 개별 학습)
		- LSTM (장기상태와 단기상태를 별도로 관리하여 과거 정보의 손실 방지)
		- ResNet (skip connections)을 도입하여 기울기가 특정 계층을 우회하여 흐를 수 있도록 하여 정보 손실을 줄이고 학습을 용이하게 함
		- ReLU: 양수 입력에 대한 기울기를 1로 유지함으로써 기울기 소실 문제 완화

- Precision(정밀도) 와 Recall(재현율) 에 대한 차이를 설명해주세요
	- Precision
		- 모델이 양성이라고 예측한 결과 중에서 실제 양성인것
		- TP / (TP + FP)
		- 의료 검진
	- recall
		- 실제 양성인 모든 사례중 모델이 올바르게 양성으로 예측한 비율
		-  TP / (TP + FN)
		- 사기탐지

- SpageMaker 내장 알고리즘중 XGBoost에 대해 간략하게 설명해주세요
	- 결정트리의 앙상블 대표 기법 중 하나로, 새로운 트리를 추가하여 이전 트리의 오류를 수정하는 방식으로 작동된다.
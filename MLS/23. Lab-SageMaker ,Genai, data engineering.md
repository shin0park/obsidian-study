
- Amazon Sagemaker > Notebook instances > crate notebook instance
	- open notebook
	- upload - Transformers 관련 py 파일

Transformer - Bert
- hugging face - 사전 훈련된 모델의 대규모 저장소 의 패키지를 설치한다 (pip install)
	- transformer 모델들 많이 있음
	- free
- Tokenizers
	- transformers 패키지로부터 tokenizer 하기 위한 패키지를 import
		- BertModel, BertTokenizer
		- Bert는 only encoder로 만들어진 트랜스포머이다 (디코더로만 구현된 GPT와는 반대로)
		- `tokenized = tokoenizer("I read a good novel.")`
- Positional Encoding
	- 병렬적으로 처리
	- sin/cos 함수를 통해 메트릭 추출
- Self-Attention
	- BertModel, BertTokenizer 를 import 해서 가져온다
	- 시각화를 위해 show 패키지 import
	- ```
	  tokenizer_viz = BertTokenizer.from_pretrained(modelName)
	  model_viz = BertModel.from_pretrained(modelName)
	  show(model_viz, "bert", tokenizer_viz, "I read a good novel.", display_mode = "light", head=11)
	  ```
	  - ex3ERT 라는 사이트에서도 시각화 가능
		  - ![](images/Pasted%20image%2020251229000640.png)

GPT2 model (137M parameter)
 - hugging face에서 가져온 transformer 패키지(pipeline)를 통해 사전에 훈련된 모델들을 쉽게 가져올 수 있다.
 - pipeline을 통해 gpt2 모델 가져옴
 - generator 함수에 특정 문장을 넣고 실행 -> 글을 작성
	 - 어떤 영화인지 말을 해주지 않아도 매우 구체적인 글을 작성해준 것을 볼 수 있음
- "Star Trek" 이란 문장을 넣고 실행
	- 위키피디아의 정보를 준 것을 볼 수 있음.

---

### Foundation Model

- 사전 훈련된 거대한 트랜스포머 모델
- 특정 Task에 맞게 fine tuning 을 거치게 된다.
- ex) GPT-n (OpenAI, Microsoft)
	- Bert (Google)
	- DALL-E (OpenAI, Microsoft)
	- LLaMa (Meta)
	- Segment Anything (Meta)


![](images/Pasted%20image%2020251229002125.png)


		  
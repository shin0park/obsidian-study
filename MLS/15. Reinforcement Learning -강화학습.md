## Reinforcement Learning (강화학습)

- Agent(행위자)가 환경(Environment)을 탐색하며, 행동(Action)의 결과에 따라 **보상(Reward)** 을 학습
	- 특정 환경 내에서 **에이전트(Agent)** 가 시행착오를 통해 최적의 행동 정책을 학습하는 기계 학습 방식
	- 이 과정에서 에이전트는 해당 공간을 능동적으로 "탐색(explores)"하며 환경과 상호작용 함
	- 지도 학습이나 비지도 학습과 달리, RL은 명확한 정답 레이블 대신 보상(Reward) 신호를 기반으로 학습
	- 에이전트는 탐색을 진행하면서 다양한 조건 하의 **상태 변화(state changes)** 에 대한 **가치(value)** 를 학습하게 되며 , 이 가치 정보는 장기적인 보상을 최대화하기 위한 최적 정책을 발견하는 데 사용되어 이후 에이전트의 행동을 결정하는 기준이 된다.

- 학습 결과로 각 상태(State)-행동(Action) 쌍의 가치(Q-value)를 계산하여 이후 행동을 결정

- 적용 사례: 게임 AI(예: 팩맨), 공급망 관리, HVAC (난방, 환기, 공조 시스템) 시스템, 로봇 공학, 자율주행, 대화 시스템(Dialog systems) 등
	- 이러한 복잡한 시스템에 RL을 적용하기 위해서는 대규모 병렬 처리 및 분산 컴퓨팅 환경이 필수

- 한 번 환경을 충분히 탐색하면, 이후에는 빠르고 효율적인 의사결정 가능 - fast on-line performance 제공


### Q-Learning

- 강화 학습을 구현하는 특정 알고리즘 중 하나로, 상태-행동 쌍의 가치를 학습하는 방식으로 작동. 상태와 행동을 입력했을 때 이에 대한 가치를 출력으로 주는 함수
- state-action value function
- ![400](images/Pasted%20image%2020251013215120.png)
- [https://spacebike.tistory.com/53#google_vignette](https://spacebike.tistory.com/53#google_vignette)

- 구성요소:
	- 상태 집합 S
	- 가능한 행동 집합 A
	- 상태/행동 가치함수 Q(s,a)
	- 초기 Q값은 0에서 시작, 에이전트가 환경을 탐색하며 Q값을 지속적으로 업데이트
	- 행동의 결과가 좋으면 Q 증가, 나쁘면 감소
	  
- Q-Learning의 핵심은 현재 상태 에서 행동 를 취했을 때 얻을 수 있는 장기적인 가치를 반영하여 Q 값을 갱신하는 것
- 업데이트 공식은 즉각적인 보상과 다음 상태에서 기대할 수 있는 최대 미래 보상을 통합하여 현재 가치를 조정한다.
- Q를 찾으려면 **믿고 시작**하는 게 필요. 현재에는 Q를 모르지만 미래에는 Q를 안다고 믿는
- 학습 식:
	- Q(s,a) ← Q(s,a) + α * (reward(s,a) + γ * max(Q(s′)) – Q(s,a))
	- (α: 학습률, γ: 할인율(Discount factor), s′: 다음 상태)
	- 할인 계수를 사용하면 에이전트가 현재 보상뿐만 아니라 미래에 얻을 수 있는 보상까지 고려하여 행동의 가치를 평가 -> 한 단계 이상 미리 보기(look ahead) 기능을 구현하는 효과를 낸다
	- 다음 상태에서 가능한 모든 행동 중 가장 큰 Q 값을 사용하여 현재의 Q 값을 업데이트하므로, 에이전트의 실제 행동 정책과 무관하게 최적 정책을 학습하는 오프-폴리시(Off-Policy) 방식에 해당
		- **On-policy:** 현재 사용하는 정책(행동 선택 방식)으로 데이터를 모으고, 그 정책 자체를 개선
		- **Off-policy:** 데이터를 모으는 정책과, 실제 학습하여 개선하는 정책이 다름
			- Q-learning은 대표적인 Off-policy로. 즉, 행동은 e-greedy로 선택하면서, 학습은 항상 “가장 좋은 행동(최적 Q값)”을 기준으로 업데이트한다.
		- On-policy: “내가 지금 하는 방식 그대로 배워!”
		- Off-policy: “데이터는 이렇게 모으지만, 배울 때는 항상 최선의 선택만 기준으로 할 거야!”

---

### Exploration Problem (탐색 문제)

- 값이 높은 행동만 고르면 새로운 최적 경로를 찾기 어렵다.
- 강화 학습에서 에이전트가 환경을 탐색할 때, 최적의 행동만을 선택하는 것이 아니라 가끔은 무작위로 행동을 선택해야 할 때가 있다. 
- 이러한 탐험과 활용의 균형을 잡아주는 방법 중 하나가 바로 `엡실론-그리디(Epsilon-Greedy)` 정책이다.

- ε-탐욕 정책(Epsilon-Greedy Policy) 사용:
	- Greedy: 현재 시점에서 가장 좋아 보이는 선택을 하는 알고리즘 또는 의사결정 방식
	- 탐험(Exploration)과 활용(Exploitation)을 적절히 섞는 방식
	- 대부분의 경우(1-epsilon 확률)에는 Q값이 가장 높은, 즉 가장 좋은 행동을 선택합
	- 하지만 가끔(epsilon 확률로)은 무작위로 행동을 선택
		- “보통은 내가 생각하기에 제일 좋은 선택을 하지만, 가끔은 다른 것도 시도해볼게!” 
		  예를 들어, e=0.1이면 90%는 최적의 행동, 10%는 랜덤 행동
	- 랜덤 수 < ε이면 무작위 행동 선택 → 지속적인 탐색 보장
	- ε은 시간이 지남에 따라 점진적으로 줄여감 (탐색 → 활용 전환)

---

### Markov Decision Process (MDP) 마르코프 결정 과정

- 강화학습을 수학적으로 정의하는 수학적 모델링 프레임워크
- MDP는 결과가 일부는 무작위로 결정되고 일부는 의사 결정자의 행동에 의해 통제되는 상황에서의 의사 결정을 모델링하기 위해 사용
- RL 과정을 수학적 표기법으로 구조화
	- 상태: s, s′
	- 상태 간의 이동 확률을 나타내는 상태 전이 함수(State transition functions): Pₐ(s, s′)
	- Q 값에 해당하는 개념: 보상 함수 Rₐ(s, s′)
	- 상태 전이 확률 Pₐ(s, s′), 보상 함수 **Rₐ(s, s′)** 로 표현
- **불확실성(stochastic)** 을 포함한 순차적 의사결정 모델
- 즉, MDP는 Q-Learning을 수식으로 일반화한 형태
- MDP는 보다 전문적인 용어로 **이산 시간 확률 제어 과정(discrete time stochastic control process)**이라고도 불림

---
### Reinforcement Learning in SageMaker

- AWS SageMaker는 대규모 심층 강화 학습(DRL) 워크로드를 위해 관리형 서비스를 제공하며, 강력한 프레임워크 및 분산 처리 기능을 통합한다.
- **프레임워크:** TensorFlow, MXNet
	- SageMaker RL은 Q 값을 근사화하기 위해 심층 신경망을 사용하는 DRL을 지원하며, 이를 위해 **TensorFlow**와 **MXNet**을 포함한 딥러닝 프레임워크를 활용
- **지원 도구:** Intel Coach, Ray RLlib (오픈소스 툴킷)
- **환경(Environment):**
	- 높은 유연성을 제공하여 다양한 시뮬레이션 환경과의 통합을 지원한다.
	- 이는 **사용자 정의 환경, 오픈 소스 환경, 상업용 환경**을 모두 포함한다.
    - MATLAB, Simulink
	- EnergyPlus, RoboSchool, PyBullet
	- Amazon Sumerian, **AWS RoboMaker**
		- AWS RoboMaker와의 통합: 모델을 실제 환경에 배포하기 전에 클라우드 기반 가상 환경에서 수백만 번의 시행착오를 거쳐 빠르게 훈련시키는 **Sim2Real** 워크플로우를 효율적으로 구축할 수 있도록 한다.
- **분산 학습:** 다중 코어/다중 인스턴스에서 학습과 환경 시뮬레이션 병렬 수행 가능
	- 학습(training)과 **환경 롤아웃(environment rollout)** 모두를 분산시킬 수 있다
    

---

#### 강화학습 주요 용어

| 용어          | 의미                                          |
| ----------- | ------------------------------------------- |
| Environment | 에이전트가 상호작용하는 보드, 미로 등 전체적인 시스템 배치           |
| State       | 현재 상태 (예: 플레이어 위치)                          |
| Action      | 주어진 상태에서 에이전트가 취할 수 있는 행위 (예: 이동)           |
| Reward      | 특정 상태에서의 행동에 대한 보상                          |
| Observation | 환경으로부터 에이전트가 감지하는 정보 (예: 미로의 주변 환경, 체스판 상태) |

#### 강화학습 하이퍼파라미터 튜닝

- SageMaker의 **자동 하이퍼파라미터 튜닝 기능**을 활용해 최적화 가능
- 사용자 정의 변수들을 추상화하여 자동 탐색 수행
    
####  강화학습 인스턴스 유형

- 공식 가이드라인 명시는 없으나, **GPU** 사용 권장
- 멀티 인스턴스 및 멀티코어 분산 학습 지원

---
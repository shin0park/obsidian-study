
## Transformers and Generative AI 

- **Transformer**는 대규모 언어 모델(LLM)의 핵심 아키텍처이다.
    
- GPT와 같은 **생성형 AI 모델**은 Transformer 구조를 기반으로 한다.
    
- AWS에서는 이러한 모델을 **Amazon SageMaker**, **Amazon Bedrock** 등을 통해 학습·배포·활용할 수 있도록 지원한다.
    
- Transformer는 기존 순차 모델보다 **확장성(scale)** 과 **병렬 처리 성능**이 뛰어나다.

---

## Transformer Architecture 개요 

- Transformer의 핵심은 **Self-Attention 메커니즘**이다.
    
- 입력 시퀀스 전체를 동시에 고려하여 각 토큰의 중요도를 계산한다.
    
- RNN 계열과 달리 시간 순서에 의존하지 않는다.


---

## Transformers 이전: RNN / LSTM 

- **RNN, LSTM**은 시계열·자연어처럼 **순차 데이터**를 모델링하는 데 사용되었다.
    
- 이전 단계의 hidden state를 다음 단계로 전달하는 **피드백 구조**를 가진다.
    
- 긴 문장이나 긴 시퀀스에서는 **장기 의존성 문제**가 발생할 수 있다.
    
---

## Encoder–Decoder 구조의 한계

- 기계 번역에서는 **Encoder–Decoder RNN 구조**가 사용되었다.
    
- Encoder의 마지막 hidden state 하나로 전체 문장을 요약 → **정보 병목 현상 발생**
    
- 문장 앞부분의 정보가 소실될 수 있다.

---

## Attention 개념 도입

- 각 토큰마다 **개별 hidden state**를 유지한다.
    
- 단어 순서 차이에 더 강건하다.
    
- 단어 간 **관계(문맥)** 를 일부 파악 가능해졌다.
    
- 하지만 여전히 RNN 기반 → **병렬 처리 불가**

---

## Transformer의 등장

- RNN을 제거하고 **Feed-Forward Neural Network(FFNN)** 사용
    
- 핵심은 **Self-Attention**
    
- 모든 토큰을 **병렬 처리** 가능 → 대규모 데이터 학습 가능
    
- AWS에서 대규모 분산 학습에 매우 적합한 구조

---

## Self-Attention 기본 개념 

- 각 토큰은 **임베딩 벡터**를 가진다.
    
- Self-Attention은 모든 토큰 임베딩의 **가중 평균**을 계산한다.
    
- 각 토큰이 문맥상 **어떤 토큰을 얼마나 참고해야 하는지** 학습한다.
    
- 결과적으로 문맥을 반영한 새로운 임베딩을 생성한다.
    
---

## Query / Key / Value 

- 학습되는 세 개의 가중치 행렬:
    
    - **Query (Wq)**
        
    - **Key (Wk)**
        
    - **Value (Wv)**
        
- 각 토큰 임베딩은 q, k, v 벡터로 변환된다.
    
- 이 과정은 역전파(backpropagation)로 학습된다.

---

## Scaled Dot-Product Attention 

- Query와 Key의 **내적(dot product)** 으로 유사도 점수 계산
    
- 점수에 **softmax** 적용 → 확률 분포
    
- 어떤 토큰이 더 중요한지 수치적으로 결정
    
- AWS 문서에서도 표준 Attention 계산 방식으로 설명된다.
    
---

## Masked Self-Attention 

- 미래 토큰을 보지 못하도록 **마스크 적용**
    
- **GPT 계열 모델**은 Masked Self-Attention 사용
    
- BERT는 다른 방식(MLM)을 사용
    
- 자동 회귀(auto-regressive) 생성에 필수적 개념

---

## Value 가중합 계산

- softmax 결과(가중치)를 **Value 벡터에 곱한 뒤 합산**
    
- 결과 벡터는 해당 토큰의 **문맥 반영 표현**
    
- 이 출력이 다음 FFNN으로 전달된다.

---

## 전체 Self-Attention 흐름

- 모든 토큰에 대해 **동시에 반복 수행**
    
- 병렬 처리 가능
    
- 업데이트된 임베딩을 FFNN에 입력
    
- Transformer 블록의 핵심 처리 단계

---

## Multi-Head Self-Attention 

- q, k, v를 여러 **head**로 분할
    
- 각 head는 서로 다른 관점의 문맥을 학습
    
- 결과를 결합하여 표현력 향상
    
- AWS 문서에서도 Transformer 성능 향상의 핵심 요소로 설명됨

---

## Transformer 활용 분야 

- 챗봇(Chat)
- 질의응답(QA)
- 텍스트 분류 (감성 분석)
- 개체명 인식(NER)
- 요약(Summarization)
- 번역
- 코드 생성
- 자동화 고객 서비스
- AWS Bedrock / SageMaker JumpStart에서 대표적으로 활용
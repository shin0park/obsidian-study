
## Transformers and Generative AI 

- **Transformer**는 대규모 언어 모델(LLM)의 핵심 아키텍처이다.
	- LLM은 Transformer 아키텍처 기반으로 하며, 방대한 양의 데이터로 사전 훈련된 초대형 딥러닝 모델이다.
	- 트랜스포머는 self-attention 기능을 포함하는 인코더와 디코더로 구성된 일련의 신경망으로 정의된다.
	- 이 인코더와 디코더는 입력 텍스트 시퀀스에서 의미를 추출하고, 시퀀스 내 단어 및 구문 간의 복잡한 관계를 파악하는 핵심적인 역할을 한다.
- 트랜스포머 LLM은 전통적인 지도 학습 방식과 달리, 대규모 비정형 데이터셋을 활용하여 레이블이 지정되지 않은 데이터로부터 스스로 레이블을 생성하며 학습하는 자기 지도 학습(Self-supervised Learning) 방식을 채택했다.
	- 이러한 Self-Learning 능력을 통해 광범위한 지식을 습득하고 초대형 모델을 가능하게 했으며, ommon Crawl(500억 개 이상의 웹 페이지)이나 Wikipedia(약 5,700만 페이지)와 같은 방대한 데이터 소스를 소화할 수 있는 기반을 마련했다
- GPT와 같은 **생성형 AI 모델**은 Transformer 구조를 기반으로 한다
- AWS에서는 이러한 모델을 **Amazon SageMaker**, **Amazon Bedrock** 등을 통해 학습·배포·활용할 수 있도록 지원한다
	- Bedrock은 데이터 소스를 연결하여 모델 정확도와 관련성을 높이는 RAG(Retrieval Augmented Generation)나 모델을 특정 도메인에 맞게 최적화하는 Fine-tuning과 같은 기술을 통해 모델의 개인화 및 맞춤화를 지원한다.
	  
- Transformer는 기존 순차 모델보다 **확장성(scale)** 과 **병렬 처리 성능**이 뛰어나다
- Transformer의 핵심은 **Self-Attention 메커니즘**이다.
- 입력 시퀀스 전체를 동시에 고려하여 각 토큰의 중요도를 계산한다.
- RNN 계열과 달리 시간 순서에 의존하지 않는다.

---

## 트랜스포머 아키텍처의 진화: 순환 신경망의 한계 (The Evolution of Transformers)

- 트랜스포머의 등장은 이전 세대의 순차 모델인 순환 신경망(RNN)과 LSTM(Long Short-Term Memory)의 한계를 극복하는 과정에서 비롯되었다.
	- ![300](images/Pasted%20image%2020251221205344.png)
	- **RNN, LSTM**은 시계열·자연어처럼 **순차 데이터**를 모델링하는 데 사용되었다.
	- 특히 정보를 순방향으로 전파하는 Feedback loop 를 도입한 것이 특징이다.
	- 이 구조는 이 구조는 $t$ 시점의 입력뿐만 아니라 이전 시점의 은닉 상태(Hidden State)를 사용하여 현재 시점의 출력을 결정함으로써, 시계열 데이터나 언어와 같이 순차적 특성을 갖는 데이터를 모델링하는 데 매우 유용했다.
    
- 그러나 RNN 기반 모델은 근본적인 구조적 한계를 가지고 있었다. 
	- 정보의 처리가 순차적(sequential)으로 이루어져야 했기 때문에, 긴 시퀀스를 처리할 때 문제가 발생했다. 
	- 시퀀스의 앞쪽에 위치한 정보가 여러 단계를 거치면서 점진적으로 희석되거나 소실되는 현상, 즉 **장거리 의존성(Long-range Dependencies) 문제**를 효과적으로 포착하기 어려웠다.
    

### 인코더-디코더 아키텍처의 정보 병목

- 기계 번역에서는 **Encoder–Decoder RNN 구조**가 사용되었다.
- 기계 번역과 같은 시퀀스-투-시퀀스(Sequence-to-Sequence) 태스크의 초기 발전은 인코더/디코더 아키텍처에 의해 주도되었다. 
	- 이 아키텍처에서 인코더와 디코더는 모두 RNN으로 구성되었다. 
	- 인코더는 입력 문장을 처리하여 그 핵심 정보를 추출하고, 이 정보를 **단 하나의 최종 은닉 상태(Hidden State) 벡터**에 압축하여 디코더로 전달했다.
    
- Encoder의 마지막 hidden state 하나로 전체 문장을 요약 → **정보 병목 현상(Information Bottleneck) 발생**
	- 특히 입력 시퀀스가 길어질수록, 인코더의 시작 부분에 있는 중요한 정보가 이 단일 벡터에 모두 담기지 못하고 손실될 가능성이 높아졌다.
	- 이는 모델이 시퀀스 시작 부분의 문맥을 잊어버리게 만들어 번역의 품질을 저해하는 주요 원인이 되었다.
---

## “Attention is all you need” Attention 개념 도입

![500](images/Pasted%20image%2020251221205913.png)

![500](images/Pasted%20image%2020251221205437.png)

- 이러한 병목 현상을 해결하기 위해 "Attention is all you need"라는 논문에서 트랜스포머가 제안되기 전에 **어텐션 메커니즘**이 인코더-디코더 모델에 도입되었다. 
	- 어텐션 메커니즘은 단일 은닉 상태 벡터 대신, 인코더의 **각 단계(토큰)마다 은닉 상태**를 제공했다
	- 어텐션은 디코더가 출력을 생성하는 각 단계에서 입력 시퀀스의 모든 은닉 상태를 동적으로 참조하도록 허용했다. 
	- 이 과정에서 현재 생성해야 할 출력 토큰과 가장 관련성이 높은 입력 토큰에 동적으로 **어텐션 가중치(Attention weights)** 를 부여했다. 
	- 이를 통해 모델은 단어 순서의 차이에 더 잘 대처할 수 있게 되었고, 입력 시퀀스 내의 단어들 사이의 관계 개념을 형성하기 시작했다.

- 하지만 여전히 RNN 기반 → **병렬 처리 불가**
	- 어텐션 메커니즘은 문맥적 이해도를 비약적으로 향상시켰지만
	- RNN의 본질적인 순차적 특성 때문에, 각 토큰의 계산은 이전 토큰의 결과에 의존하여 진행되어야 했다.
	- 이는 계산 과정의 **병렬화(Parallelization)** 를 불가능하게 만들었고, 결과적으로 모델을 대규모 데이터셋으로 효율적으로 훈련시키는 데 한계로 작용했다.


## Transformer의 등장: 병렬 처리의 실현 (The Shift to Parallelism)

트랜스포머 아키텍처는 2017년 제안된 이후 자연어 처리 분야에 혁명적인 변화를 가져왔다. 
이 혁신의 핵심은 '어텐션' 그 자체라기보다는, 아키텍처의 구조적 변화를 통해 획득한 **병렬 처리 능력**에 있었다.

#### RNN의 폐기와 FFNN의 채택

- RNN을 제거하고 **Feed-Forward Neural Network(FFNN)** 사용
	- 기존 RNN 기반 모델의 순차적 계산 의존성을 완전히 제거하기 위해
	- 인코더와 디코더에서 순환 신경망(RNNs)을 사용하지 않고, 대신 **피드포워드 신경망(FFNNs)** 을 사용한다. 
	- FFNN은 순차적 메모리를 필요로 하지 않기 때문에 병렬 계산에 매우 적합하다

####  자기-어텐션(Self-Attention)의 핵심 역할 및 병렬 처리

- 트랜스포머 아키텍처의 핵심은 **자기-어텐션(Self-Attention)** 메커니즘
    
- 모든 토큰을 **병렬 처리** 가능 → 대규모 데이터 학습 가능
	- 기존 어텐션이 인코더와 디코더 사이에 적용되어 서로 다른 시퀀스 간의 관계를 매핑했다면, Self-Attention은 단일 시퀀스 내에서 모든 토큰이 다른 모든 토큰과의 관계를 동시에 계산하도록 한다
	- 모델 훈련 과정의 **병렬화**가 가능해졌고, 이는 트랜스포머의 가장 중요한 기능적 이점으로 작용한다

---
## Self-Attention 기본 개념 

- 각 토큰은 **임베딩 벡터**를 가진다.
- Self-Attention은 모든 토큰 임베딩의 **가중 평균**을 계산한다.
- 각 토큰이 문맥상 **어떤 토큰을 얼마나 참고해야 하는지** 학습한다.
- 결과적으로 문맥을 반영한 새로운 임베딩을 생성한다.

- Self-Attention 메커니즘은 시퀀스 내에서 각 토큰의 문맥적 의미를 파악하는 핵심 단계이다. 
	- ![500](images/Pasted%20image%2020251221211046.png)
	- 이 과정은 단순히 토큰들의 가중 평균을 구하는 것을 넘어, **문맥 내 의미(Meaning in Context)** 를 동적으로 포착하는 지능형 정보 검색 시스템으로 기능한다.
	- Self-Attention은 모든 토큰 임베딩의 가중 평균을 계산한다. 
	- 이 과정의 'magic'은 가중치, 즉 어텐션 스코어의 계산 방식에 있다. 
		- 계산 결과, 특정 토큰은 자신의 문맥을 이해하는 데 중요한 다른 토큰들과 강하게 연결된다. 
		- 예를 들어, 문장 "Attention is a novel idea"에서 'novel'이라는 단어는 'book'보다는 'idea'와 더 강하게 연결되어 '새로운'이라는 문맥적 의미를 포착하게 된다. 이 과정을 통해 토큰은 자신의 주변 환경을 반영하는 새로운 임베딩 벡터를 획득한다


### Query, Key, Value (QKV) 행렬의 학습

- Self-Attention은 역전파(back-propagation)를 통해 학습되는 세 가지 가중치 행렬에 의존한다. 
- 학습되는 세 개의 가중치 행렬:
    - **Query (Wq)**       
    - **Key (Wk)**
    - **Value (Wv)**
- ![500](images/Pasted%20image%2020251221211714.png)
- 각 입력 토큰의 초기 임베딩($X$)은 이 세 행렬과 곱해져 세 가지 독립적인 벡터를 생성한다:
	1. **쿼리 벡터 ($q$):** 해당 토큰이 시퀀스 내 다른 토큰들에게 '질의'하는 내용.
	2. **키 벡터 ($k$):** 해당 토큰이 다른 토큰들의 질의에 응답할 수 있는 '주소' 또는 '색인' 역할.
	3. **값 벡터 ($v$):** 해당 토큰이 문맥 정보로 제공할 실제 '정보 페이로드'.
- 즉, 모든 토큰은 자신의 $q, k, v$ 벡터를 가지며, 이 벡터들은 해당 토큰의 위치와 의미를 기반으로 한다.

## Scaled Dot-Product Attention 

![500](images/Pasted%20image%2020251221211739.png)

- 자기-어텐션 메커니즘에서 토큰 간의 유사도 스코어는 쿼리($q$) 벡터와 모든 키($k$) 벡터 사이의 **내적(Dot Product)** 을 계산하여 산출된다.
	- 내적은 여기서 두 벡터 간의 유사도를 측정하는 함수로 사용된다.
	- 계산된 내적 값은 키 벡터의 차원 수($d_k$)의 제곱근으로 나누어 **스케일링**된다. 
	- 이는 내적 값이 클 때 발생하는 기울기 폭발(gradient explosion)을 방지하고, 소프트맥스 함수의 안정성을 높이는 역할을 수행한다.
    
- 점수에 **softmax** 적용 → 확률 분포
	- 유사도 스코어가 계산된 후, 이 스코어에 **소프트맥스(Softmax)** 함수가 적용된다. 
	- 소프트맥스는 스코어를 합이 1이 되는 확률 분포(attention weights)로 정규화한다. 
	- 이 정규화된 가중치는 현재 토큰이 시퀀스 내의 다른 각 토큰으로부터 문맥 정보를 얼마나 많이 가져와야 하는지를 결정하는 역할을 한다. 
	- 예를 들어, 특정 토큰에 대해 50%, 30%, 20%의 가중치가 할당될 수 있다
    
- 어떤 토큰이 더 중요한지 수치적으로 결정


## Masked Self-Attention 

![500](images/Pasted%20image%2020251221212111.png)

- 트랜스포머 모델이 순차적으로 텍스트를 생성하는 작업(예: 언어 생성 또는 GPT 계열 모델)을 수행할 때는 **Masked Self-Attention** 이 적용된다.

- 미래 토큰을 보지 못하도록 **마스크 적용**
	- **목적:** 디코더가 현재 생성 중인 토큰 시점 이후에 등장할 **미래 토큰(future tokens)** 의 정보를 미리 "엿보는 것(peeking)"을 기술적으로 방지하기 위함
- **작동 방식:** 마스크는 소프트맥스 이전에 적용되며, 미래 토큰에 해당하는 스코어에 매우 낮은 값(예: $-\infty$)을 부여하여, 소프트맥스 통과 후 해당 가중치가 0이 되도록 만든다.

- **모델별 차이:** GPT와 같은 생성 모델은 이 마스킹을 사용하여 엄격한 단방향 문맥 흐름을 유지한다. 
	- 반면, BERT와 같은 모델은 양방향 문맥 이해를 위해 입력 토큰 일부를 가리는 마스크드 언어 모델링(Masked Language Modeling)을 사용한다.
    


### 문맥화된 임베딩 업데이트 및 FFNN 전달
#### 가중치 합산 (Summation)

- softmax 결과(가중치)를 **Value(값) 벡터에 곱한 뒤 합산**
	- 즉, 각 토큰의 값 벡터는 해당 토큰이 현재 처리 중인 토큰에 얼마나 중요한지에 비례하여 가중치를 부여받는다.
	- 이 가중치가 부여된 값 벡터들은 모두 **합산(Summation)** 되어 최종적인 출력 벡터($Z$)를 형성한다.
	- 이 벡터($Z$)는 해당 토큰의 새로운, 문맥화된 임베딩이며, 시퀀스 전체의 정보를 반영하고 있다.
    
- 결과 벡터는 해당 토큰의 **문맥 반영 표현**
- 이 출력이 다음 FFNN으로 전달된다.


#### 병렬 처리의 반복

- 모든 토큰에 대해 **동시에 반복 수행**
- 병렬 처리 가능
	- 이러한 병렬 처리를 통해 모든 토큰은 동시에 자신의 문맥적 의미를 업데이트한다. 
	- 결과적으로, 각 토큰에 대해 업데이트된 임베딩이 생성되며, 이 임베딩은 해당 토큰이 시퀀스에서 차지하는 문맥적 중요성을 효과적으로 가중치 부여한 정보를 포함한다
- 업데이트된 임베딩을 FFNN에 입력
	- FFNN은 비선형 변환을 통해 이 문맥 벡터를 추가적으로 처리하고, 이를 다음 트랜스포머 레이어로 전달하거나 최종 출력에 사용한다
- Transformer 블록의 핵심 처리 단계


## Multi-Head Self-Attention 

- 쿼리($q$), 키($k$), 값($v$) 벡터가 여러 개의 독립적인 "헤드(Head)"로 분할된 행렬로 재구성된다
- 각 헤드는 분할된 입력에 대해 독립적으로 어텐션 계산(QKV 계산, 스코어링, 합산)을 수행하며, 이 모든 계산이 병렬로 실행된다. 
- 마지막으로, 각 헤드에서 도출된 문맥 벡터 결과물들은 연결(Concatenation)된 다음, 최종적인 선형 변환을 거쳐 하나의 출력 벡터로 통합된다.
    
- 결과를 결합하여 표현력 향상
	- 멀티-헤드 어텐션의 주요 목적은 모델의 **표현 능력(Representational Power)** 을 확장하는 것이다. 여러 개의 어텐션 헤드를 사용함으로써, 모델은 시퀀스 내에서 **다양한 유형의 관계**를 동시에 포착하고 학습할 수 있다.
    
- AWS 문서에서도 Transformer 성능 향상의 핵심 요소로 설명됨


## Transformer 활용 분야 

- 챗봇(Chat)
- 질의응답(QA)
- 텍스트 분류 (감성 분석)
- 개체명 인식(NER)
- 요약(Summarization)
- 번역
- 코드 생성
- 자동화 고객 서비스
- AWS Bedrock / SageMaker JumpStart에서 대표적으로 활용

---

## From Transformers to GPT 

자연어 처리와 생성형 인공지능의 발전은 기존의 RNN 아키텍처에서 트랜스포머 기반의 아키텍처로의 패러다임 전환을 통해 가속화되었다. 이러한 변화의 중심에는 생성형 사전 학습 트랜스포머인 GPT가 자리 잡고 있다.

- **GPT(Generative Pre-Trained Transformer)** 는 Transformer 아키텍처를 기반으로 한 생성형 모델이다.
- Transformer 구조는 대규모 텍스트 데이터를 학습해(시퀀스 내 구성 요소 간의 복잡한 관계를 추적) **다음 토큰을 예측**하는 데 최적화되어 있다.
	- 이는 과거의 순환 신경망(RNN)이나 컨볼루션 신경망(CNN)이 데이터를 순차적 또는 계층적으로 평가하여 발생했던 속도 저하와 장거리 의존성 소실 문제(문장이 길어질수록 앞부분의 정보를 잃어버리는 '기울기 소실' 문제)를 극복한 결과이다.
	- 트랜스포머는 자기 주의(Self-Attention) 메커니즘을 도입하여 시퀀스 내의 모든 요소가 서로 동시에 상호작용할 수 있도록 설계되었다. 
		- 이러한 병렬 처리 능력은 대규모 데이터셋에서의 학습 효율성을 극적으로 높였으며, 수천억 개의 파라미터를 가진 거대 언어 모델(LLM)의 탄생을 뒷받침했다.
    
- GPT는 이러한 트랜스포머 아키텍처 중 디코더 구조를 극대화하여 설계된 모델이다.
- GPT는 이름에서 알 수 있듯이 '생성형(Generative)', '사전 학습된(Pre-trained)', '트랜스포머(Transformer)'라는 세 가지 핵심 속성을 가진다. 
	- 이 모델은 인간과 유사한 텍스트 및 콘텐츠를 생성하고 대화형 방식으로 질문에 답할 수 있는 기능을 제공하며, 업계 전반에서 Q&A 봇, 텍스트 요약, 검색 및 콘텐츠 생성에 광범위하게 활용되고 있다.

---

### GPT 모델 구조 개요


![500](images/Pasted%20image%2020251222222705.png)

- GPT의 구조적 특징 중 가장 두드러지는 점은 디코더 블록의 스택으로 구성된 디코더 전용(Decoder-only) 아키텍처라는 점이다. 
	- 일반적인 트랜스포머 모델이 인코더와 디코더를 모두 사용하는 것과 달리, GPT는 오직 디코더만을 활용하여 입력된 토큰 다음에 올 토큰을 반복적으로 생성하는 방식에 집중한다. 
		- 비교를 위해, BERT는 인코더만 사용하며, T5는 인코더와 디코더를 모두 사용하는 대표적인 모델이다.
	- 여러 개의 **Decoder Block**이 층층이 쌓여 있음을 볼 수 있다.
        
- 각 Decoder Block은 다음으로 구성된다.
    - **Masked Self-Attention**    
	    - 모델이 특정 위치의 토큰을 생성할 때, 시퀀스 내에서 해당 위치 이후에 나타나는 토큰들을 참조하지 못하도록 차단(Masking)한다.
	    - 이는 생성 모델이 미래의 정답을 미리 보고 학습하는 것을 방지하여, 실제 추론 시와 동일한 인과 관계적(Causal) 환경을 구축하기 위함이다.
    - **Feed-Forward Neural Network**
	    - attention 계층의 출력을 받아 비선형 변환을 수행한다. 
	    - 일반적으로 두 개의 선형 계층과 ReLU 또는 GeLU와 같은 활성화 함수로 구성되며, 모델이 학습한 복잡한 패턴을 고차원 공간에서 재배치하는 역할을 한다
        
- GPT는 입력/출력 개념이 명확하지 않고,
    - 이전 토큰들을 기반으로 **다음 토큰을 계속 생성**한다.
        
- 사용자는 텍스트를 **프롬프트(prompt)** 로 제공한다.
    
- 라벨 없는 대규모 텍스트 데이터로 학습 가능 → **사전학습(Pre-training)** 이 핵심
    
- 특정 태스크가 아니라 **언어 자체를 학습**
    
- 수백억~수천억 개의 파라미터 사용
    

---

## GPT 입력 처리 과정 

![500](images/Pasted%20image%2020251222223207.png)

- **Tokenization / Token Encoding**
    - 텍스트를 토큰 단위로 분해
        
- **Token Embedding**
    - 토큰 간 의미적 유사성과 관계를 벡터로 표현
        
- **Positional Encoding**
    - 토큰의 **위치 정보**를 모델에 제공
    - Transformer는 순서 개념이 없기 때문에 필수
    - **사인·코사인 기반(sinusoidal) 함수** 사용
    - 입력 길이가 달라져도 일반화 가능
        
- AWS 문서에서도 Transformer 입력의 표준 처리 방식으로 설명됨


## GPT 출력 처리 과정

![500](images/Pasted%20image%2020251222223249.png)

- Decoder 스택의 마지막에서 **출력 벡터** 생성
    
- 이 벡터를 **토큰 임베딩 행렬과 곱함**
    
- 각 토큰이 다음에 올 확률값(**logits**) 생성
    
- 일반적으로 가장 확률이 높은 토큰을 선택하지만,
    
    - **Temperature** 값을 사용해 확률 분포를 조절 가능
        
    - 생성 결과의 다양성 증가
        
- AWS 문서에서 생성형 모델의 기본 출력 메커니즘으로 설명됨

## Transformer 기반 전이 학습 (Fine-Tuning)

![400](images/Pasted%20image%2020251222223351.png)

- 사전학습된 GPT 모델을 기반으로 **추가 학습(Fine-tuning)** 가능
    
- 방법:
    - 일부 레이어 고정(freeze), 일부만 재학습
    - 특정 도메인에 맞는 데이터 추가
    - 새로운 토크나이저 학습 가능
    - 출력단에 새로운 레이어 추가
        
- 소량의 데이터만으로도 성능 개선 가능
    
- 프롬프트–응답 예시를 통해 모델 적응
    
- 생성 모델을 **분류, 감성 분석 등 태스크용**으로 전환 가능
    
- AWS SageMaker Fine-tuning, Bedrock Custom Model 방식과 개념적으로 동일

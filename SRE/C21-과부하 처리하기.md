# Handling Overload

아무리 효율적일 로드 밸런싱을 하더라도, 시스템의 일부는 결국 **과부화** 상태가 될 가능성이 있다.
신뢰도 높은 시스템을 운영하기 위해서 과부하 상태를 잘 처리하는 것이 필수다.

- 과부하를 처리하는 한 가지 방법은 **저하된 응답(degraded responses)**을 제공하는 것이다.
- 이는 정상적인 응답보다 정확도가 낮거나 데이터가 적지만, 계산이 더 쉬운 응답을 의미한다.
	- ex)
	- 검색 쿼리에 대해 전체 데이터를 검색하는 대신, 후보 데이터의 일부만 검색하여 결과 제공
	- 최신 데이터가 아닐 수 있지만, 원본 데이터 저장소에 접근하는 것보다 비용이 적게 드는 로컬 복사본을 사용
- 하지만 극단적인 과부화 상태에서는 저하된 응답조차 계산하고 제공할 수 없는 경우가 있다.
---
## "초당 쿼리 수"의 함정

- 쿼리 비용은 클라이언트 코드(여러 클라이언트가 있는 서비스의 경우) 또는 시간대 (가정 사용자, 직장 사용자, 일괄 트래픽)와 같은 여러가지 요인에 의해 달라질 수 있다.

#### 한계
- 초당 쿼리수 또는 요청의 통계(읽는 키의 수)을 사용하여 시스템의 **수용량**을 모델링 하면 실망스러운 결과를 볼 수 있다.
- 러한 지표가 한 시점에서는 적절하게 작동할 수 있지만, 비율이 변할 수 있다.
- 때로는 점진적으로, 때로는 급격하게 변하기도 한다. 
- (예: 소프트웨어의 새 버전이 일부 요청의 리소스 요구 사항을 크게 줄이는 경우)
- 목표치가 오락가락하면 로드밸런싱을 설계하고 구현하기가 어려워진다.

#### 더 나은 방법: 직접적인 수용량 측적
- 사용 가능한 리소스를 직접 측정하는 것이다.
- ex) 특정 데이터센터에서 특정 서비스를 위해 500개의 CPU 코어와 1TB의 메모리가 예약되어 있다면, 이러한 숫자를 직접 사용하여 데이터센터의 수용량을 모델링하는 것이 훨씬 효과적
- 대부분 CPU 사용량이 프로비저닝 신호로 사용하는 것이 잘 작동한다는 것을 발견
	- 가비지 컬렉션을 수행하는 플랫폼의 경우, 메모리에 대한 부하는 본질적으로 CPU 사용률의 증가를 동반한다.
	- 다른 플랫폼의 경우, 나머지 자원이 CPU보다 먼저 바닥나는 경우는 극히 드물기 때문에 해당 자원을 준비할 여력이 충분하다.
---
## 사용자별 제한
Per-Customer Limits

- 현실적으로 global overload 글로벌 과부화는 자주 발생한다.
- 글로벌 과부화 발생시, 서비스가 비정상적인 사용을 하는 고객에게만 에러 응답을 리턴하고 나머지 고객들은 영향을 받지 않도록 하는 것이 중요하다.
- 서비스 소유자는 고객의 행위를 고려해서 자신들의 수용량을 준비하고, 서비스 수준 동의에 따라 **사용자별 할당량(per-customer quotas)**를 정의해아한다.
- 예를 들어, **총 10,000개 CPU** 를 사용하는 백엔드 서비스가 있다면, 고객별로 다음과 같은 제한을 둘 수 있다.
	- Gmail: 초당 4000 CPU 허용
	- Calendar: 초당 4000 CPU 허용
	- Android: 초당 3000 CPU 허용
	- Google: 초당 2000 CPU 허용
	- 다른 사용자: 초당 500 CPU
- 이는 총합이 10,000 CPU를 초과할 수도 있지만, 서비스 소유자는 모든 고객이 동시에 리소스 제한에 도달할 가능성이 낮다는 사실에 의존한다.
---

## 클라이언트 측에서의 사용량 제한
Client-Side Throttling

백엔드 작업이 요청을 처리할 수 없는 경우, 이를 빠르게 거부하는 것이 중요하다.

- 단순히 요청을 거부하는 것이 항상 효율적인 것은 아니다.
	- 간단한 RAM 조회가 필요한 요청을 거부하는 것은 요청을 수락하고 실행하는 것과 거의 동일한 비용이 들 수 있다. (요청/응답 프로토콜 처리의 오버헤드가 응답 생성의 오버헤드보다 훨씬 큰 경우)
	- **너무 많은 요청을 거절할 경우**, 백엔드가 거절하는 데에만 CPU를 소모하게 되어 과부하 발생 가능하다
- 이를 해결하기 위해 **클라이언트 측에서 사용량을 제한**하는 적응형 사용량 제한**(adaptive throttling)"** 을 도입했다.
- 클라이언트가 자체적으로 실패한 것으로 처리한 것이다.

#### 적응형 사용량 제한(adaptive throttling)

- **requests**: 애플리케이션 계층에서 시도한 요청 수
- **accepts**: 백엔드에서 받아드린 요청 수

- 백엔드가 트래픽을 거부하기 시작하면, **accepts** 수가 **requests** 수보다 작아진다.
- 클라이언트는 **requests**가 **accepts**의 K배가 될 때까지 백엔드에 요청을 계속 발행할 수 있다.
- 이 한계에 도달하면, 클라이언트는 자체적으로 규제를 시작하고 새로운 요청을 로컬에서 거부한다.
- 거부 확률 
	- ![](images/Pasted%20image%2020250210222930.png)
- 애플리케이션이 백엔드에 요청을 시도하는 비율이 백엔드가 수락하는 비율보다 상대적으로 증가함에 따라, 새로운 요청을 거부할 확률이 높아짐.
- K 배율
	- 감소하면 조금 더 적극적으로 적응형 사용률 제한을 적용
	- 증가하면 적응형 사용률 제한을 조금 더 여유있게 적용
- 일반적으로 2배의 배율을 선호
- 과부하 상황에서도 백엔드 요청을 안정적으로 유지가능하며, 클라이언트가 로컬에서 빠르게 대응 가능하다. (추가적인 종속성 없음)
- 백엔드가 거부하는 데 소비하는 리소스를 줄일 수 있다.
- 단, 클라이언트가 간헐적으로만 요청을 보내는 경우, 제대로 동작하지 않을 수 있다. 각 클라이언트가 백엔드의 상태를 파악하기 어렵기 때문이다.

---

## 중요도
Criticality

- 중요도는 전역 할당량(quota) 및 제한(throttling)에 있어 큰 영향을 미친다.

- **CRITICAL_PLUS**
    - 가장 중요한 요청으로, 실패하면 사용자에게 **직접적 영향을 미치는 경우** 에 해당
- **CRITICAL**
    - 기본적으로 **운영 환경(production jobs)에서 발생하는 요청**
    - 실패 시 사용자에게 영향을 미치지만, **CRITICAL_PLUS보다는 덜 심각**
    - 서비스는 예상되는 **CRITICAL 및 CRITICAL_PLUS 트래픽을 처리할 충분한 용량** 을 준비해야 한다.
- **SHEDDABLE_PLUS**
    - 어느정도 실패가 발생하는 것(partial unavailability)**을 용인할 수 있는 트래픽
    - 배치 작업(batch jobs)과 같은 요청에 해당하며, 몇 분 또는 몇 시간 후 다시 시도 가능
- **SHEDDABLE**
    - **빈번한 부분적 실패가 자주 발생하거나** 간혹 아예 **사용이 불가능할 것**으로 예상되는 작업들 

- 위 네가지의 값을 더 세분화 할수는 있지만 이를 또 운영하기 위해 더 많은 리소스가 필요
- 이 중요도를 RPC 시스템의 최우선 요소로 채택
	- 전역 할당량을 초과했을때, 백엔드가 이미 중요도가 낮은 모든 요청을 거부하고 있다면, 그 다음 지정된 중요도를 가진 요청을 거부한다.
	- 태스트 자체에 과부하가 발생하면, 중요도가 더 낮은 요청들은 거부
	- 적응형 사용량 제한 시스템은 각 중요도 별로 통계를 관리한다.
- **중요도의 자동 전파 (Criticality Propagation)**
	- 백엔드가 **요청 A** 를 수신하고 이를 처리하는 과정에서 **추가 요청 B와 C** 를 다른 백엔드로 보낼 경우,
    - 기본적으로 요청 B와 C는 **요청 A와 동일한 중요도를 갖게된다.
- 과거에는 Google의 여러 시스템이 자체적으로 중요도를 정의하여 **서비스 간 일관성이 부족** 했다.
- 이제는 **중요도를 표준화하고 RPC 시스템에서 자동으로 전파** 하여 모든 종속 서비스가 중요도 기준을 일관되게 관리 할 수 있게 되었다.
---

## 활용도에 대한 신호들
Utilization Signals

- **활용도(Utilization)**은 대부분 그저 CPU 사용률을 측정한 것이지만, 어떤 경우 메모리 비율을 측정하기도 한다.
- 활용률이 설정된 한계치에 다다르면, 요청의 중요도를 기반으로 요청을 거부한다.
- 활용률 신호는 직업의 로컬 상태를 기반으로 하며 다양한 신호에 대한 구현하고 있다.
- 일반적으로 사용되는 신호는 프로세스의 부하이며, **실행자 부하 평균 executor load average"이라고 부르는 시스템에 의해 결정된다.

#### 실행기 부하 평균 executor load average
- 프로세스 내에서 **활성 스레드(active threads) 수를 계산**
	-  활성된 스레드가 태스크를 실행할 수 있는 프로세서의 수를 넘어서면 요청을 거부하기 시작한다.
	- 단기간에 끝나는 작업들은 아주 짧은 시간동안 부하가 급격히 높아지는데, 이 값을 대부분 다듬어서 현상을 상쇄할 수 있다.
---

## 과부화 요류 처리하기
Handling Overload Errors

- 클라이언트가 **과부하 관련 오류 응답을 받았을 때 어떻게 반응해야 하는지** 역시 중요한 문제이다.

과부화 에러 유형
- 데이터센터 내의 대량의 백엔드 태스크에 과부화가 걸리는 상황
	- 이상적인 로드 밸런싱이 작동한다면 이런 상태는 발생하지 않아야 한다.
	- 이 경우 **요청을 재시도(retry)하지 않고 오류를 최종 사용자에게 반환** 해야 한다.
- 데이터센터 내의 일부 백엔드 태스크에 과부화가 걸리는 상황
	- 이는 데이터 센터 내부에서 **완벽하지 않은 로드 밸런싱** 으로 인해 발생한다.
	- 어느 한 태스크가 최근 리소스를 많이 소모하는 요청을 수신한 경우
	- 대부분의 경우, **데이터 센터 내 다른 작업에는 여유 리소스가 있으므로 즉시 요청을 재시도하는 것이 바람직하다.*

- 일반적으로 클라이언트에 가장 가까운 백엔드가 할당된다.
- 로드밸런싱 적책 관점에서 재시도 요청과 새로운 요청을 구분하지 않는다.
	- 재시도된 요청이 새로운 요청처럼 다른 태스크에서 처리될 수 있으므로 자연스레 로드밸런싱이 되어 부하가 분산된다.

---

## 재시도 여부 결정하기

- 재시도 허용 수준(Per-Request Retry Budget)
	- 각 요청은 **최대 3회까지 재시도 가능**
	- 3회 시도 후에도 실패하면, 요청을 호출자에게 반환한다.
	- 동일한 요청이 **3번 연속 과부하된 작업에 할당** 되었다면, **데이터 센터 전체가 과부하 상태일 가능성이 높으므로 추가 시도가 의미 없다**
- 클라이언트별 재시도(Per-Client Retry Budget)
	- 각 클라이언트는 **전체 요청 대비 재시도 요청 비율** 을 추적한다.
	- 재시도 요청 비율이 10% 이하일 때만 재시도 허용한다.
	- 과도한 재시도를 방지하여 시스템 안정성 유지할 수 있다.
	- ex
		- 클라이언트 측 로직에 의해 총 전달된 요청 수를 `X` 라고 하면,
	    - **재시도 없이** 처리될 경우: `X`
	    - **최대 3회 재시도 적용 시**: `3X` - 상당한 양
	    - **클라이언트별 재시도 예산(10% 적용) 시**: `1.1X` (큰 효율)
- 재시도 횟수 트래킹 Request Retry Count Tracking
	- 클라이언트가 메타데이터 내에 해당 요청에 대한 처리 시도가 몇번 있었는지 기록해두는 것
		- 첫 번째 요청: `0`
		- 한 번 재시도 시: `1`
		- 두 번째 재시도 시: `2`
		- **3회 도달 시 더 이상 재시도하지 않음**


- Google의 대규모 서비스는 **여러 계층(stack)의 시스템** 으로 구성된다.
	-  각 계층이 다른 백엔드 서비스에 의존하기에, 오류가 발생한 계층 바로 위에서만 재시도 하는 것이 바람직하다.
	- 여러 계층에서 재시도를 수행하면, 재시도 요청이 기하급수적으로 증가하여 위험하다.
	- ![200](images/Pasted%20image%2020250210231434.png)

---

## 연결에 대한 부하

- 요청수만이 아니라 **연결을 유지하는 비용**도 고려해야한다. (CPU, 메모리 소비)
- 연결이 많아질수록 관리비용이 증가할 것이다.
- 비활성 클라이언트는 **주기적으로 TCP 연결을 끊고 UDP로 헬스 체크 수행한다**
	- 요청이 적은 클라이언트가 많으면, **헬스 체크 비용이 실제 요청 처리 비용보다 더 커질 수 있다**
	- 빈도를 조절하거나 동적으로 연결을 생성 및 해제해서 개선할 순 있다.
- 대량의 새로운 연결 처리 (Handling Connection Bursts)
	- 대규모 배치 작업(batch job)이 한꺼번에 많은 클라이언트를 생성하면 너무 많은 새로운 연결 요청이 한 번에 발생하여 백엔드가 과부하될 위험이 있다.
		- 배치 프록시 계층을 추가
			- **"배치 클라이언트 → 배치 프록시 → 백엔드"** 구조
			- 배치 프록시가 모든 요청을 중재하여 **실제 백엔드의 부하를 줄이도록 한다.**
			- 연결 수 감소 및 로드밸런싱 개선 효과

---
결론
1. **백엔드는 과부하 상태에서도 최대한 많은 트래픽을 수용해야 한다**
2. **하지만 과부하 시 요청을 "적절히" 거절하며, 비정상적 부하 증가를 방지해야 한다**
3. **효율적인 재시도 정책을 적용하여 불필요한 트래픽 증가를 방지**
4. **연결 유지 비용을 고려하여 최적의 연결 전략을 사용**

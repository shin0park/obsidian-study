# Addressing Cascading Failures

연속적 장애 (Cascading failure): 
정상적인 것 처럼 보여지는 응답 때문에 시간이 지나면서 장애가 계속해서 가중되는 현상이다. 
즉, 전체 시스템의 일부에서 장애가 발생했을때 주로 나타나며, 이로인해 시스템의 다른 부분에도 장애가 발생할 가능성이 늘어나게 된다. 이처럼 부하가 늘어나고 장애 가능성이 증가하며 도미노처럼 서비스의 전체에 장애가 발생하는 현상이다.

### 연속적 장애의 원인과 그 대책
Causes of Cascading Failures and Designing to Avoid Them

#### 서버 과부화 Server Overload
- 가장 일반적인 원인은 과부화다 
- ![400](images/Pasted%20image%2020250217224122.png)
  예를 들어, 클러스터 A의 프론트엔드가 초당 1,000 QPS(초당 요청 수)를 처리하고 있을 때, 클러스터 B가 실패하면 모든 트래픽이 클러스터 A로 이동하여 A의 부하가 1,200 QPS로 증가한다. 이로 인해 클러스터 A의 서버가 과부하되어 실패하고, 전체 시스템에 연쇄적인 장애를 일으킬 수 있다.

#### 자원의 부족 Resource Exhaustion

- 자원의 부족은 높은 지연응답과 에러율 증가, 낮은 품질의 응답을 야기 할 수 있다.
- CPU, 메모리, 디스크 공간, 네트워크 대역폭 등 시스템 리소스가 고갈되면, 시스템이 정상적으로 동작하지 못할 수 있으며, 자원의 부족은 연쇄 장애를 유발하는 주요 원인 중 하나이다
- CPU 자원이 부족한 경우
	- 모든 요청의 처리가 느려진다.
	- 요청처리가 느려지므로 동시에 처리 중인 요청의 수가 증가하게 된다.
	- 모든 요청을 안정적으로 처리할 수 없게 되면 서버의 큐가 넘쳐나게 된다.
	- thread starvation 스레드 기아: 스레드가 대기상태에 빠진다.
	- RPC 시간 초과: 서버에 과부화가 발생하면 클라이언트에 대한 RPC 응답도 늦어지게 되어 클라이언트에서 시간초과가 발생할 수 있다. 그러면 서버에서 수행한 작업은 아무런 소용이 없어지고 클라이언트는 RPC 요청을 재시도하여 더 많은 과부하가 발생한다.
	- CPU 캐시 이점의 감소: 더 많은 CPU를 사용할 수록 더 많은 코어에서 누수가 발생할 가능성이 커지고 그결과 로컬 캐시 및 CPU 효율성이 감소한다.
- 메모리 자원이 부족
	- 메모리 기아 memory starvation 은 다음과 같은 현상을 유발한다.
		- 자바의 가비지 컬렉션 Garbage Collection, GC)
			- 자바의 가비지 컬렉션이 비정상적으로 동작하면 가용한 CPU 자원이 줄어들고
			- 이로 인해 요청 처리가 느려지며 RAM 사용량이 증가
			- 그 결과 GC 작업이 더 빈번하게 수행되면 CPU 자원이 더 모자르게 되는 악순환이 발생
			- -> 죽음의 GC 소용돌이 death spiral
		- 캐시 활용률의 감소
			- 가용한 RAM이 부족해지면 애플리케이션 수준의 캐시 활용률이 감소하며 그결과 더 많은 요청이 백엔드로 전달되어 백엔드에 과부화를 초래한다.
		- 스레드 기아 thread starvation
			- thread starvation는 직접적으로 에러를 발생시키거나 헬스체크의 실패를 야기한다.
				- 스레드 기아가 발생하면 요청이 무기한 대기 상태에 빠지게 되는데 결과적으로 시스템이 클라이언트 요청을 적절하게 처리하지 못하면서 HTTP 500 같은 오류를 리턴할 수 있다
				- 헬스체크도 주기적으로 실행될텐데 스레드기아 상태에 도달하면 정상적으로 동작하지 않는다고 판단하고 실패처리를 할 것이다.
			- 많은 RAM을 소비할 수도 있다
				- 스레드가 실행되지 않으면 요청들인 대기열 큐에 계속 쌓일 것이고 쌓일수록 더 많은 RAM을 필요로 할 것이다.
			- 스레드 기아로 인해 프로세스 ID가 모두 소진되는 결과를 초래할 수도 있다.
				- 스레드가 응답하지 않으면 새로운 스레드를 계속 생성하고 프로세스 ID가 할당되기 때문에 소진되는 결과를 초래할 수도 있다.
---
#### 서버 과부화 방지하기
Preventing Server Overload

- 서버 수용량 한계에 대한 부하 테스트, 과부화 상태에서의 실패에 대한 테스트
	- Load test the server’s capacity limits, and test the failure mode for overload
	- 가장 중요한 사안이다.
	- 실제 환경에서의 시스템의 한계를 파악하는 것이 중요하다.
- 경감된 응답 제공하기
	- Serve degraded results
	- 과부화시 일부 기능을 제한하거나 품질이 낮지만 더 수월한 결과를 제공한다.
- 과부화 상태에서 요청을 거부화도록 서비스 구현하기
	- Instrument the server to reject requests when overloaded
	- 과부화 상황에서 스스로 보호할 수 있어야한다. 신속하게 거부하여 리소스 낭비를 줄인다.
- 고수준의 시스템들이 서버에 과부화를 유발하지 않고 요청을 거부하도록 구현하기
	- Instrument higher-level systems to reject requests, rather than overloading servers
	- 상위시스템에서 요청을 거부한다
	- 리버스 프록시 reverse proxy 나 로드밸런서에서 과도한 요청을 차단핟.
- 수용량 계획 실행하기
	- Perform capacity planning
	- 예상되는 트래픽에 맞게 시스템 자원을 적절히 할당한다.

---
#### 큐 관리하기 Queue Management

- 큐는 정상 상태에서는 영향을 미치지 않겠지만, 과부화일 수록 큐에 요청을 적재하면 더 많은 메모리를 소비하고 지연 응답 또한 증가될 것이다.
- 과부하가 발생하며 큐에 요청이 쌓이기만 한다면 대부분의 시간을 큐에서 소비할 수도 있다.
- 따라서 큐의 크기를 스레드 풀 크기의 몇% 이하로 유지하거나, 각 요청 처리 시간, 과부화의 크기와 빈도 등 트래픽 패턴에 따라 조정하는 것이 좋다.
- 지메일 같은 시스템에서는 큐를 사용하지 않고, 스레드가 모두 사용 중일 때 새로운 요청을 거부하는 방식을 채택하기도 한다.
---
#### 부하 제한과 적절한 퇴보

Load Shedding and Graceful Degradation
- 서버가 과부화 상태에 도달하면 부하 제한(load shedding)을 통해 유입되는 트래픽을 감소시켜 어느 정도의 부하를 덜어낼 수 있다. 
	- 시스템이 과부하 상태에 빠지지 않도록, 일부 요청을 의도적으로 거부하는 전략이다.
	- 우선순위가 낮은 요청을 거부하거나, 특정 클라이언트의 요청을 제한하는 방식으로 부하를 분산시킬 수 있다.
- 적절한 퇴보(graceful degradation)는 부하 제한의 개념에서 한걸음 더 나아가 실행해야할 작업양을 감소시키는 방법이다.
	- 전체 db가 아닌 메모리 캐싱에 저장된 일부 데이터에 대해서만 검색한다던가
	- 비교적 정확도가 떨어지지만 더 빠른 알고리즘을 적용할 수도 있다.
	- 하지만 적절한 퇴보는 너무 자주 발생하면 안된다.
	- 대부분 수용량 계획이 실패했거나 혹은 예상치 못한 부항가 발생하는 경우만 적용되어야한다.
---
#### 재시도 Retries

- 재시도 횟수와 간격을 적절히 조정하여 무분별한 잿도을 방지해야한다.
- 무분별한 재시도는 서버에 추가적인 부하를 일으킬 수 있으므로, 재시도 정책을 잘 설계하는 것이 중요하다.
	- 지수 백오프(Exponential Backoff) 방식을 사용하여 재시도 간격을 점점 늘려가는 방식으로 방지할 수도 있다.
	- 클라이언트의 영구적인 에러나 유효하지 않는 요청들은 재시도ㄹ를 해도 결코 성공응답이 나오지 않기때문에 재시도할 필요가 없다. 이런 경우 재시도하지 않도록 잘 설계해야한다.
---
#### 지연응답과 마감기한
Latency and Deadlines

- 데드라인 설정 picking a Deadlines & 마감기한의 상실 Missing deadlines
	- 각 요청에 대한 최대 허용시간을 설정하고, 각 단계에서의 데드라인을 확인하여 이미 초과된 작업을 중단함으로써 불필요한 리소스 사용을 방지한다.
	- 즉, 클라이언트는 이미 데드라인이 지나면 해당 요청 처리를 포기하므로 서버가 무슨 작업을 수행하는지는 신경 쓰지 않는다.
	- 서버는 각 단계의 데드라인이 얼마나 남았는지 확인하고 유효한 처리만 하도록 한다.
- 데드라인 전파
	- 백엔드에 RPC를 보낼때 설정한 데드라인을 찾아내는 것 보다, 서버가 직접 데드라인을 전파하고 작업 취소를 전파해야한다.
---
#### 이중 지연응답 Bimodal latency

- 요청 처리 시간이 두 가지(이중) 패턴으로 나뉘어 일부 요청은 빠르게 완료되지만, 일부 요청은 비정상적으로 오래 걸리는 문제가 발생할 수 있다.
- 예로, 어떤 특정 원인으로 인해 **5%의 요청이 절대 완료되지 않는 문제**가 발생될 수 있다.
---
### 느긋한 시작과 콜드 캐싱
Slow Startup and Cold Caching

- 어떤 프로세스든지 처음 시작된 직후에는 steady state보다 응답속도가 더 느려지는 경향이 있다. 아래의 이유로 발생할 수 있다.
	- 초기화가 필요한 경우: 첫번째 요청을 받을때 필요한 백엔드와의 연결을 설정해야 하는 경우
	- 런타임 성능 최적화: 
		- 일부 언어 특히 자바에서 런타임 성능 향상을 위한 추가 작업이 실행되는 경우
		- JIT(Just-In-Time) 컴파일
		- 핫스팟 최적화(Hotspot Optimization)
		- 지연된 클래스 로딩(Deferred Class Loading)

#### 콜드 캐싱 문제
- 일부 바이너리들은 캐시가 채워져 있지 않으면 효율성이 떨어지는 경우가 있다.
	- Google의 일부 서비스는 요청 대부분을 캐시에서 제공하며, 캐시가 비어 있으면 요청 처리 비용이 훨씬 높아진다.
	- 평상시에는 일부 요청만 캐시 미스(Cache Miss)를 경험하지만, 캐시가 완전히 비어 있는 경우 모든 요청이 비싼 연산을 필요로 한다.
- 콜드 캐시 상태에서 요청을 처리할 준비가 되어 있지 않으면 서비스가 장애를 겪을 가능성이 커지므로 사전 예방 조치가 필요하다.

#### 콜드 캐시 발생하는 경우

- 새로운 클러스터를 추가할 때 → 신규 클러스터는 캐시가 비어 있음
- 유지보수 후 클러스터를 복구할 때 → 캐시가 오래되어 데이터가 유용하지 않을 가능성이 있음
- 작업이 재시작될 때 → 캐시를 채우는 데 시간이 필요하다
	- 캐시를 개별 서버가 아닌 별도의 캐시 서버(memcache 등)에 두어 여러 서버가 공유할 수 있는 장점을 얻을 수도 있다. 다만 추가적인 RPC 호출과 지연이 발생할 수 있다.

#### 콜드 캐시 해결책

- **서비스 오버프로비전(Overprovision)**
	- 아래의 두 가지 캐시를 구분하는게 중요하다
	    - **지연 시간 캐시(Latency Cache)**: 캐시가 비어도 기본적인 요청 처리는 가능
	    - **용량 캐시(Capacity Cache)**: 캐시가 비어 있으면 정상적인 트래픽 처리가 어려움 → 신중한 설계 필요
- **연쇄 장애(Cascading Failure) 방지 기법 적용**
    - 과부하 상태에서는 요청을 거부하고, 서비스 저하 모드(Degraded Mode)로 전환해야 함
- **부하 증가를 점진적으로 수행**
    - 초기 요청량을 천천히 늘려 캐시를 미리 채운 후, 전체 트래픽을 증가시키는 것이 좋다

---
### 연속적 장애의 발생 요인
Triggering Conditions for Cascading Failures

- **프로세스 중단(Process Death)** 
	- 일부 서버 태스크들이 중단되어 가용 용량이 감소한다.
    - 쿼리의 중단, 클러스터 이슈 등 여러 원인을 인해 서비스가 중단되기 직전의 상황까지 몰릴 수 있다.
- **프로세스 업데이트(Process Updates)**
    - 대규모 업데이트가 동시에 적용되면 장애 위험이 증가한다.
    - **off-pick 시간**에 배포하거나, 가용 용량을 고려한 배포 전략 필요하다.
- **새로운 배포(New Rollouts)**
    - 새로운 바이너리, 설정 변경 등으로 인해 성능 저하 발생할 수 있다.
    - 최근 변경 사항을 추적하고 필요시 되돌릴 수 있어야 한다
- **자연적인 트래픽 증가(Organic Growth)**
    - 시스템이 계획 없이 증가하는 트래픽을 감당하지 못하는 경우
- **요청 프로파일의 변화 (Request profile changes)**
	- 로드밸런싱 설정의 변경이나 트래픽 mix, cluster fullness 로 인해 트래픽이 다른 서비스로 옮겨지면서 백엔드 서비스가 다른 클러스터로부터 발신된 요청을 수신하는 경우가 발생한다.
- **자원 한계(Resource Limits)**
    - 클러스터의 CPU 여유분(slack CPU)을 의존하는 것은 위험하다 → 여분의 CPU는 온전히 클러스터의 다른 작업들의 행동에 따라 달라지므로 어느 시점에 갑자기 사용성이 홀쩍 떨어질 수 있다.

---
### 연속적 장애 테스트하기
Testing for Cascading Failures

- **최대 부하까지 테스트(Test Until Failure and Beyond)**
    - 과부화 상태에서 서비스가 어떻게 동작하는지를 이해한다.
    - 부하가 지나치게 많아졌을 때의 복구능력을 검증한다.
- **부하 테스트 시 캐싱 효과 고려**
    - 부하를 점진적으로 증가시키는 경우와 급격하게 증가시키는 경우 결과가 다를 수 있다
    - 이를 모두 테스트 해야한다.
- **실제 트래픽을 활용한 테스트**
    - 컴포넌트마다 장애가 발생하는 지점은 각자 다르기 때문에 각 컴포넌트마다 별도로 부하테스트를 진행해야한다.
    - 실제 트래픽을 테스트할때 주의해야될 점은, 자동화된 보호 장치가 동작하지 않을 경우로 대비해 충분한 여분의 수용량을 확보해야하며, 수동으로 장애조치를 수행할 수 있어야한다.

---
### 연속적 장애를 처리하기 위한 즉각적인 대처
Immediate Steps to Address Cascading Failures

- **리소스 추가(Increase Resources)**
    - 시스템의 가용성이 줄어들었을때 여분의 자원이 있다면 장애 조치로 추가하는 것이 가장 적절한 방법이다.
- **헬스 체크 조정(Stop Health Check Failures)**
    - 과도한 헬스 체크가 오히려 시스템을 불안정하게 만들 수도 있으므로 일시적으로 중지할 수 있다.
    - 즉, **일시적으로 헬스 체크를 비활성화**하면 전체 작업이 정상적으로 수행될 때까지 시스템을 안정화하는 데 도움이 될 수 있다.
	 - **헬스 체크 구분의 중요성**
		- **프로세스 헬스 체크 (Process Health Check)**: 실행 중인지 확인하는 작업 (클러스터 스케줄러에 사용됨)
		- **서비스 헬스 체크 (Service Health Check)**: 현재 특정 요청을 처리할 수 있는지 확인하는 작업 (로드 밸런서에 사용됨) 
			- 두 가지 헬스 체크를 명확하게 구분하면 불필요한 장애를 방지할 수 있다.
- **서버 재시작(Restart Servers)**
    - Java GC(가비지 컬렉션) 문제, 데드락 발생 시 재시작이 효과적일 수 있다.
    - 주의 사항
	    - 반드시 **연쇄 장애의 근본 원인을 파악한 후** 서버를 재시작해야 한다.
	    - 무작정 재시작하면 부하가 단순히 다른 곳으로 이동할 수 있으며, 콜드 캐시 문제로 인해 장애가 더 악화될 수도 있다.
- **트래픽 제한(Drop Traffic)**
    - 전체 시스템을 보호하기 위해 일부 트래픽을 차단하는 것이 필요할 수도 있다.
    - 강력한 해결책이지만, **연쇄 장애가 심각한 상황에서만** 사용해야 한다.
- **퇴보 모드로 들어가기(Enter Degraded Modes)**
	- 일부 요청을 **덜 중요한 방식으로 처리**하거나 **불필요한 트래픽을 줄이는 방식**으로 운영한다.
	- 어떤 트래픽을 저하할 수 있는지 구별해낼 수 있어야 한다.
- **배치 작업 제거(Eliminate Batch Load)**
	- 인덱스 수정, 데이터 복사, 통계 수집 같은 작업은 중요하지만, **서비스가 장애 상태일 때는 비활성화하는** 것을 고려하자
---
#### **결론**

- 시스템이 과부하 상태일 때, **일부 요청을 희생하더라도 전체적인 장애를 방지하는 것이 중요하다.**
- **연쇄 장애를 피하기 위해, 장애 발생 시의 시스템 동작 방식을 미리 이해하고 대비해야 한다.**
- 서비스 성능을 개선하려는 시도가 오히려 **대규모 장애를 유발할 수도 있으므로**, 변경 사항을 신중히 평가해야 한다.
    - **재시도 로직, 로드 밸런싱, 헬스 체크, 캐시 추가 등은 연쇄 장애를 촉발할 수도 있다**
-  **부하를 줄이는 전략과 시스템의 한계를 미리 파악하는 것이 연쇄 장애를 예방하는 핵심이다.** 
